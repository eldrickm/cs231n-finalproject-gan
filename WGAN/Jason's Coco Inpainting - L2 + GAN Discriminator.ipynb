{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  6930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc8082704f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = dset.CocoCaptions(\n",
    "    root='inpainting/train2014',\n",
    "    annFile='inpainting/annotations/captions_train2014.json',\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    \"\"\"Data Iterator for COCO.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path='inpainting/train2014',\n",
    "        dev_path='inpainting/val2014',\n",
    "        train_annotation_path='inpainting/annotations/captions_train2014.json',\n",
    "        dev_annotation_path='inpainting/annotations/captions_val2014.json',\n",
    "    ):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.train_annotation_path = train_annotation_path\n",
    "        self.dev_path = dev_path\n",
    "        self.dev_annotation_path = dev_annotation_path\n",
    "        print('Processing data ...')\n",
    "        self._get_real_and_fake_images()\n",
    "\n",
    "    def _get_real_and_fake_images(self):\n",
    "        \"\"\"Get real and fake images from path.\"\"\"\n",
    "        self.train_dataset = dset.CocoCaptions(\n",
    "            root=self.train_path,\n",
    "            annFile=self.train_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.valid_dataset = dset.CocoCaptions(\n",
    "            root=self.dev_path, \n",
    "            annFile=self.dev_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        \n",
    "        print('Populating training images & captions ...')\n",
    "        train_images = []\n",
    "        train_captions = []\n",
    "        \n",
    "        # There appears to be one image missing for some weird reason.\n",
    "        try:\n",
    "            for img, captions in self.train_dataset:\n",
    "                train_images.append(img)\n",
    "                train_captions.append(captions)\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "        train_images = torch.stack(train_images)\n",
    "        \n",
    "        print('Populating validation images ...')\n",
    "        valid_images = torch.stack([x[0] for x in self.valid_dataset])\n",
    "        valid_captions = [x[1] for x in self.valid_dataset]\n",
    "        \n",
    "        print('Cropping 32x32 patch for training images ...')\n",
    "        noisy_train_images = copy.deepcopy(train_images.numpy())\n",
    "        noisy_train_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_train_images = torch.from_numpy(noisy_train_images)\n",
    "\n",
    "        print('Cropping 32x32 patch for validation images ...')\n",
    "        noisy_valid_images = copy.deepcopy(valid_images.numpy())\n",
    "        noisy_valid_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_valid_images = torch.from_numpy(noisy_valid_images)\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "\n",
    "        self.noisy_train_images = noisy_train_images\n",
    "        self.noisy_valid_images = noisy_valid_images\n",
    "        \n",
    "        self.num_train = len(train_images)\n",
    "        self.num_valid = len(valid_images)\n",
    "\n",
    "    def get_train_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.train_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_train_images[index: index + batch_size]).cuda()\n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples\n",
    "\n",
    "    def get_valid_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.valid_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_valid_images[index: index + batch_size]).cuda()\n",
    "        \n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Populating training images & captions ...\n",
      "Populating validation images ...\n",
      "Cropping 32x32 patch for training images ...\n",
      "Cropping 32x32 patch for validation images ...\n"
     ]
    }
   ],
   "source": [
    "iterator = DataIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(iterator.train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(iterator.noisy_train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGeneratorSkip(nn.Module):\n",
    "    \"\"\"Generator module.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize generator.\"\"\"\n",
    "        super(UNetGeneratorSkip, self).__init__()\n",
    "\n",
    "        #################################\n",
    "        ####### DOWNSAMPLER MODULE ######\n",
    "        #################################\n",
    "\n",
    "        # 3 x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        #################################\n",
    "        ####### UPSAMPLER MODULE ########\n",
    "        #################################\n",
    "\n",
    "        # 64 x 4 x 4\n",
    "        self.tconv1 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels = 4000,\n",
    "            kernel_size=4, bias=False\n",
    "        )\n",
    "        self.tbn1 = nn.BatchNorm2d(4000)\n",
    "\n",
    "        # 48 x 8 x 8 + 48 x 8 x 8 = [96 x 8 x 8]\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=4000, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.tbn2 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 32 x 16 x 16 + 32 x 16 x 16 = [64 x 16 x 16]\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 8, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "        \n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn4 = nn.BatchNorm2d(start_filter * 2)\n",
    "        \n",
    "        self.tconv5 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn5 = nn.BatchNorm2d(start_filter)\n",
    "        \n",
    "        self.tconv6 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter, out_channels=3,\n",
    "            kernel_size=5, stride=1, padding=2, bias=False\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the generator.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Upsampling steps.\n",
    "        tconv1 = F.leaky_relu(self.tbn1(self.tconv1(conv4)), negative_slope=0.2, inplace=True)\n",
    "        tconv2 = F.relu(self.tbn2(self.tconv2(tconv1)), True)\n",
    "        tconv3 = F.relu(self.tbn3(self.tconv3(tconv2)), True)\n",
    "        tconv4 = F.relu(self.tbn4(self.tconv4(tconv3)), True)\n",
    "        tconv5 = F.relu(self.tbn5(self.tconv5(tconv4)), True)\n",
    "        tconv6 = F.tanh(self.tconv6(tconv5))\n",
    "\n",
    "        return tconv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 3 x 32 x 32\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 16 x 16\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 8 x 8\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 4 x 4\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 64 x 2 x 2\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels=1,\n",
    "            kernel_size=2, stride=1, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        # print 'input', input.size()\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.sigmoid(self.conv5(conv4))\n",
    "        return conv4.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNetGeneratorSkip(start_filter=32).cuda()\n",
    "discriminator = Discriminator(start_filter=32).cuda()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "clamp_lower = -0.03\n",
    "clamp_upper = 0.03\n",
    "loss_criterion = nn.MSELoss().cuda()\n",
    "save_dir = 'inpainting/samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(epoch, fake_images, real_images, real_examples_full):\n",
    "    j = np.random.randint(low=0, high=500)\n",
    "    real_examples_full, real_examples, fake_images = iterator.get_valid_minibatch(j, 32)\n",
    "    generator.eval()\n",
    "    reconstructions = generator(fake_images)\n",
    "    # fig = plt.figure(figsize=(20, 40))\n",
    "    # idx = 1\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    real = real_examples_full.data.cpu().numpy()\n",
    "    real_copy = copy.deepcopy(real)\n",
    "    real_copy[:, :, 16:48, 16:48] = reconstructions\n",
    "    real_copy = torch.from_numpy(real_copy)\n",
    "    real = torch.from_numpy(real)\n",
    "    out_tensor = torch.zeros(1, real_copy.size(1), real_copy.size(2), real_copy.size(3))\n",
    "    for zz, zzz in zip(real_copy[:10], real[:10]):\n",
    "        out_tensor = torch.cat([out_tensor, zz.unsqueeze(0)])\n",
    "        out_tensor = torch.cat([out_tensor, zzz.unsqueeze(0)])\n",
    "    vutils.save_image(out_tensor[1:], 'inpainting/samples/epoch_%d_samples.png' % (epoch), normalize=True, scale_each=True, nrow=4)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss_D: -0.183781 Loss_G: -0.101822\n",
      "[1] Loss_D: -0.147319 Loss_G: -0.120449\n",
      "[2] Loss_D: -0.090972 Loss_G: -0.166253\n",
      "[3] Loss_D: -0.199895 Loss_G: -0.098228\n",
      "[4] Loss_D: -0.204985 Loss_G: -0.104863\n",
      "[5] Loss_D: -0.069599 Loss_G: -0.199079\n",
      "[6] Loss_D: -0.163615 Loss_G: -0.094405\n",
      "[7] Loss_D: -0.159474 Loss_G: -0.106967\n",
      "[8] Loss_D: -0.195363 Loss_G: -0.100920\n",
      "[9] Loss_D: -0.205226 Loss_G: -0.107400\n",
      "[10] Loss_D: -0.206264 Loss_G: -0.109709\n",
      "[11] Loss_D: -0.206622 Loss_G: -0.111383\n",
      "[12] Loss_D: -0.206849 Loss_G: -0.112846\n",
      "[13] Loss_D: -0.206923 Loss_G: -0.114108\n",
      "[14] Loss_D: -0.206977 Loss_G: -0.114994\n",
      "[15] Loss_D: -0.207092 Loss_G: -0.115664\n",
      "[16] Loss_D: -0.207078 Loss_G: -0.115790\n",
      "[17] Loss_D: -0.207123 Loss_G: -0.116650\n",
      "[18] Loss_D: -0.207129 Loss_G: -0.118425\n",
      "[19] Loss_D: -0.207092 Loss_G: -0.119325\n",
      "[20] Loss_D: -0.207112 Loss_G: -0.120400\n",
      "[21] Loss_D: -0.207141 Loss_G: -0.121075\n",
      "[22] Loss_D: -0.113299 Loss_G: -0.135107\n",
      "[23] Loss_D: -0.196163 Loss_G: -0.106867\n",
      "[24] Loss_D: -0.205439 Loss_G: -0.114841\n",
      "[25] Loss_D: -0.206267 Loss_G: -0.118359\n",
      "[26] Loss_D: -0.206312 Loss_G: -0.120829\n",
      "[27] Loss_D: -0.206674 Loss_G: -0.122144\n",
      "[28] Loss_D: -0.206489 Loss_G: -0.123020\n",
      "[29] Loss_D: -0.206705 Loss_G: -0.123778\n",
      "[30] Loss_D: -0.206859 Loss_G: -0.123900\n",
      "[31] Loss_D: -0.206958 Loss_G: -0.124888\n",
      "[32] Loss_D: -0.207050 Loss_G: -0.125684\n",
      "[33] Loss_D: -0.206596 Loss_G: -0.142662\n",
      "[34] Loss_D: -0.076318 Loss_G: -0.185108\n",
      "[35] Loss_D: -0.178913 Loss_G: -0.125275\n",
      "[36] Loss_D: -0.178623 Loss_G: -0.113717\n",
      "[37] Loss_D: -0.204278 Loss_G: -0.119255\n",
      "[38] Loss_D: -0.205662 Loss_G: -0.122744\n",
      "[39] Loss_D: -0.206058 Loss_G: -0.124495\n",
      "[40] Loss_D: -0.206492 Loss_G: -0.125669\n",
      "[41] Loss_D: -0.206682 Loss_G: -0.126448\n",
      "[42] Loss_D: -0.206701 Loss_G: -0.127285\n",
      "[43] Loss_D: -0.206756 Loss_G: -0.128012\n",
      "[44] Loss_D: -0.206828 Loss_G: -0.128217\n",
      "[45] Loss_D: -0.206945 Loss_G: -0.128077\n",
      "[46] Loss_D: -0.206972 Loss_G: -0.127910\n",
      "[47] Loss_D: -0.206413 Loss_G: -0.128462\n",
      "[48] Loss_D: -0.201707 Loss_G: -0.128974\n",
      "[49] Loss_D: -0.005485 Loss_G: -0.258322\n",
      "[50] Loss_D: -0.056510 Loss_G: -0.154212\n",
      "[51] Loss_D: -0.107946 Loss_G: -0.135441\n",
      "[52] Loss_D: -0.101627 Loss_G: -0.146960\n",
      "[53] Loss_D: -0.182300 Loss_G: -0.117019\n",
      "[54] Loss_D: -0.203368 Loss_G: -0.120645\n",
      "[55] Loss_D: -0.204647 Loss_G: -0.122983\n",
      "[56] Loss_D: -0.204412 Loss_G: -0.124768\n",
      "[57] Loss_D: -0.119853 Loss_G: -0.140712\n",
      "[58] Loss_D: -0.198266 Loss_G: -0.121206\n",
      "[59] Loss_D: -0.201796 Loss_G: -0.125619\n",
      "[60] Loss_D: -0.203670 Loss_G: -0.127216\n",
      "[61] Loss_D: -0.106207 Loss_G: -0.193010\n",
      "[62] Loss_D: -0.116491 Loss_G: -0.175385\n",
      "[63] Loss_D: -0.145617 Loss_G: -0.156886\n",
      "[64] Loss_D: -0.178499 Loss_G: -0.127215\n",
      "[65] Loss_D: -0.190403 Loss_G: -0.129135\n",
      "[66] Loss_D: -0.168983 Loss_G: -0.130507\n",
      "[67] Loss_D: -0.200967 Loss_G: -0.127665\n",
      "[68] Loss_D: -0.202062 Loss_G: -0.128938\n",
      "[69] Loss_D: -0.203264 Loss_G: -0.129730\n",
      "[70] Loss_D: -0.115270 Loss_G: -0.134413\n",
      "[71] Loss_D: -0.156861 Loss_G: -0.131664\n",
      "[72] Loss_D: -0.200520 Loss_G: -0.126822\n",
      "[73] Loss_D: -0.203652 Loss_G: -0.129113\n",
      "[74] Loss_D: -0.203886 Loss_G: -0.129804\n",
      "[75] Loss_D: -0.204772 Loss_G: -0.130311\n",
      "[76] Loss_D: -0.205068 Loss_G: -0.130846\n",
      "[77] Loss_D: -0.205640 Loss_G: -0.131173\n",
      "[78] Loss_D: -0.205710 Loss_G: -0.131233\n",
      "[79] Loss_D: -0.205966 Loss_G: -0.131339\n",
      "[80] Loss_D: -0.206081 Loss_G: -0.131480\n",
      "[81] Loss_D: -0.206053 Loss_G: -0.131604\n",
      "[82] Loss_D: -0.205967 Loss_G: -0.131406\n",
      "[83] Loss_D: -0.206133 Loss_G: -0.131476\n",
      "[84] Loss_D: -0.205749 Loss_G: -0.131600\n",
      "[85] Loss_D: -0.206126 Loss_G: -0.131383\n",
      "[86] Loss_D: -0.150999 Loss_G: -0.143700\n",
      "[87] Loss_D: -0.104856 Loss_G: -0.172136\n",
      "[88] Loss_D: -0.198568 Loss_G: -0.118286\n",
      "[89] Loss_D: -0.204236 Loss_G: -0.126561\n",
      "[90] Loss_D: -0.205139 Loss_G: -0.128662\n",
      "[91] Loss_D: -0.205449 Loss_G: -0.129988\n",
      "[92] Loss_D: -0.197379 Loss_G: -0.132433\n",
      "[93] Loss_D: -0.112348 Loss_G: -0.141056\n",
      "[94] Loss_D: -0.173000 Loss_G: -0.129152\n",
      "[95] Loss_D: -0.195817 Loss_G: -0.129898\n",
      "[96] Loss_D: -0.172989 Loss_G: -0.143008\n",
      "[97] Loss_D: -0.203717 Loss_G: -0.131265\n",
      "[98] Loss_D: -0.205210 Loss_G: -0.132223\n",
      "[99] Loss_D: -0.205946 Loss_G: -0.132712\n",
      "[100] Loss_D: -0.206225 Loss_G: -0.133091\n",
      "[101] Loss_D: -0.206384 Loss_G: -0.133546\n",
      "[102] Loss_D: -0.206444 Loss_G: -0.133639\n",
      "[103] Loss_D: -0.206628 Loss_G: -0.133861\n",
      "[104] Loss_D: -0.206725 Loss_G: -0.134156\n",
      "[105] Loss_D: -0.206792 Loss_G: -0.134225\n",
      "[106] Loss_D: -0.206852 Loss_G: -0.134346\n",
      "[107] Loss_D: -0.206844 Loss_G: -0.134510\n",
      "[108] Loss_D: -0.206896 Loss_G: -0.134410\n",
      "[109] Loss_D: -0.206897 Loss_G: -0.134513\n",
      "[110] Loss_D: -0.206996 Loss_G: -0.134875\n",
      "[111] Loss_D: -0.206970 Loss_G: -0.134803\n",
      "[112] Loss_D: -0.206938 Loss_G: -0.134830\n",
      "[113] Loss_D: -0.206968 Loss_G: -0.134990\n",
      "[114] Loss_D: -0.207022 Loss_G: -0.134891\n",
      "[115] Loss_D: -0.206994 Loss_G: -0.135127\n",
      "[116] Loss_D: -0.207037 Loss_G: -0.135600\n",
      "[117] Loss_D: -0.207074 Loss_G: -0.135710\n",
      "[118] Loss_D: -0.206668 Loss_G: -0.136015\n",
      "[119] Loss_D: -0.074134 Loss_G: -0.223585\n",
      "[120] Loss_D: -0.093239 Loss_G: -0.169178\n",
      "[121] Loss_D: -0.144196 Loss_G: -0.138328\n",
      "[122] Loss_D: -0.111106 Loss_G: -0.154048\n",
      "[123] Loss_D: -0.183353 Loss_G: -0.123498\n",
      "[124] Loss_D: -0.124608 Loss_G: -0.145056\n",
      "[125] Loss_D: -0.145370 Loss_G: -0.130419\n",
      "[126] Loss_D: -0.161303 Loss_G: -0.139573\n",
      "[127] Loss_D: -0.156786 Loss_G: -0.140489\n",
      "[128] Loss_D: -0.140998 Loss_G: -0.141675\n",
      "[129] Loss_D: -0.174028 Loss_G: -0.133275\n",
      "[130] Loss_D: -0.092267 Loss_G: -0.159682\n",
      "[131] Loss_D: -0.172227 Loss_G: -0.127936\n",
      "[132] Loss_D: -0.138493 Loss_G: -0.146582\n",
      "[133] Loss_D: -0.102028 Loss_G: -0.193798\n",
      "[134] Loss_D: -0.141319 Loss_G: -0.166867\n",
      "[135] Loss_D: -0.160726 Loss_G: -0.140454\n",
      "[136] Loss_D: -0.162745 Loss_G: -0.133595\n",
      "[137] Loss_D: -0.149324 Loss_G: -0.137942\n",
      "[138] Loss_D: -0.194546 Loss_G: -0.132033\n",
      "[139] Loss_D: -0.197520 Loss_G: -0.132925\n",
      "[140] Loss_D: -0.171054 Loss_G: -0.132915\n",
      "[141] Loss_D: -0.149733 Loss_G: -0.145350\n",
      "[142] Loss_D: -0.195704 Loss_G: -0.125546\n",
      "[143] Loss_D: -0.200469 Loss_G: -0.130893\n",
      "[144] Loss_D: -0.138316 Loss_G: -0.138212\n",
      "[145] Loss_D: -0.120005 Loss_G: -0.137155\n",
      "[146] Loss_D: -0.156596 Loss_G: -0.147373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-fd9bc9605546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mD2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mdiscriminator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(15000):\n",
    "    discriminator_losses = []\n",
    "    generator_losses = []\n",
    "    for j in range(0, iterator.num_train, 500):        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        \n",
    "        for disc_updates in range(5):\n",
    "            real_examples_full, real_examples, fake_images = iterator.get_train_minibatch(j, 32)\n",
    "            D1 = discriminator(real_examples)\n",
    "            fake = generator(fake_images)\n",
    "            D2 = discriminator(fake)\n",
    "            discriminator_loss = -.5 * ((D1 - D2).mean())\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            discriminator_losses.append(discriminator_loss.data[0])\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        \n",
    "        generated_images = generator(fake_images)\n",
    "        generator_loss = -.5 * discriminator( generated_images).mean() + loss_criterion(generated_images, real_examples)\n",
    "        optimizer_generator.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "        generator_losses.append(generator_loss.data[0])\n",
    "\n",
    "    print('[%d] Loss_D: %f Loss_G: %f' % (i, np.mean(discriminator_losses), np.mean(generator_losses)))\n",
    "    save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (32,3,64,64) into shape (32,3,32,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-b57993df24bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-210-05dd7cca13c7>\u001b[0m in \u001b[0;36msave_plots\u001b[0;34m(epoch, fake_images, real_images, real_examples_full)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_examples_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mreal_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mreal_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mreal_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (32,3,64,64) into shape (32,3,32,32)"
     ]
    }
   ],
   "source": [
    "save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    # Forward Pass\n",
    "    scores = model(X)\n",
    "    scores = scores.gather(1, y.view(-1, 1)).squeeze()  \n",
    "    \n",
    "    # Backward Pass\n",
    "    gradients_init = torch.FloatTensor([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "    scores.backward(gradients_init)\n",
    "\n",
    "    saliency = X.grad.data\n",
    "    saliency = saliency.abs()\n",
    "    saliency, i = torch.max(saliency, dim = 1)\n",
    "    saliency = saliency.squeeze() \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n",
    "show_saliency_maps(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fooling_np = deprocess(fake_images.clone())\n",
    "X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X[idx])\n",
    "plt.title(class_names[y[idx]])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_fooling_np)\n",
    "plt.title(class_names[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "X_pre = preprocess(Image.fromarray(X[idx]))\n",
    "diff = np.asarray(deprocess(fake_images - real_examples, should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "diff = np.asarray(deprocess(10 * (fake_images - real_examples), should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(12, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_examples[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake_images[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

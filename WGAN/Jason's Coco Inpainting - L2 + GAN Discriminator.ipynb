{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  6311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9c65164570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = dset.CocoCaptions(\n",
    "    root='inpainting/train2014',\n",
    "    annFile='inpainting/annotations/captions_train2014.json',\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    \"\"\"Data Iterator for COCO.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path='inpainting/train2014',\n",
    "        dev_path='inpainting/val2014',\n",
    "        train_annotation_path='inpainting/annotations/captions_train2014.json',\n",
    "        dev_annotation_path='inpainting/annotations/captions_val2014.json',\n",
    "    ):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.train_annotation_path = train_annotation_path\n",
    "        self.dev_path = dev_path\n",
    "        self.dev_annotation_path = dev_annotation_path\n",
    "        print('Processing data ...')\n",
    "        self._get_real_and_fake_images()\n",
    "\n",
    "    def _get_real_and_fake_images(self):\n",
    "        \"\"\"Get real and fake images from path.\"\"\"\n",
    "        self.train_dataset = dset.CocoCaptions(\n",
    "            root=self.train_path,\n",
    "            annFile=self.train_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.valid_dataset = dset.CocoCaptions(\n",
    "            root=self.dev_path, \n",
    "            annFile=self.dev_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        \n",
    "        print('Populating training images & captions ...')\n",
    "        train_images = []\n",
    "        train_captions = []\n",
    "        \n",
    "        # There appears to be one image missing for some weird reason.\n",
    "        try:\n",
    "            for img, captions in self.train_dataset:\n",
    "                train_images.append(img)\n",
    "                train_captions.append(captions)\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "        train_images = torch.stack(train_images)\n",
    "        \n",
    "        print('Populating validation images ...')\n",
    "        valid_images = torch.stack([x[0] for x in self.valid_dataset])\n",
    "        valid_captions = [x[1] for x in self.valid_dataset]\n",
    "        \n",
    "        print('Cropping 32x32 patch for training images ...')\n",
    "        noisy_train_images = copy.deepcopy(train_images.numpy())\n",
    "        noisy_train_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_train_images = torch.from_numpy(noisy_train_images)\n",
    "\n",
    "        print('Cropping 32x32 patch for validation images ...')\n",
    "        noisy_valid_images = copy.deepcopy(valid_images.numpy())\n",
    "        noisy_valid_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_valid_images = torch.from_numpy(noisy_valid_images)\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "\n",
    "        self.noisy_train_images = noisy_train_images\n",
    "        self.noisy_valid_images = noisy_valid_images\n",
    "        \n",
    "        self.num_train = len(train_images)\n",
    "        self.num_valid = len(valid_images)\n",
    "\n",
    "    def get_train_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.train_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_train_images[index: index + batch_size]).cuda()\n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples\n",
    "\n",
    "    def get_valid_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.valid_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_valid_images[index: index + batch_size]).cuda()\n",
    "        \n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Populating training images & captions ...\n",
      "Populating validation images ...\n",
      "Cropping 32x32 patch for training images ...\n",
      "Cropping 32x32 patch for validation images ...\n"
     ]
    }
   ],
   "source": [
    "iterator = DataIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(iterator.train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(iterator.noisy_train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGeneratorSkip(nn.Module):\n",
    "    \"\"\"Generator module.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize generator.\"\"\"\n",
    "        super(UNetGeneratorSkip, self).__init__()\n",
    "\n",
    "        #################################\n",
    "        ####### DOWNSAMPLER MODULE ######\n",
    "        #################################\n",
    "\n",
    "        # 3 x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        #################################\n",
    "        ####### UPSAMPLER MODULE ########\n",
    "        #################################\n",
    "\n",
    "        # 64 x 4 x 4\n",
    "        self.tconv1 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels = 4000,\n",
    "            kernel_size=4, bias=False\n",
    "        )\n",
    "        self.tbn1 = nn.BatchNorm2d(4000)\n",
    "\n",
    "        # 48 x 8 x 8 + 48 x 8 x 8 = [96 x 8 x 8]\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=4000, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.tbn2 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 32 x 16 x 16 + 32 x 16 x 16 = [64 x 16 x 16]\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 8, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "        \n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn4 = nn.BatchNorm2d(start_filter * 2)\n",
    "        \n",
    "        self.tconv5 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn5 = nn.BatchNorm2d(start_filter)\n",
    "        \n",
    "        self.tconv6 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter, out_channels=3,\n",
    "            kernel_size=5, stride=1, padding=2, bias=False\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the generator.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Upsampling steps.\n",
    "        tconv1 = F.leaky_relu(self.tbn1(self.tconv1(conv4)), negative_slope=0.2, inplace=True)\n",
    "        tconv2 = F.relu(self.tbn2(self.tconv2(tconv1)), True)\n",
    "        tconv3 = F.relu(self.tbn3(self.tconv3(tconv2)), True)\n",
    "        tconv4 = F.relu(self.tbn4(self.tconv4(tconv3)), True)\n",
    "        tconv5 = F.relu(self.tbn5(self.tconv5(tconv4)), True)\n",
    "        tconv6 = F.tanh(self.tconv6(tconv5))\n",
    "\n",
    "        return tconv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 3 x 32 x 32\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 16 x 16\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 8 x 8\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 4 x 4\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 64 x 2 x 2\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels=1,\n",
    "            kernel_size=2, stride=1, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        # print 'input', input.size()\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.sigmoid(self.conv5(conv4))\n",
    "        return conv4.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNetGeneratorSkip(start_filter=32).cuda()\n",
    "discriminator = Discriminator(start_filter=32).cuda()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "clamp_lower = -0.03\n",
    "clamp_upper = 0.03\n",
    "loss_criterion = nn.MSELoss().cuda()\n",
    "save_dir = 'inpainting/samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(epoch, fake_images, real_images, real_examples_full):\n",
    "    j = np.random.randint(low=0, high=500)\n",
    "    real_examples_full, real_examples, fake_images = iterator.get_valid_minibatch(j, 32)\n",
    "    generator.eval()\n",
    "    reconstructions = generator(fake_images)\n",
    "    # fig = plt.figure(figsize=(20, 40))\n",
    "    # idx = 1\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    real = real_examples_full.data.cpu().numpy()\n",
    "    real_copy = copy.deepcopy(real)\n",
    "    real_copy[:, :, 16:48, 16:48] = reconstructions\n",
    "    real_copy = torch.from_numpy(real_copy)\n",
    "    real = torch.from_numpy(real)\n",
    "    out_tensor = torch.zeros(1, real_copy.size(1), real_copy.size(2), real_copy.size(3))\n",
    "    for zz, zzz in zip(real_copy[:10], real[:10]):\n",
    "        out_tensor = torch.cat([out_tensor, zz.unsqueeze(0)])\n",
    "        out_tensor = torch.cat([out_tensor, zzz.unsqueeze(0)])\n",
    "    vutils.save_image(out_tensor[1:], 'inpainting/samples/epoch_%d_samples.png' % (epoch), normalize=True, scale_each=True, nrow=4)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss_D: -0.142982 Loss_G: -0.074419\n",
      "[1] Loss_D: -0.090142 Loss_G: -0.178838\n",
      "[2] Loss_D: -0.133295 Loss_G: -0.119767\n",
      "[3] Loss_D: -0.162644 Loss_G: -0.108215\n",
      "[4] Loss_D: -0.203356 Loss_G: -0.102200\n",
      "[5] Loss_D: -0.183330 Loss_G: -0.109460\n",
      "[6] Loss_D: -0.193832 Loss_G: -0.100061\n",
      "[7] Loss_D: -0.205734 Loss_G: -0.106303\n",
      "[8] Loss_D: -0.206386 Loss_G: -0.109261\n",
      "[9] Loss_D: -0.206553 Loss_G: -0.111002\n",
      "[10] Loss_D: -0.206547 Loss_G: -0.111752\n",
      "[11] Loss_D: -0.206907 Loss_G: -0.112656\n",
      "[12] Loss_D: -0.206974 Loss_G: -0.113748\n",
      "[13] Loss_D: -0.207003 Loss_G: -0.115063\n",
      "[14] Loss_D: -0.206692 Loss_G: -0.116867\n",
      "[15] Loss_D: -0.135593 Loss_G: -0.196461\n",
      "[16] Loss_D: -0.074916 Loss_G: -0.166792\n",
      "[17] Loss_D: -0.175209 Loss_G: -0.108742\n",
      "[18] Loss_D: -0.176052 Loss_G: -0.110311\n",
      "[19] Loss_D: -0.203588 Loss_G: -0.113906\n",
      "[20] Loss_D: -0.205484 Loss_G: -0.117529\n",
      "[21] Loss_D: -0.199336 Loss_G: -0.135181\n",
      "[22] Loss_D: -0.117418 Loss_G: -0.156912\n",
      "[23] Loss_D: -0.203070 Loss_G: -0.116342\n",
      "[24] Loss_D: -0.202000 Loss_G: -0.120863\n",
      "[25] Loss_D: -0.104356 Loss_G: -0.179554\n",
      "[26] Loss_D: -0.187098 Loss_G: -0.119917\n",
      "[27] Loss_D: -0.203852 Loss_G: -0.119858\n",
      "[28] Loss_D: -0.206037 Loss_G: -0.121902\n",
      "[29] Loss_D: -0.206251 Loss_G: -0.122975\n",
      "[30] Loss_D: -0.206596 Loss_G: -0.123755\n",
      "[31] Loss_D: -0.206734 Loss_G: -0.124437\n",
      "[32] Loss_D: -0.206761 Loss_G: -0.124753\n",
      "[33] Loss_D: -0.206790 Loss_G: -0.125181\n",
      "[34] Loss_D: -0.206947 Loss_G: -0.125973\n",
      "[35] Loss_D: -0.206978 Loss_G: -0.125739\n",
      "[36] Loss_D: -0.206512 Loss_G: -0.125855\n",
      "[37] Loss_D: -0.207057 Loss_G: -0.125894\n",
      "[38] Loss_D: -0.207079 Loss_G: -0.126400\n",
      "[39] Loss_D: -0.206511 Loss_G: -0.126073\n",
      "[40] Loss_D: -0.207032 Loss_G: -0.125775\n",
      "[41] Loss_D: -0.207040 Loss_G: -0.126655\n",
      "[42] Loss_D: -0.207026 Loss_G: -0.127998\n",
      "[43] Loss_D: -0.206955 Loss_G: -0.128920\n",
      "[44] Loss_D: -0.207068 Loss_G: -0.129354\n",
      "[45] Loss_D: -0.207073 Loss_G: -0.129215\n",
      "[46] Loss_D: -0.196057 Loss_G: -0.145596\n",
      "[47] Loss_D: -0.049625 Loss_G: -0.217166\n",
      "[48] Loss_D: -0.092912 Loss_G: -0.163918\n",
      "[49] Loss_D: -0.201838 Loss_G: -0.109599\n",
      "[50] Loss_D: -0.204146 Loss_G: -0.118213\n",
      "[51] Loss_D: -0.123782 Loss_G: -0.153663\n",
      "[52] Loss_D: -0.134059 Loss_G: -0.137668\n",
      "[53] Loss_D: -0.202066 Loss_G: -0.114749\n",
      "[54] Loss_D: -0.204484 Loss_G: -0.120667\n",
      "[55] Loss_D: -0.205563 Loss_G: -0.123196\n",
      "[56] Loss_D: -0.205763 Loss_G: -0.124606\n",
      "[57] Loss_D: -0.206113 Loss_G: -0.125788\n",
      "[58] Loss_D: -0.205993 Loss_G: -0.126948\n",
      "[59] Loss_D: -0.138404 Loss_G: -0.162298\n",
      "[60] Loss_D: -0.154211 Loss_G: -0.126269\n",
      "[61] Loss_D: -0.201762 Loss_G: -0.123168\n",
      "[62] Loss_D: -0.204374 Loss_G: -0.125496\n",
      "[63] Loss_D: -0.204808 Loss_G: -0.127202\n",
      "[64] Loss_D: -0.204925 Loss_G: -0.127341\n",
      "[65] Loss_D: -0.205801 Loss_G: -0.127814\n",
      "[66] Loss_D: -0.206169 Loss_G: -0.128203\n",
      "[67] Loss_D: -0.206478 Loss_G: -0.128731\n",
      "[68] Loss_D: -0.206624 Loss_G: -0.129269\n",
      "[69] Loss_D: -0.206734 Loss_G: -0.129358\n",
      "[70] Loss_D: -0.206675 Loss_G: -0.129634\n",
      "[71] Loss_D: -0.206727 Loss_G: -0.129341\n",
      "[72] Loss_D: -0.206458 Loss_G: -0.129501\n",
      "[73] Loss_D: -0.206735 Loss_G: -0.129679\n",
      "[74] Loss_D: -0.206822 Loss_G: -0.130432\n",
      "[75] Loss_D: -0.206538 Loss_G: -0.130373\n",
      "[76] Loss_D: -0.196907 Loss_G: -0.145577\n",
      "[77] Loss_D: -0.080800 Loss_G: -0.170509\n",
      "[78] Loss_D: -0.158221 Loss_G: -0.128296\n",
      "[79] Loss_D: -0.142225 Loss_G: -0.121571\n",
      "[80] Loss_D: -0.202908 Loss_G: -0.109597\n",
      "[81] Loss_D: -0.204885 Loss_G: -0.120005\n",
      "[82] Loss_D: -0.205585 Loss_G: -0.124728\n",
      "[83] Loss_D: -0.163796 Loss_G: -0.144115\n",
      "[84] Loss_D: -0.086202 Loss_G: -0.203510\n",
      "[85] Loss_D: -0.142486 Loss_G: -0.127071\n",
      "[86] Loss_D: -0.198522 Loss_G: -0.126611\n",
      "[87] Loss_D: -0.201368 Loss_G: -0.127862\n",
      "[88] Loss_D: -0.042736 Loss_G: -0.140568\n",
      "[89] Loss_D: -0.032173 Loss_G: -0.181330\n",
      "[90] Loss_D: -0.103337 Loss_G: -0.131920\n",
      "[91] Loss_D: -0.164290 Loss_G: -0.125154\n",
      "[92] Loss_D: -0.169930 Loss_G: -0.134048\n",
      "[93] Loss_D: -0.175342 Loss_G: -0.144505\n",
      "[94] Loss_D: -0.165485 Loss_G: -0.156100\n",
      "[95] Loss_D: -0.186718 Loss_G: -0.130137\n",
      "[96] Loss_D: -0.133821 Loss_G: -0.164359\n",
      "[97] Loss_D: -0.178500 Loss_G: -0.129134\n",
      "[98] Loss_D: -0.190957 Loss_G: -0.129300\n",
      "[99] Loss_D: -0.201562 Loss_G: -0.129183\n",
      "[100] Loss_D: -0.202121 Loss_G: -0.130031\n",
      "[101] Loss_D: -0.174936 Loss_G: -0.132581\n",
      "[102] Loss_D: -0.191183 Loss_G: -0.130100\n",
      "[103] Loss_D: -0.182390 Loss_G: -0.132328\n",
      "[104] Loss_D: -0.200239 Loss_G: -0.129747\n",
      "[105] Loss_D: -0.202832 Loss_G: -0.130530\n",
      "[106] Loss_D: -0.203093 Loss_G: -0.131026\n",
      "[107] Loss_D: -0.202801 Loss_G: -0.131167\n",
      "[108] Loss_D: -0.203357 Loss_G: -0.131337\n",
      "[109] Loss_D: -0.204267 Loss_G: -0.131577\n",
      "[110] Loss_D: -0.204713 Loss_G: -0.131765\n",
      "[111] Loss_D: -0.205130 Loss_G: -0.132029\n",
      "[112] Loss_D: -0.205085 Loss_G: -0.132116\n",
      "[113] Loss_D: -0.205348 Loss_G: -0.132409\n",
      "[114] Loss_D: -0.205176 Loss_G: -0.132496\n",
      "[115] Loss_D: -0.205262 Loss_G: -0.132653\n",
      "[116] Loss_D: -0.126226 Loss_G: -0.166896\n",
      "[117] Loss_D: -0.122991 Loss_G: -0.170855\n",
      "[118] Loss_D: -0.116110 Loss_G: -0.141578\n",
      "[119] Loss_D: -0.186364 Loss_G: -0.121597\n",
      "[120] Loss_D: -0.086073 Loss_G: -0.148304\n",
      "[121] Loss_D: -0.145606 Loss_G: -0.129046\n",
      "[122] Loss_D: -0.179539 Loss_G: -0.128450\n",
      "[123] Loss_D: -0.178291 Loss_G: -0.129501\n",
      "[124] Loss_D: -0.185506 Loss_G: -0.130283\n",
      "[125] Loss_D: -0.200478 Loss_G: -0.130010\n",
      "[126] Loss_D: -0.147352 Loss_G: -0.132651\n",
      "[127] Loss_D: -0.146981 Loss_G: -0.140094\n",
      "[128] Loss_D: -0.180930 Loss_G: -0.130021\n",
      "[129] Loss_D: -0.179134 Loss_G: -0.131451\n",
      "[130] Loss_D: -0.200941 Loss_G: -0.130560\n",
      "[131] Loss_D: -0.202658 Loss_G: -0.131194\n",
      "[132] Loss_D: -0.203260 Loss_G: -0.131410\n",
      "[133] Loss_D: -0.101923 Loss_G: -0.150920\n",
      "[134] Loss_D: -0.177432 Loss_G: -0.130063\n",
      "[135] Loss_D: -0.197765 Loss_G: -0.130346\n",
      "[136] Loss_D: -0.162746 Loss_G: -0.133938\n",
      "[137] Loss_D: -0.193935 Loss_G: -0.130532\n",
      "[138] Loss_D: -0.203177 Loss_G: -0.130791\n",
      "[139] Loss_D: -0.204836 Loss_G: -0.131495\n",
      "[140] Loss_D: -0.205147 Loss_G: -0.131870\n",
      "[141] Loss_D: -0.205145 Loss_G: -0.132215\n",
      "[142] Loss_D: -0.205337 Loss_G: -0.132496\n",
      "[143] Loss_D: -0.111591 Loss_G: -0.175635\n",
      "[144] Loss_D: -0.129636 Loss_G: -0.141272\n",
      "[145] Loss_D: -0.125482 Loss_G: -0.135988\n",
      "[146] Loss_D: -0.168774 Loss_G: -0.132132\n",
      "[147] Loss_D: -0.169599 Loss_G: -0.149821\n",
      "[148] Loss_D: -0.171528 Loss_G: -0.134597\n",
      "[149] Loss_D: -0.171691 Loss_G: -0.134006\n",
      "[150] Loss_D: -0.166505 Loss_G: -0.149740\n",
      "[151] Loss_D: -0.200843 Loss_G: -0.131409\n",
      "[152] Loss_D: -0.202993 Loss_G: -0.132150\n",
      "[153] Loss_D: -0.141326 Loss_G: -0.159705\n",
      "[154] Loss_D: -0.181022 Loss_G: -0.131836\n",
      "[155] Loss_D: -0.166656 Loss_G: -0.133280\n",
      "[156] Loss_D: -0.159201 Loss_G: -0.135598\n",
      "[157] Loss_D: -0.193462 Loss_G: -0.131200\n",
      "[158] Loss_D: -0.201489 Loss_G: -0.132250\n",
      "[159] Loss_D: -0.174145 Loss_G: -0.135647\n",
      "[160] Loss_D: -0.170552 Loss_G: -0.132750\n",
      "[161] Loss_D: -0.189727 Loss_G: -0.132721\n",
      "[162] Loss_D: -0.182935 Loss_G: -0.134681\n",
      "[163] Loss_D: -0.191305 Loss_G: -0.129090\n",
      "[164] Loss_D: -0.201271 Loss_G: -0.131392\n",
      "[165] Loss_D: -0.202688 Loss_G: -0.132372\n",
      "[166] Loss_D: -0.203396 Loss_G: -0.132859\n",
      "[167] Loss_D: -0.119039 Loss_G: -0.154756\n",
      "[168] Loss_D: -0.179959 Loss_G: -0.130617\n",
      "[169] Loss_D: -0.173045 Loss_G: -0.135451\n",
      "[170] Loss_D: -0.176355 Loss_G: -0.140546\n",
      "[171] Loss_D: -0.139198 Loss_G: -0.146465\n",
      "[172] Loss_D: -0.160698 Loss_G: -0.134861\n",
      "[173] Loss_D: -0.176791 Loss_G: -0.134160\n",
      "[174] Loss_D: -0.164321 Loss_G: -0.145939\n",
      "[175] Loss_D: -0.183346 Loss_G: -0.133440\n",
      "[176] Loss_D: -0.196061 Loss_G: -0.133677\n",
      "[177] Loss_D: -0.159606 Loss_G: -0.136580\n",
      "[178] Loss_D: -0.194925 Loss_G: -0.133056\n",
      "[179] Loss_D: -0.175837 Loss_G: -0.136839\n",
      "[180] Loss_D: -0.181298 Loss_G: -0.134941\n",
      "[181] Loss_D: -0.184625 Loss_G: -0.134153\n",
      "[182] Loss_D: -0.166477 Loss_G: -0.141930\n",
      "[183] Loss_D: -0.182012 Loss_G: -0.134922\n",
      "[184] Loss_D: -0.192912 Loss_G: -0.134382\n",
      "[185] Loss_D: -0.186496 Loss_G: -0.134259\n",
      "[186] Loss_D: -0.192255 Loss_G: -0.133937\n",
      "[187] Loss_D: -0.173142 Loss_G: -0.137003\n",
      "[188] Loss_D: -0.186915 Loss_G: -0.134231\n",
      "[189] Loss_D: -0.183676 Loss_G: -0.137006\n",
      "[190] Loss_D: -0.190104 Loss_G: -0.134640\n",
      "[191] Loss_D: -0.189345 Loss_G: -0.135991\n",
      "[192] Loss_D: -0.181499 Loss_G: -0.134680\n",
      "[193] Loss_D: -0.192742 Loss_G: -0.133912\n",
      "[194] Loss_D: -0.190773 Loss_G: -0.134469\n",
      "[195] Loss_D: -0.183495 Loss_G: -0.137372\n",
      "[196] Loss_D: -0.187295 Loss_G: -0.134636\n",
      "[197] Loss_D: -0.177952 Loss_G: -0.142245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198] Loss_D: -0.172103 Loss_G: -0.137011\n",
      "[199] Loss_D: -0.181594 Loss_G: -0.137561\n",
      "[200] Loss_D: -0.192436 Loss_G: -0.134374\n",
      "[201] Loss_D: -0.187553 Loss_G: -0.139131\n",
      "[202] Loss_D: -0.185337 Loss_G: -0.135501\n",
      "[203] Loss_D: -0.179222 Loss_G: -0.136124\n",
      "[204] Loss_D: -0.187583 Loss_G: -0.135669\n",
      "[205] Loss_D: -0.193900 Loss_G: -0.135484\n",
      "[206] Loss_D: -0.194315 Loss_G: -0.135483\n",
      "[207] Loss_D: -0.198530 Loss_G: -0.135603\n",
      "[208] Loss_D: -0.169545 Loss_G: -0.152993\n",
      "[209] Loss_D: -0.146384 Loss_G: -0.150541\n",
      "[210] Loss_D: -0.161828 Loss_G: -0.137083\n",
      "[211] Loss_D: -0.189448 Loss_G: -0.135575\n",
      "[212] Loss_D: -0.187709 Loss_G: -0.136421\n",
      "[213] Loss_D: -0.189286 Loss_G: -0.135585\n",
      "[214] Loss_D: -0.171015 Loss_G: -0.143254\n",
      "[215] Loss_D: -0.180383 Loss_G: -0.136445\n",
      "[216] Loss_D: -0.193847 Loss_G: -0.136041\n",
      "[217] Loss_D: -0.190648 Loss_G: -0.137012\n",
      "[218] Loss_D: -0.184472 Loss_G: -0.135881\n",
      "[219] Loss_D: -0.193689 Loss_G: -0.135717\n",
      "[220] Loss_D: -0.171983 Loss_G: -0.138609\n",
      "[221] Loss_D: -0.188930 Loss_G: -0.136310\n",
      "[222] Loss_D: -0.178548 Loss_G: -0.138562\n",
      "[223] Loss_D: -0.187902 Loss_G: -0.135688\n",
      "[224] Loss_D: -0.185274 Loss_G: -0.137087\n",
      "[225] Loss_D: -0.190450 Loss_G: -0.135640\n",
      "[226] Loss_D: -0.195180 Loss_G: -0.135596\n",
      "[227] Loss_D: -0.178038 Loss_G: -0.137776\n",
      "[228] Loss_D: -0.180815 Loss_G: -0.136458\n",
      "[229] Loss_D: -0.189112 Loss_G: -0.135948\n",
      "[230] Loss_D: -0.194429 Loss_G: -0.135951\n",
      "[231] Loss_D: -0.173248 Loss_G: -0.140341\n",
      "[232] Loss_D: -0.182188 Loss_G: -0.137361\n",
      "[233] Loss_D: -0.186275 Loss_G: -0.136649\n",
      "[234] Loss_D: -0.190281 Loss_G: -0.136432\n",
      "[235] Loss_D: -0.182966 Loss_G: -0.139661\n",
      "[236] Loss_D: -0.165427 Loss_G: -0.138276\n",
      "[237] Loss_D: -0.184400 Loss_G: -0.137182\n",
      "[238] Loss_D: -0.189627 Loss_G: -0.136924\n",
      "[239] Loss_D: -0.190085 Loss_G: -0.137034\n",
      "[240] Loss_D: -0.173497 Loss_G: -0.139932\n",
      "[241] Loss_D: -0.192645 Loss_G: -0.136997\n",
      "[242] Loss_D: -0.181385 Loss_G: -0.138907\n",
      "[243] Loss_D: -0.192052 Loss_G: -0.137035\n",
      "[244] Loss_D: -0.192515 Loss_G: -0.137519\n",
      "[245] Loss_D: -0.187751 Loss_G: -0.137538\n",
      "[246] Loss_D: -0.191397 Loss_G: -0.137231\n",
      "[247] Loss_D: -0.169891 Loss_G: -0.141083\n",
      "[248] Loss_D: -0.171778 Loss_G: -0.139281\n",
      "[249] Loss_D: -0.187488 Loss_G: -0.137094\n",
      "[250] Loss_D: -0.186733 Loss_G: -0.138373\n",
      "[251] Loss_D: -0.192237 Loss_G: -0.137294\n",
      "[252] Loss_D: -0.182403 Loss_G: -0.137943\n",
      "[253] Loss_D: -0.190347 Loss_G: -0.137503\n",
      "[254] Loss_D: -0.190396 Loss_G: -0.137605\n",
      "[255] Loss_D: -0.190297 Loss_G: -0.137801\n",
      "[256] Loss_D: -0.174985 Loss_G: -0.139468\n",
      "[257] Loss_D: -0.190268 Loss_G: -0.137527\n",
      "[258] Loss_D: -0.188234 Loss_G: -0.138157\n",
      "[259] Loss_D: -0.183972 Loss_G: -0.138389\n",
      "[260] Loss_D: -0.175331 Loss_G: -0.140135\n",
      "[261] Loss_D: -0.186059 Loss_G: -0.138193\n",
      "[262] Loss_D: -0.184991 Loss_G: -0.138034\n",
      "[263] Loss_D: -0.185175 Loss_G: -0.138165\n",
      "[264] Loss_D: -0.188200 Loss_G: -0.138170\n",
      "[265] Loss_D: -0.189552 Loss_G: -0.137732\n",
      "[266] Loss_D: -0.183055 Loss_G: -0.138857\n",
      "[267] Loss_D: -0.186329 Loss_G: -0.138075\n",
      "[268] Loss_D: -0.186941 Loss_G: -0.138218\n",
      "[269] Loss_D: -0.189192 Loss_G: -0.138034\n",
      "[270] Loss_D: -0.182591 Loss_G: -0.138557\n",
      "[271] Loss_D: -0.175929 Loss_G: -0.139471\n",
      "[272] Loss_D: -0.187783 Loss_G: -0.137857\n",
      "[273] Loss_D: -0.185633 Loss_G: -0.138025\n",
      "[274] Loss_D: -0.184795 Loss_G: -0.138079\n",
      "[275] Loss_D: -0.189614 Loss_G: -0.137611\n",
      "[276] Loss_D: -0.186505 Loss_G: -0.137895\n",
      "[277] Loss_D: -0.184622 Loss_G: -0.138812\n",
      "[278] Loss_D: -0.183122 Loss_G: -0.138523\n",
      "[279] Loss_D: -0.174147 Loss_G: -0.140844\n",
      "[280] Loss_D: -0.173045 Loss_G: -0.139843\n",
      "[281] Loss_D: -0.185305 Loss_G: -0.139760\n",
      "[282] Loss_D: -0.175594 Loss_G: -0.138866\n",
      "[283] Loss_D: -0.187974 Loss_G: -0.138361\n",
      "[284] Loss_D: -0.185059 Loss_G: -0.138779\n",
      "[285] Loss_D: -0.188687 Loss_G: -0.138436\n",
      "[286] Loss_D: -0.188041 Loss_G: -0.138641\n",
      "[287] Loss_D: -0.179786 Loss_G: -0.139114\n",
      "[288] Loss_D: -0.187428 Loss_G: -0.138761\n",
      "[289] Loss_D: -0.186632 Loss_G: -0.138697\n",
      "[290] Loss_D: -0.183975 Loss_G: -0.139408\n",
      "[291] Loss_D: -0.182414 Loss_G: -0.138881\n",
      "[292] Loss_D: -0.181539 Loss_G: -0.139812\n",
      "[293] Loss_D: -0.189404 Loss_G: -0.138766\n",
      "[294] Loss_D: -0.185946 Loss_G: -0.139123\n",
      "[295] Loss_D: -0.175809 Loss_G: -0.141206\n",
      "[296] Loss_D: -0.188794 Loss_G: -0.138728\n",
      "[297] Loss_D: -0.186080 Loss_G: -0.138710\n",
      "[298] Loss_D: -0.188970 Loss_G: -0.138761\n",
      "[299] Loss_D: -0.186618 Loss_G: -0.138929\n",
      "[300] Loss_D: -0.181912 Loss_G: -0.139895\n",
      "[301] Loss_D: -0.177090 Loss_G: -0.139533\n",
      "[302] Loss_D: -0.186544 Loss_G: -0.139141\n",
      "[303] Loss_D: -0.184607 Loss_G: -0.139102\n",
      "[304] Loss_D: -0.190993 Loss_G: -0.138585\n",
      "[305] Loss_D: -0.189683 Loss_G: -0.138693\n",
      "[306] Loss_D: -0.183462 Loss_G: -0.139411\n",
      "[307] Loss_D: -0.183364 Loss_G: -0.139368\n",
      "[308] Loss_D: -0.184549 Loss_G: -0.139122\n",
      "[309] Loss_D: -0.180150 Loss_G: -0.139691\n",
      "[310] Loss_D: -0.188229 Loss_G: -0.139245\n",
      "[311] Loss_D: -0.183319 Loss_G: -0.139761\n",
      "[312] Loss_D: -0.184479 Loss_G: -0.139639\n",
      "[313] Loss_D: -0.189982 Loss_G: -0.139285\n",
      "[314] Loss_D: -0.178037 Loss_G: -0.141105\n",
      "[315] Loss_D: -0.191320 Loss_G: -0.139079\n",
      "[316] Loss_D: -0.188982 Loss_G: -0.139173\n",
      "[317] Loss_D: -0.191972 Loss_G: -0.139293\n",
      "[318] Loss_D: -0.176226 Loss_G: -0.140798\n",
      "[319] Loss_D: -0.165657 Loss_G: -0.141438\n",
      "[320] Loss_D: -0.184634 Loss_G: -0.139507\n",
      "[321] Loss_D: -0.186191 Loss_G: -0.139351\n",
      "[322] Loss_D: -0.189373 Loss_G: -0.139246\n",
      "[323] Loss_D: -0.190968 Loss_G: -0.139415\n",
      "[324] Loss_D: -0.187362 Loss_G: -0.139705\n",
      "[325] Loss_D: -0.184268 Loss_G: -0.140046\n",
      "[326] Loss_D: -0.188574 Loss_G: -0.139981\n",
      "[327] Loss_D: -0.191632 Loss_G: -0.139402\n",
      "[328] Loss_D: -0.181112 Loss_G: -0.140690\n",
      "[329] Loss_D: -0.181227 Loss_G: -0.140096\n",
      "[330] Loss_D: -0.194365 Loss_G: -0.139078\n",
      "[331] Loss_D: -0.188173 Loss_G: -0.139699\n",
      "[332] Loss_D: -0.189843 Loss_G: -0.139435\n",
      "[333] Loss_D: -0.190793 Loss_G: -0.139677\n",
      "[334] Loss_D: -0.189217 Loss_G: -0.139532\n",
      "[335] Loss_D: -0.171811 Loss_G: -0.141884\n",
      "[336] Loss_D: -0.173089 Loss_G: -0.140766\n",
      "[337] Loss_D: -0.179233 Loss_G: -0.140631\n",
      "[338] Loss_D: -0.190315 Loss_G: -0.139100\n",
      "[339] Loss_D: -0.177718 Loss_G: -0.141792\n",
      "[340] Loss_D: -0.189341 Loss_G: -0.139371\n",
      "[341] Loss_D: -0.193831 Loss_G: -0.139503\n",
      "[342] Loss_D: -0.173603 Loss_G: -0.142740\n",
      "[343] Loss_D: -0.191410 Loss_G: -0.139258\n",
      "[344] Loss_D: -0.189601 Loss_G: -0.139867\n",
      "[345] Loss_D: -0.186550 Loss_G: -0.140069\n",
      "[346] Loss_D: -0.191250 Loss_G: -0.139664\n",
      "[347] Loss_D: -0.178926 Loss_G: -0.141641\n",
      "[348] Loss_D: -0.190266 Loss_G: -0.139774\n",
      "[349] Loss_D: -0.186356 Loss_G: -0.140516\n",
      "[350] Loss_D: -0.190226 Loss_G: -0.139844\n",
      "[351] Loss_D: -0.187384 Loss_G: -0.140247\n",
      "[352] Loss_D: -0.179702 Loss_G: -0.141462\n",
      "[353] Loss_D: -0.184824 Loss_G: -0.140398\n",
      "[354] Loss_D: -0.174717 Loss_G: -0.141770\n",
      "[355] Loss_D: -0.190889 Loss_G: -0.140099\n",
      "[356] Loss_D: -0.187372 Loss_G: -0.140694\n",
      "[357] Loss_D: -0.187331 Loss_G: -0.140302\n",
      "[358] Loss_D: -0.189740 Loss_G: -0.139946\n",
      "[359] Loss_D: -0.187108 Loss_G: -0.140446\n",
      "[360] Loss_D: -0.181414 Loss_G: -0.141198\n",
      "[361] Loss_D: -0.167031 Loss_G: -0.142973\n",
      "[362] Loss_D: -0.191227 Loss_G: -0.139602\n",
      "[363] Loss_D: -0.180630 Loss_G: -0.141700\n",
      "[364] Loss_D: -0.189857 Loss_G: -0.139976\n",
      "[365] Loss_D: -0.190436 Loss_G: -0.140247\n",
      "[366] Loss_D: -0.184816 Loss_G: -0.141035\n",
      "[367] Loss_D: -0.168459 Loss_G: -0.144201\n",
      "[368] Loss_D: -0.184727 Loss_G: -0.140369\n",
      "[369] Loss_D: -0.189792 Loss_G: -0.139562\n",
      "[370] Loss_D: -0.185815 Loss_G: -0.140506\n",
      "[371] Loss_D: -0.191653 Loss_G: -0.140011\n",
      "[372] Loss_D: -0.187246 Loss_G: -0.140019\n",
      "[373] Loss_D: -0.191778 Loss_G: -0.139867\n",
      "[374] Loss_D: -0.155694 Loss_G: -0.146623\n",
      "[375] Loss_D: -0.168706 Loss_G: -0.141249\n",
      "[376] Loss_D: -0.184424 Loss_G: -0.141020\n",
      "[377] Loss_D: -0.180851 Loss_G: -0.140572\n",
      "[378] Loss_D: -0.192062 Loss_G: -0.139637\n",
      "[379] Loss_D: -0.190378 Loss_G: -0.139998\n",
      "[380] Loss_D: -0.174055 Loss_G: -0.141837\n",
      "[381] Loss_D: -0.188918 Loss_G: -0.139738\n",
      "[382] Loss_D: -0.186359 Loss_G: -0.140460\n",
      "[383] Loss_D: -0.184507 Loss_G: -0.141419\n",
      "[384] Loss_D: -0.188831 Loss_G: -0.140166\n",
      "[385] Loss_D: -0.190708 Loss_G: -0.140288\n",
      "[386] Loss_D: -0.188820 Loss_G: -0.140515\n",
      "[387] Loss_D: -0.176989 Loss_G: -0.142007\n",
      "[388] Loss_D: -0.178996 Loss_G: -0.141379\n",
      "[389] Loss_D: -0.186672 Loss_G: -0.140260\n",
      "[390] Loss_D: -0.189734 Loss_G: -0.140315\n",
      "[391] Loss_D: -0.191561 Loss_G: -0.140134\n",
      "[392] Loss_D: -0.182591 Loss_G: -0.141180\n",
      "[393] Loss_D: -0.167495 Loss_G: -0.143094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394] Loss_D: -0.176692 Loss_G: -0.141012\n",
      "[395] Loss_D: -0.190749 Loss_G: -0.139814\n",
      "[396] Loss_D: -0.194257 Loss_G: -0.140011\n",
      "[397] Loss_D: -0.186637 Loss_G: -0.140980\n",
      "[398] Loss_D: -0.180367 Loss_G: -0.141540\n",
      "[399] Loss_D: -0.170112 Loss_G: -0.142880\n",
      "[400] Loss_D: -0.191260 Loss_G: -0.140564\n",
      "[401] Loss_D: -0.187989 Loss_G: -0.140496\n",
      "[402] Loss_D: -0.187172 Loss_G: -0.140770\n",
      "[403] Loss_D: -0.188975 Loss_G: -0.140834\n",
      "[404] Loss_D: -0.187029 Loss_G: -0.140660\n",
      "[405] Loss_D: -0.181730 Loss_G: -0.141848\n",
      "[406] Loss_D: -0.181727 Loss_G: -0.140775\n",
      "[407] Loss_D: -0.175520 Loss_G: -0.143795\n",
      "[408] Loss_D: -0.173456 Loss_G: -0.143982\n",
      "[409] Loss_D: -0.189844 Loss_G: -0.139951\n",
      "[410] Loss_D: -0.189271 Loss_G: -0.140474\n",
      "[411] Loss_D: -0.187509 Loss_G: -0.140389\n",
      "[412] Loss_D: -0.174084 Loss_G: -0.142898\n",
      "[413] Loss_D: -0.187831 Loss_G: -0.140380\n",
      "[414] Loss_D: -0.183054 Loss_G: -0.141260\n",
      "[415] Loss_D: -0.189444 Loss_G: -0.140603\n",
      "[416] Loss_D: -0.184933 Loss_G: -0.140959\n",
      "[417] Loss_D: -0.171895 Loss_G: -0.145546\n",
      "[418] Loss_D: -0.176339 Loss_G: -0.142660\n",
      "[419] Loss_D: -0.182323 Loss_G: -0.140107\n",
      "[420] Loss_D: -0.186976 Loss_G: -0.140278\n",
      "[421] Loss_D: -0.171976 Loss_G: -0.142741\n",
      "[422] Loss_D: -0.190415 Loss_G: -0.139971\n",
      "[423] Loss_D: -0.192107 Loss_G: -0.140148\n",
      "[424] Loss_D: -0.177659 Loss_G: -0.143073\n",
      "[425] Loss_D: -0.189060 Loss_G: -0.140323\n",
      "[426] Loss_D: -0.189668 Loss_G: -0.140895\n",
      "[427] Loss_D: -0.186041 Loss_G: -0.140874\n",
      "[428] Loss_D: -0.188044 Loss_G: -0.141135\n",
      "[429] Loss_D: -0.188951 Loss_G: -0.140678\n",
      "[430] Loss_D: -0.163931 Loss_G: -0.147450\n",
      "[431] Loss_D: -0.188354 Loss_G: -0.140189\n",
      "[432] Loss_D: -0.188988 Loss_G: -0.140473\n",
      "[433] Loss_D: -0.173022 Loss_G: -0.144700\n",
      "[434] Loss_D: -0.177678 Loss_G: -0.142130\n",
      "[435] Loss_D: -0.180224 Loss_G: -0.142386\n",
      "[436] Loss_D: -0.175403 Loss_G: -0.142403\n",
      "[437] Loss_D: -0.183492 Loss_G: -0.141086\n",
      "[438] Loss_D: -0.192315 Loss_G: -0.140642\n",
      "[439] Loss_D: -0.178841 Loss_G: -0.143028\n",
      "[440] Loss_D: -0.181129 Loss_G: -0.142145\n",
      "[441] Loss_D: -0.177142 Loss_G: -0.142234\n",
      "[442] Loss_D: -0.184881 Loss_G: -0.140723\n",
      "[443] Loss_D: -0.182932 Loss_G: -0.141857\n",
      "[444] Loss_D: -0.188333 Loss_G: -0.140921\n",
      "[445] Loss_D: -0.189101 Loss_G: -0.141074\n",
      "[446] Loss_D: -0.173233 Loss_G: -0.143209\n",
      "[447] Loss_D: -0.190591 Loss_G: -0.140686\n",
      "[448] Loss_D: -0.181616 Loss_G: -0.142466\n",
      "[449] Loss_D: -0.178953 Loss_G: -0.141108\n",
      "[450] Loss_D: -0.189751 Loss_G: -0.140729\n",
      "[451] Loss_D: -0.189748 Loss_G: -0.140717\n",
      "[452] Loss_D: -0.164323 Loss_G: -0.144783\n",
      "[453] Loss_D: -0.179097 Loss_G: -0.141366\n",
      "[454] Loss_D: -0.193051 Loss_G: -0.140454\n",
      "[455] Loss_D: -0.183290 Loss_G: -0.141686\n",
      "[456] Loss_D: -0.190511 Loss_G: -0.140782\n",
      "[457] Loss_D: -0.186605 Loss_G: -0.141165\n",
      "[458] Loss_D: -0.186841 Loss_G: -0.140740\n",
      "[459] Loss_D: -0.191216 Loss_G: -0.140954\n",
      "[460] Loss_D: -0.186134 Loss_G: -0.141311\n",
      "[461] Loss_D: -0.163150 Loss_G: -0.144525\n",
      "[462] Loss_D: -0.181487 Loss_G: -0.140842\n",
      "[463] Loss_D: -0.190333 Loss_G: -0.140605\n",
      "[464] Loss_D: -0.190858 Loss_G: -0.140639\n",
      "[465] Loss_D: -0.177707 Loss_G: -0.142496\n",
      "[466] Loss_D: -0.176755 Loss_G: -0.141717\n",
      "[467] Loss_D: -0.186823 Loss_G: -0.140812\n",
      "[468] Loss_D: -0.190604 Loss_G: -0.140928\n",
      "[469] Loss_D: -0.172700 Loss_G: -0.143586\n",
      "[470] Loss_D: -0.170014 Loss_G: -0.144123\n",
      "[471] Loss_D: -0.183600 Loss_G: -0.140989\n",
      "[472] Loss_D: -0.185241 Loss_G: -0.141290\n",
      "[473] Loss_D: -0.168022 Loss_G: -0.144179\n",
      "[474] Loss_D: -0.188381 Loss_G: -0.141153\n",
      "[475] Loss_D: -0.185073 Loss_G: -0.141612\n",
      "[476] Loss_D: -0.181656 Loss_G: -0.141190\n",
      "[477] Loss_D: -0.191121 Loss_G: -0.141009\n",
      "[478] Loss_D: -0.181869 Loss_G: -0.142671\n",
      "[479] Loss_D: -0.183885 Loss_G: -0.141612\n",
      "[480] Loss_D: -0.186528 Loss_G: -0.141363\n",
      "[481] Loss_D: -0.176953 Loss_G: -0.142514\n",
      "[482] Loss_D: -0.184830 Loss_G: -0.141679\n",
      "[483] Loss_D: -0.189050 Loss_G: -0.141503\n",
      "[484] Loss_D: -0.171341 Loss_G: -0.144165\n",
      "[485] Loss_D: -0.173130 Loss_G: -0.142069\n",
      "[486] Loss_D: -0.187588 Loss_G: -0.141079\n",
      "[487] Loss_D: -0.177077 Loss_G: -0.142846\n",
      "[488] Loss_D: -0.165571 Loss_G: -0.146239\n",
      "[489] Loss_D: -0.183859 Loss_G: -0.141468\n",
      "[490] Loss_D: -0.188016 Loss_G: -0.141058\n",
      "[491] Loss_D: -0.171959 Loss_G: -0.142971\n",
      "[492] Loss_D: -0.186910 Loss_G: -0.141114\n",
      "[493] Loss_D: -0.180709 Loss_G: -0.142389\n",
      "[494] Loss_D: -0.175584 Loss_G: -0.142437\n",
      "[495] Loss_D: -0.182433 Loss_G: -0.141651\n",
      "[496] Loss_D: -0.187574 Loss_G: -0.141023\n",
      "[497] Loss_D: -0.185569 Loss_G: -0.141341\n",
      "[498] Loss_D: -0.181915 Loss_G: -0.141830\n",
      "[499] Loss_D: -0.166046 Loss_G: -0.145088\n",
      "[500] Loss_D: -0.187555 Loss_G: -0.140710\n",
      "[501] Loss_D: -0.168183 Loss_G: -0.145574\n",
      "[502] Loss_D: -0.188484 Loss_G: -0.141170\n",
      "[503] Loss_D: -0.180259 Loss_G: -0.141979\n",
      "[504] Loss_D: -0.184993 Loss_G: -0.141762\n",
      "[505] Loss_D: -0.182082 Loss_G: -0.141417\n",
      "[506] Loss_D: -0.183538 Loss_G: -0.142923\n",
      "[507] Loss_D: -0.182860 Loss_G: -0.141541\n",
      "[508] Loss_D: -0.188307 Loss_G: -0.141511\n",
      "[509] Loss_D: -0.182485 Loss_G: -0.141673\n",
      "[510] Loss_D: -0.183457 Loss_G: -0.141688\n",
      "[511] Loss_D: -0.185316 Loss_G: -0.142548\n",
      "[512] Loss_D: -0.172968 Loss_G: -0.143593\n",
      "[513] Loss_D: -0.189459 Loss_G: -0.141281\n",
      "[514] Loss_D: -0.176923 Loss_G: -0.143249\n",
      "[515] Loss_D: -0.181027 Loss_G: -0.142191\n",
      "[516] Loss_D: -0.185144 Loss_G: -0.141769\n",
      "[517] Loss_D: -0.155263 Loss_G: -0.149145\n",
      "[518] Loss_D: -0.181248 Loss_G: -0.140427\n",
      "[519] Loss_D: -0.188146 Loss_G: -0.140891\n",
      "[520] Loss_D: -0.186523 Loss_G: -0.141099\n",
      "[521] Loss_D: -0.187973 Loss_G: -0.141348\n",
      "[522] Loss_D: -0.186398 Loss_G: -0.141652\n",
      "[523] Loss_D: -0.186510 Loss_G: -0.141299\n",
      "[524] Loss_D: -0.183941 Loss_G: -0.142383\n",
      "[525] Loss_D: -0.185301 Loss_G: -0.142344\n",
      "[526] Loss_D: -0.178910 Loss_G: -0.142447\n",
      "[527] Loss_D: -0.189497 Loss_G: -0.141592\n",
      "[528] Loss_D: -0.185743 Loss_G: -0.141583\n",
      "[529] Loss_D: -0.179734 Loss_G: -0.142452\n",
      "[530] Loss_D: -0.169644 Loss_G: -0.145289\n",
      "[531] Loss_D: -0.172814 Loss_G: -0.144263\n",
      "[532] Loss_D: -0.181572 Loss_G: -0.141627\n",
      "[533] Loss_D: -0.188686 Loss_G: -0.141892\n",
      "[534] Loss_D: -0.171076 Loss_G: -0.144569\n",
      "[535] Loss_D: -0.179139 Loss_G: -0.142705\n",
      "[536] Loss_D: -0.186084 Loss_G: -0.142019\n",
      "[537] Loss_D: -0.178840 Loss_G: -0.142488\n",
      "[538] Loss_D: -0.185239 Loss_G: -0.141227\n",
      "[539] Loss_D: -0.186662 Loss_G: -0.141603\n",
      "[540] Loss_D: -0.183935 Loss_G: -0.142256\n",
      "[541] Loss_D: -0.184998 Loss_G: -0.141527\n",
      "[542] Loss_D: -0.187838 Loss_G: -0.141683\n",
      "[543] Loss_D: -0.170894 Loss_G: -0.145296\n",
      "[544] Loss_D: -0.176698 Loss_G: -0.142779\n",
      "[545] Loss_D: -0.188567 Loss_G: -0.141439\n",
      "[546] Loss_D: -0.178581 Loss_G: -0.142472\n",
      "[547] Loss_D: -0.184993 Loss_G: -0.142111\n",
      "[548] Loss_D: -0.180571 Loss_G: -0.143053\n",
      "[549] Loss_D: -0.171352 Loss_G: -0.145214\n",
      "[550] Loss_D: -0.180188 Loss_G: -0.143433\n",
      "[551] Loss_D: -0.178284 Loss_G: -0.142031\n",
      "[552] Loss_D: -0.187212 Loss_G: -0.141630\n",
      "[553] Loss_D: -0.189017 Loss_G: -0.142040\n",
      "[554] Loss_D: -0.164226 Loss_G: -0.144893\n",
      "[555] Loss_D: -0.183197 Loss_G: -0.141700\n",
      "[556] Loss_D: -0.186191 Loss_G: -0.142072\n",
      "[557] Loss_D: -0.180236 Loss_G: -0.142471\n",
      "[558] Loss_D: -0.163730 Loss_G: -0.145669\n",
      "[559] Loss_D: -0.180625 Loss_G: -0.142141\n",
      "[560] Loss_D: -0.185087 Loss_G: -0.142139\n",
      "[561] Loss_D: -0.182377 Loss_G: -0.142368\n",
      "[562] Loss_D: -0.181569 Loss_G: -0.142252\n",
      "[563] Loss_D: -0.186841 Loss_G: -0.142102\n",
      "[564] Loss_D: -0.179409 Loss_G: -0.142787\n",
      "[565] Loss_D: -0.181300 Loss_G: -0.142831\n",
      "[566] Loss_D: -0.179290 Loss_G: -0.142418\n",
      "[567] Loss_D: -0.185665 Loss_G: -0.141702\n",
      "[568] Loss_D: -0.182275 Loss_G: -0.142520\n",
      "[569] Loss_D: -0.184737 Loss_G: -0.141812\n",
      "[570] Loss_D: -0.185836 Loss_G: -0.142319\n",
      "[571] Loss_D: -0.177545 Loss_G: -0.142868\n",
      "[572] Loss_D: -0.185038 Loss_G: -0.141929\n",
      "[573] Loss_D: -0.182618 Loss_G: -0.142490\n",
      "[574] Loss_D: -0.180243 Loss_G: -0.143050\n",
      "[575] Loss_D: -0.183780 Loss_G: -0.142019\n",
      "[576] Loss_D: -0.174794 Loss_G: -0.143389\n",
      "[577] Loss_D: -0.172603 Loss_G: -0.143547\n",
      "[578] Loss_D: -0.186010 Loss_G: -0.142113\n",
      "[579] Loss_D: -0.182763 Loss_G: -0.142516\n",
      "[580] Loss_D: -0.180301 Loss_G: -0.142267\n",
      "[581] Loss_D: -0.186059 Loss_G: -0.142090\n",
      "[582] Loss_D: -0.184087 Loss_G: -0.142190\n",
      "[583] Loss_D: -0.181013 Loss_G: -0.142949\n",
      "[584] Loss_D: -0.185353 Loss_G: -0.142419\n",
      "[585] Loss_D: -0.178519 Loss_G: -0.142926\n",
      "[586] Loss_D: -0.180109 Loss_G: -0.142326\n",
      "[587] Loss_D: -0.182180 Loss_G: -0.142622\n",
      "[588] Loss_D: -0.185227 Loss_G: -0.142381\n",
      "[589] Loss_D: -0.174228 Loss_G: -0.143972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[590] Loss_D: -0.182813 Loss_G: -0.142329\n",
      "[591] Loss_D: -0.182009 Loss_G: -0.142787\n",
      "[592] Loss_D: -0.176902 Loss_G: -0.142861\n",
      "[593] Loss_D: -0.182614 Loss_G: -0.142255\n",
      "[594] Loss_D: -0.186154 Loss_G: -0.142466\n",
      "[595] Loss_D: -0.181363 Loss_G: -0.143384\n",
      "[596] Loss_D: -0.176306 Loss_G: -0.142787\n",
      "[597] Loss_D: -0.182378 Loss_G: -0.142494\n",
      "[598] Loss_D: -0.181469 Loss_G: -0.142577\n",
      "[599] Loss_D: -0.178237 Loss_G: -0.142980\n",
      "[600] Loss_D: -0.187619 Loss_G: -0.142145\n",
      "[601] Loss_D: -0.178145 Loss_G: -0.143028\n",
      "[602] Loss_D: -0.176237 Loss_G: -0.144059\n",
      "[603] Loss_D: -0.180407 Loss_G: -0.142123\n",
      "[604] Loss_D: -0.181483 Loss_G: -0.142558\n",
      "[605] Loss_D: -0.177458 Loss_G: -0.142939\n",
      "[606] Loss_D: -0.179527 Loss_G: -0.143242\n",
      "[607] Loss_D: -0.182778 Loss_G: -0.142260\n",
      "[608] Loss_D: -0.179117 Loss_G: -0.142589\n",
      "[609] Loss_D: -0.181099 Loss_G: -0.142107\n",
      "[610] Loss_D: -0.183701 Loss_G: -0.142517\n",
      "[611] Loss_D: -0.170670 Loss_G: -0.144905\n",
      "[612] Loss_D: -0.188444 Loss_G: -0.141843\n",
      "[613] Loss_D: -0.183955 Loss_G: -0.142142\n",
      "[614] Loss_D: -0.177946 Loss_G: -0.143246\n",
      "[615] Loss_D: -0.178994 Loss_G: -0.143189\n",
      "[616] Loss_D: -0.178796 Loss_G: -0.143507\n",
      "[617] Loss_D: -0.182709 Loss_G: -0.142720\n",
      "[618] Loss_D: -0.180008 Loss_G: -0.142662\n",
      "[619] Loss_D: -0.171265 Loss_G: -0.145327\n",
      "[620] Loss_D: -0.172923 Loss_G: -0.143561\n",
      "[621] Loss_D: -0.179876 Loss_G: -0.142785\n",
      "[622] Loss_D: -0.183434 Loss_G: -0.141864\n",
      "[623] Loss_D: -0.187770 Loss_G: -0.141947\n",
      "[624] Loss_D: -0.172757 Loss_G: -0.144083\n",
      "[625] Loss_D: -0.183307 Loss_G: -0.142058\n",
      "[626] Loss_D: -0.181687 Loss_G: -0.142485\n",
      "[627] Loss_D: -0.184817 Loss_G: -0.142298\n",
      "[628] Loss_D: -0.183394 Loss_G: -0.142331\n",
      "[629] Loss_D: -0.185803 Loss_G: -0.142279\n",
      "[630] Loss_D: -0.183455 Loss_G: -0.142216\n",
      "[631] Loss_D: -0.181606 Loss_G: -0.142950\n",
      "[632] Loss_D: -0.185484 Loss_G: -0.142325\n",
      "[633] Loss_D: -0.172267 Loss_G: -0.144604\n",
      "[634] Loss_D: -0.178991 Loss_G: -0.142677\n",
      "[635] Loss_D: -0.159743 Loss_G: -0.145456\n",
      "[636] Loss_D: -0.168552 Loss_G: -0.144045\n",
      "[637] Loss_D: -0.179549 Loss_G: -0.142118\n",
      "[638] Loss_D: -0.185386 Loss_G: -0.141585\n",
      "[639] Loss_D: -0.182283 Loss_G: -0.142322\n",
      "[640] Loss_D: -0.187923 Loss_G: -0.142210\n",
      "[641] Loss_D: -0.176731 Loss_G: -0.145025\n",
      "[642] Loss_D: -0.177999 Loss_G: -0.142177\n",
      "[643] Loss_D: -0.181023 Loss_G: -0.143640\n",
      "[644] Loss_D: -0.181841 Loss_G: -0.141791\n",
      "[645] Loss_D: -0.181972 Loss_G: -0.142351\n",
      "[646] Loss_D: -0.179435 Loss_G: -0.143113\n",
      "[647] Loss_D: -0.182589 Loss_G: -0.142547\n",
      "[648] Loss_D: -0.184314 Loss_G: -0.142329\n",
      "[649] Loss_D: -0.185302 Loss_G: -0.142335\n",
      "[650] Loss_D: -0.186603 Loss_G: -0.142084\n",
      "[651] Loss_D: -0.178187 Loss_G: -0.143031\n",
      "[652] Loss_D: -0.180369 Loss_G: -0.143066\n",
      "[653] Loss_D: -0.182411 Loss_G: -0.142825\n",
      "[654] Loss_D: -0.180839 Loss_G: -0.143274\n",
      "[655] Loss_D: -0.175014 Loss_G: -0.143956\n",
      "[656] Loss_D: -0.183289 Loss_G: -0.142514\n",
      "[657] Loss_D: -0.182117 Loss_G: -0.142791\n",
      "[658] Loss_D: -0.172468 Loss_G: -0.143848\n",
      "[659] Loss_D: -0.181461 Loss_G: -0.143338\n",
      "[660] Loss_D: -0.163021 Loss_G: -0.146181\n",
      "[661] Loss_D: -0.182625 Loss_G: -0.142050\n",
      "[662] Loss_D: -0.184939 Loss_G: -0.142291\n",
      "[663] Loss_D: -0.175739 Loss_G: -0.144369\n",
      "[664] Loss_D: -0.178700 Loss_G: -0.143946\n",
      "[665] Loss_D: -0.180011 Loss_G: -0.142792\n",
      "[666] Loss_D: -0.171987 Loss_G: -0.145587\n",
      "[667] Loss_D: -0.184635 Loss_G: -0.142524\n",
      "[668] Loss_D: -0.181363 Loss_G: -0.142236\n",
      "[669] Loss_D: -0.186958 Loss_G: -0.142140\n",
      "[670] Loss_D: -0.163374 Loss_G: -0.146318\n",
      "[671] Loss_D: -0.183242 Loss_G: -0.141976\n",
      "[672] Loss_D: -0.183621 Loss_G: -0.142544\n",
      "[673] Loss_D: -0.181787 Loss_G: -0.142933\n",
      "[674] Loss_D: -0.184546 Loss_G: -0.142936\n",
      "[675] Loss_D: -0.175776 Loss_G: -0.143663\n",
      "[676] Loss_D: -0.183198 Loss_G: -0.142565\n",
      "[677] Loss_D: -0.175754 Loss_G: -0.144740\n",
      "[678] Loss_D: -0.184065 Loss_G: -0.142426\n",
      "[679] Loss_D: -0.172361 Loss_G: -0.144181\n",
      "[680] Loss_D: -0.181359 Loss_G: -0.142633\n",
      "[681] Loss_D: -0.189802 Loss_G: -0.142244\n",
      "[682] Loss_D: -0.182608 Loss_G: -0.142825\n",
      "[683] Loss_D: -0.178516 Loss_G: -0.143311\n",
      "[684] Loss_D: -0.157565 Loss_G: -0.147282\n",
      "[685] Loss_D: -0.176922 Loss_G: -0.143447\n",
      "[686] Loss_D: -0.184259 Loss_G: -0.142330\n",
      "[687] Loss_D: -0.176213 Loss_G: -0.145229\n",
      "[688] Loss_D: -0.171751 Loss_G: -0.144051\n",
      "[689] Loss_D: -0.174547 Loss_G: -0.143759\n",
      "[690] Loss_D: -0.184244 Loss_G: -0.143087\n",
      "[691] Loss_D: -0.160358 Loss_G: -0.146299\n",
      "[692] Loss_D: -0.182046 Loss_G: -0.142497\n",
      "[693] Loss_D: -0.182694 Loss_G: -0.142736\n",
      "[694] Loss_D: -0.177480 Loss_G: -0.143576\n",
      "[695] Loss_D: -0.182189 Loss_G: -0.142521\n",
      "[696] Loss_D: -0.184177 Loss_G: -0.142521\n",
      "[697] Loss_D: -0.185598 Loss_G: -0.143059\n",
      "[698] Loss_D: -0.175836 Loss_G: -0.143443\n",
      "[699] Loss_D: -0.176357 Loss_G: -0.144053\n",
      "[700] Loss_D: -0.185892 Loss_G: -0.142529\n",
      "[701] Loss_D: -0.186573 Loss_G: -0.142452\n",
      "[702] Loss_D: -0.184756 Loss_G: -0.142756\n",
      "[703] Loss_D: -0.171187 Loss_G: -0.144603\n",
      "[704] Loss_D: -0.174711 Loss_G: -0.144076\n",
      "[705] Loss_D: -0.180827 Loss_G: -0.142485\n",
      "[706] Loss_D: -0.182644 Loss_G: -0.142628\n",
      "[707] Loss_D: -0.174516 Loss_G: -0.144136\n",
      "[708] Loss_D: -0.178564 Loss_G: -0.143222\n",
      "[709] Loss_D: -0.163371 Loss_G: -0.148701\n",
      "[710] Loss_D: -0.177936 Loss_G: -0.142797\n",
      "[711] Loss_D: -0.177816 Loss_G: -0.143240\n",
      "[712] Loss_D: -0.180483 Loss_G: -0.143949\n",
      "[713] Loss_D: -0.169599 Loss_G: -0.145338\n",
      "[714] Loss_D: -0.167258 Loss_G: -0.145277\n",
      "[715] Loss_D: -0.183059 Loss_G: -0.141957\n",
      "[716] Loss_D: -0.183834 Loss_G: -0.142434\n",
      "[717] Loss_D: -0.186594 Loss_G: -0.142208\n",
      "[718] Loss_D: -0.161007 Loss_G: -0.149300\n",
      "[719] Loss_D: -0.163349 Loss_G: -0.144496\n",
      "[720] Loss_D: -0.184954 Loss_G: -0.141917\n",
      "[721] Loss_D: -0.185406 Loss_G: -0.142390\n",
      "[722] Loss_D: -0.179791 Loss_G: -0.143389\n",
      "[723] Loss_D: -0.185456 Loss_G: -0.142835\n",
      "[724] Loss_D: -0.149757 Loss_G: -0.148347\n",
      "[725] Loss_D: -0.178201 Loss_G: -0.142206\n",
      "[726] Loss_D: -0.168428 Loss_G: -0.144345\n",
      "[727] Loss_D: -0.179162 Loss_G: -0.143409\n",
      "[728] Loss_D: -0.184530 Loss_G: -0.142783\n",
      "[729] Loss_D: -0.186072 Loss_G: -0.142387\n",
      "[730] Loss_D: -0.188587 Loss_G: -0.142314\n",
      "[731] Loss_D: -0.167117 Loss_G: -0.145474\n",
      "[732] Loss_D: -0.186812 Loss_G: -0.142173\n",
      "[733] Loss_D: -0.172990 Loss_G: -0.144527\n",
      "[734] Loss_D: -0.182927 Loss_G: -0.142717\n",
      "[735] Loss_D: -0.181218 Loss_G: -0.143474\n",
      "[736] Loss_D: -0.157658 Loss_G: -0.147001\n",
      "[737] Loss_D: -0.182974 Loss_G: -0.142227\n",
      "[738] Loss_D: -0.186647 Loss_G: -0.142160\n",
      "[739] Loss_D: -0.168036 Loss_G: -0.145801\n",
      "[740] Loss_D: -0.178214 Loss_G: -0.143556\n",
      "[741] Loss_D: -0.186530 Loss_G: -0.142394\n",
      "[742] Loss_D: -0.186284 Loss_G: -0.142593\n",
      "[743] Loss_D: -0.183883 Loss_G: -0.142831\n",
      "[744] Loss_D: -0.179403 Loss_G: -0.143652\n",
      "[745] Loss_D: -0.185487 Loss_G: -0.142413\n",
      "[746] Loss_D: -0.183927 Loss_G: -0.143179\n",
      "[747] Loss_D: -0.182977 Loss_G: -0.143028\n",
      "[748] Loss_D: -0.153471 Loss_G: -0.151237\n",
      "[749] Loss_D: -0.171492 Loss_G: -0.144219\n",
      "[750] Loss_D: -0.159814 Loss_G: -0.145711\n",
      "[751] Loss_D: -0.177810 Loss_G: -0.142905\n",
      "[752] Loss_D: -0.188539 Loss_G: -0.142053\n",
      "[753] Loss_D: -0.179266 Loss_G: -0.143463\n",
      "[754] Loss_D: -0.184364 Loss_G: -0.142263\n",
      "[755] Loss_D: -0.174562 Loss_G: -0.144884\n",
      "[756] Loss_D: -0.187375 Loss_G: -0.142306\n",
      "[757] Loss_D: -0.178815 Loss_G: -0.142906\n",
      "[758] Loss_D: -0.188360 Loss_G: -0.142476\n",
      "[759] Loss_D: -0.183355 Loss_G: -0.143089\n",
      "[760] Loss_D: -0.172798 Loss_G: -0.144882\n",
      "[761] Loss_D: -0.187410 Loss_G: -0.142525\n",
      "[762] Loss_D: -0.175848 Loss_G: -0.145008\n",
      "[763] Loss_D: -0.149352 Loss_G: -0.148167\n",
      "[764] Loss_D: -0.172628 Loss_G: -0.143997\n",
      "[765] Loss_D: -0.183532 Loss_G: -0.142594\n",
      "[766] Loss_D: -0.167812 Loss_G: -0.148165\n",
      "[767] Loss_D: -0.178591 Loss_G: -0.143106\n",
      "[768] Loss_D: -0.186481 Loss_G: -0.142574\n",
      "[769] Loss_D: -0.183645 Loss_G: -0.142828\n",
      "[770] Loss_D: -0.189061 Loss_G: -0.142191\n",
      "[771] Loss_D: -0.138009 Loss_G: -0.152342\n",
      "[772] Loss_D: -0.146698 Loss_G: -0.147005\n",
      "[773] Loss_D: -0.170704 Loss_G: -0.143070\n",
      "[774] Loss_D: -0.185253 Loss_G: -0.142220\n",
      "[775] Loss_D: -0.184915 Loss_G: -0.142264\n",
      "[776] Loss_D: -0.170354 Loss_G: -0.146128\n",
      "[777] Loss_D: -0.183294 Loss_G: -0.143451\n",
      "[778] Loss_D: -0.182200 Loss_G: -0.142793\n",
      "[779] Loss_D: -0.186091 Loss_G: -0.142520\n",
      "[780] Loss_D: -0.183708 Loss_G: -0.143117\n",
      "[781] Loss_D: -0.186875 Loss_G: -0.142564\n",
      "[782] Loss_D: -0.156726 Loss_G: -0.148393\n",
      "[783] Loss_D: -0.156854 Loss_G: -0.145344\n",
      "[784] Loss_D: -0.175175 Loss_G: -0.143869\n",
      "[785] Loss_D: -0.185623 Loss_G: -0.142382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[786] Loss_D: -0.176166 Loss_G: -0.144517\n",
      "[787] Loss_D: -0.187946 Loss_G: -0.142579\n",
      "[788] Loss_D: -0.178352 Loss_G: -0.143877\n",
      "[789] Loss_D: -0.175925 Loss_G: -0.143557\n",
      "[790] Loss_D: -0.181968 Loss_G: -0.143395\n",
      "[791] Loss_D: -0.185216 Loss_G: -0.143048\n",
      "[792] Loss_D: -0.175825 Loss_G: -0.144357\n",
      "[793] Loss_D: -0.148248 Loss_G: -0.150453\n",
      "[794] Loss_D: -0.166037 Loss_G: -0.144559\n",
      "[795] Loss_D: -0.170324 Loss_G: -0.146206\n",
      "[796] Loss_D: -0.185986 Loss_G: -0.142077\n",
      "[797] Loss_D: -0.190277 Loss_G: -0.142459\n",
      "[798] Loss_D: -0.170947 Loss_G: -0.144668\n",
      "[799] Loss_D: -0.183052 Loss_G: -0.142809\n",
      "[800] Loss_D: -0.184177 Loss_G: -0.142568\n",
      "[801] Loss_D: -0.175156 Loss_G: -0.145693\n",
      "[802] Loss_D: -0.170461 Loss_G: -0.147889\n",
      "[803] Loss_D: -0.172347 Loss_G: -0.143621\n",
      "[804] Loss_D: -0.187496 Loss_G: -0.143128\n",
      "[805] Loss_D: -0.153674 Loss_G: -0.146999\n",
      "[806] Loss_D: -0.168173 Loss_G: -0.146042\n",
      "[807] Loss_D: -0.165992 Loss_G: -0.146575\n",
      "[808] Loss_D: -0.177390 Loss_G: -0.143602\n",
      "[809] Loss_D: -0.184208 Loss_G: -0.143118\n",
      "[810] Loss_D: -0.175961 Loss_G: -0.144827\n",
      "[811] Loss_D: -0.178489 Loss_G: -0.143460\n",
      "[812] Loss_D: -0.166002 Loss_G: -0.145384\n",
      "[813] Loss_D: -0.181828 Loss_G: -0.143578\n",
      "[814] Loss_D: -0.187554 Loss_G: -0.142491\n",
      "[815] Loss_D: -0.187568 Loss_G: -0.142477\n",
      "[816] Loss_D: -0.168933 Loss_G: -0.150185\n",
      "[817] Loss_D: -0.176969 Loss_G: -0.143162\n",
      "[818] Loss_D: -0.177023 Loss_G: -0.145679\n",
      "[819] Loss_D: -0.163547 Loss_G: -0.146126\n",
      "[820] Loss_D: -0.178608 Loss_G: -0.143049\n",
      "[821] Loss_D: -0.169005 Loss_G: -0.145201\n",
      "[822] Loss_D: -0.183138 Loss_G: -0.142913\n",
      "[823] Loss_D: -0.184563 Loss_G: -0.143263\n",
      "[824] Loss_D: -0.159289 Loss_G: -0.147752\n",
      "[825] Loss_D: -0.165052 Loss_G: -0.146362\n",
      "[826] Loss_D: -0.182979 Loss_G: -0.143341\n",
      "[827] Loss_D: -0.162394 Loss_G: -0.147370\n",
      "[828] Loss_D: -0.178049 Loss_G: -0.145302\n",
      "[829] Loss_D: -0.165203 Loss_G: -0.145192\n",
      "[830] Loss_D: -0.183964 Loss_G: -0.142965\n",
      "[831] Loss_D: -0.179809 Loss_G: -0.143207\n",
      "[832] Loss_D: -0.184618 Loss_G: -0.143324\n",
      "[833] Loss_D: -0.182633 Loss_G: -0.144102\n",
      "[834] Loss_D: -0.173489 Loss_G: -0.144061\n",
      "[835] Loss_D: -0.184449 Loss_G: -0.143250\n",
      "[836] Loss_D: -0.184463 Loss_G: -0.143672\n",
      "[837] Loss_D: -0.157589 Loss_G: -0.148106\n",
      "[838] Loss_D: -0.176429 Loss_G: -0.143305\n",
      "[839] Loss_D: -0.186854 Loss_G: -0.142784\n",
      "[840] Loss_D: -0.184837 Loss_G: -0.143420\n",
      "[841] Loss_D: -0.174758 Loss_G: -0.144496\n",
      "[842] Loss_D: -0.165967 Loss_G: -0.144545\n",
      "[843] Loss_D: -0.180340 Loss_G: -0.143696\n",
      "[844] Loss_D: -0.185353 Loss_G: -0.143196\n",
      "[845] Loss_D: -0.154626 Loss_G: -0.147415\n",
      "[846] Loss_D: -0.175948 Loss_G: -0.144266\n",
      "[847] Loss_D: -0.168119 Loss_G: -0.145337\n",
      "[848] Loss_D: -0.180903 Loss_G: -0.143419\n",
      "[849] Loss_D: -0.185241 Loss_G: -0.142788\n",
      "[850] Loss_D: -0.165453 Loss_G: -0.146463\n",
      "[851] Loss_D: -0.183289 Loss_G: -0.142975\n",
      "[852] Loss_D: -0.178076 Loss_G: -0.144385\n",
      "[853] Loss_D: -0.143228 Loss_G: -0.150042\n",
      "[854] Loss_D: -0.158909 Loss_G: -0.151797\n",
      "[855] Loss_D: -0.158744 Loss_G: -0.144989\n",
      "[856] Loss_D: -0.175929 Loss_G: -0.143740\n",
      "[857] Loss_D: -0.180274 Loss_G: -0.143469\n",
      "[858] Loss_D: -0.172249 Loss_G: -0.144412\n",
      "[859] Loss_D: -0.188972 Loss_G: -0.142863\n",
      "[860] Loss_D: -0.173457 Loss_G: -0.145478\n",
      "[861] Loss_D: -0.176518 Loss_G: -0.143380\n",
      "[862] Loss_D: -0.186712 Loss_G: -0.142980\n",
      "[863] Loss_D: -0.172792 Loss_G: -0.145617\n",
      "[864] Loss_D: -0.170160 Loss_G: -0.145264\n",
      "[865] Loss_D: -0.176895 Loss_G: -0.143535\n",
      "[866] Loss_D: -0.185124 Loss_G: -0.143094\n",
      "[867] Loss_D: -0.162130 Loss_G: -0.147073\n",
      "[868] Loss_D: -0.172847 Loss_G: -0.146569\n",
      "[869] Loss_D: -0.165670 Loss_G: -0.145317\n",
      "[870] Loss_D: -0.182194 Loss_G: -0.143052\n",
      "[871] Loss_D: -0.180804 Loss_G: -0.143622\n",
      "[872] Loss_D: -0.180257 Loss_G: -0.143858\n",
      "[873] Loss_D: -0.173151 Loss_G: -0.144570\n",
      "[874] Loss_D: -0.185366 Loss_G: -0.143021\n",
      "[875] Loss_D: -0.170543 Loss_G: -0.146437\n",
      "[876] Loss_D: -0.167624 Loss_G: -0.145384\n",
      "[877] Loss_D: -0.178555 Loss_G: -0.145440\n",
      "[878] Loss_D: -0.173867 Loss_G: -0.143843\n",
      "[879] Loss_D: -0.168975 Loss_G: -0.145553\n",
      "[880] Loss_D: -0.160894 Loss_G: -0.146033\n",
      "[881] Loss_D: -0.177695 Loss_G: -0.143257\n",
      "[882] Loss_D: -0.178631 Loss_G: -0.144300\n",
      "[883] Loss_D: -0.177324 Loss_G: -0.144206\n",
      "[884] Loss_D: -0.171269 Loss_G: -0.145972\n",
      "[885] Loss_D: -0.180344 Loss_G: -0.143838\n",
      "[886] Loss_D: -0.182468 Loss_G: -0.142965\n",
      "[887] Loss_D: -0.183951 Loss_G: -0.143289\n",
      "[888] Loss_D: -0.179219 Loss_G: -0.144206\n",
      "[889] Loss_D: -0.178906 Loss_G: -0.143494\n",
      "[890] Loss_D: -0.172485 Loss_G: -0.144719\n",
      "[891] Loss_D: -0.182344 Loss_G: -0.143478\n",
      "[892] Loss_D: -0.160276 Loss_G: -0.146870\n",
      "[893] Loss_D: -0.170771 Loss_G: -0.145058\n",
      "[894] Loss_D: -0.167089 Loss_G: -0.146725\n",
      "[895] Loss_D: -0.176798 Loss_G: -0.144278\n",
      "[896] Loss_D: -0.179960 Loss_G: -0.143717\n",
      "[897] Loss_D: -0.178171 Loss_G: -0.144679\n",
      "[898] Loss_D: -0.166195 Loss_G: -0.146266\n",
      "[899] Loss_D: -0.155520 Loss_G: -0.147359\n",
      "[900] Loss_D: -0.181837 Loss_G: -0.142850\n",
      "[901] Loss_D: -0.186842 Loss_G: -0.143198\n",
      "[902] Loss_D: -0.174527 Loss_G: -0.144310\n",
      "[903] Loss_D: -0.187046 Loss_G: -0.143153\n",
      "[904] Loss_D: -0.178433 Loss_G: -0.143991\n",
      "[905] Loss_D: -0.182880 Loss_G: -0.143839\n",
      "[906] Loss_D: -0.152296 Loss_G: -0.149405\n",
      "[907] Loss_D: -0.170351 Loss_G: -0.143886\n",
      "[908] Loss_D: -0.167225 Loss_G: -0.145214\n",
      "[909] Loss_D: -0.181037 Loss_G: -0.143317\n",
      "[910] Loss_D: -0.183364 Loss_G: -0.143259\n",
      "[911] Loss_D: -0.187053 Loss_G: -0.142759\n",
      "[912] Loss_D: -0.184003 Loss_G: -0.144215\n",
      "[913] Loss_D: -0.160962 Loss_G: -0.147465\n",
      "[914] Loss_D: -0.167421 Loss_G: -0.146640\n",
      "[915] Loss_D: -0.170217 Loss_G: -0.145504\n",
      "[916] Loss_D: -0.185076 Loss_G: -0.143508\n",
      "[917] Loss_D: -0.183910 Loss_G: -0.143466\n",
      "[918] Loss_D: -0.181906 Loss_G: -0.145259\n",
      "[919] Loss_D: -0.124698 Loss_G: -0.160631\n",
      "[920] Loss_D: -0.144276 Loss_G: -0.145952\n",
      "[921] Loss_D: -0.162387 Loss_G: -0.144526\n",
      "[922] Loss_D: -0.175965 Loss_G: -0.143276\n",
      "[923] Loss_D: -0.172814 Loss_G: -0.144071\n",
      "[924] Loss_D: -0.169191 Loss_G: -0.144250\n",
      "[925] Loss_D: -0.177999 Loss_G: -0.143859\n",
      "[926] Loss_D: -0.184570 Loss_G: -0.142970\n",
      "[927] Loss_D: -0.187132 Loss_G: -0.143178\n",
      "[928] Loss_D: -0.174332 Loss_G: -0.144677\n",
      "[929] Loss_D: -0.183190 Loss_G: -0.143318\n",
      "[930] Loss_D: -0.162240 Loss_G: -0.147684\n",
      "[931] Loss_D: -0.175722 Loss_G: -0.144233\n",
      "[932] Loss_D: -0.183229 Loss_G: -0.143559\n",
      "[933] Loss_D: -0.156869 Loss_G: -0.148757\n",
      "[934] Loss_D: -0.161990 Loss_G: -0.149032\n",
      "[935] Loss_D: -0.171656 Loss_G: -0.145212\n",
      "[936] Loss_D: -0.177447 Loss_G: -0.143369\n",
      "[937] Loss_D: -0.172929 Loss_G: -0.144433\n",
      "[938] Loss_D: -0.172761 Loss_G: -0.145020\n",
      "[939] Loss_D: -0.157470 Loss_G: -0.147585\n",
      "[940] Loss_D: -0.176726 Loss_G: -0.144808\n",
      "[941] Loss_D: -0.150869 Loss_G: -0.148160\n",
      "[942] Loss_D: -0.167557 Loss_G: -0.144548\n",
      "[943] Loss_D: -0.177262 Loss_G: -0.144554\n",
      "[944] Loss_D: -0.178156 Loss_G: -0.144186\n",
      "[945] Loss_D: -0.176507 Loss_G: -0.144353\n",
      "[946] Loss_D: -0.178041 Loss_G: -0.144337\n",
      "[947] Loss_D: -0.174636 Loss_G: -0.146233\n",
      "[948] Loss_D: -0.167299 Loss_G: -0.145924\n",
      "[949] Loss_D: -0.182226 Loss_G: -0.144895\n",
      "[950] Loss_D: -0.172347 Loss_G: -0.147215\n",
      "[951] Loss_D: -0.159869 Loss_G: -0.148317\n",
      "[952] Loss_D: -0.164282 Loss_G: -0.146399\n",
      "[953] Loss_D: -0.174449 Loss_G: -0.144017\n",
      "[954] Loss_D: -0.175443 Loss_G: -0.146661\n",
      "[955] Loss_D: -0.167881 Loss_G: -0.150201\n",
      "[956] Loss_D: -0.140562 Loss_G: -0.150434\n",
      "[957] Loss_D: -0.166080 Loss_G: -0.144861\n",
      "[958] Loss_D: -0.171311 Loss_G: -0.144808\n",
      "[959] Loss_D: -0.177996 Loss_G: -0.143847\n",
      "[960] Loss_D: -0.186276 Loss_G: -0.142651\n",
      "[961] Loss_D: -0.183190 Loss_G: -0.143576\n",
      "[962] Loss_D: -0.186743 Loss_G: -0.143101\n",
      "[963] Loss_D: -0.183432 Loss_G: -0.143655\n",
      "[964] Loss_D: -0.160012 Loss_G: -0.147704\n",
      "[965] Loss_D: -0.178261 Loss_G: -0.144354\n",
      "[966] Loss_D: -0.181588 Loss_G: -0.144287\n",
      "[967] Loss_D: -0.179201 Loss_G: -0.145051\n",
      "[968] Loss_D: -0.169786 Loss_G: -0.144526\n",
      "[969] Loss_D: -0.171649 Loss_G: -0.145216\n",
      "[970] Loss_D: -0.176772 Loss_G: -0.146488\n",
      "[971] Loss_D: -0.170787 Loss_G: -0.146617\n",
      "[972] Loss_D: -0.164872 Loss_G: -0.153675\n",
      "[973] Loss_D: -0.151284 Loss_G: -0.154094\n",
      "[974] Loss_D: -0.174147 Loss_G: -0.143369\n",
      "[975] Loss_D: -0.156071 Loss_G: -0.149385\n",
      "[976] Loss_D: -0.167134 Loss_G: -0.145145\n",
      "[977] Loss_D: -0.170271 Loss_G: -0.147210\n",
      "[978] Loss_D: -0.163663 Loss_G: -0.146284\n",
      "[979] Loss_D: -0.180835 Loss_G: -0.143545\n",
      "[980] Loss_D: -0.175972 Loss_G: -0.143624\n",
      "[981] Loss_D: -0.184739 Loss_G: -0.143383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[982] Loss_D: -0.182443 Loss_G: -0.143778\n",
      "[983] Loss_D: -0.181107 Loss_G: -0.143656\n",
      "[984] Loss_D: -0.182841 Loss_G: -0.143945\n",
      "[985] Loss_D: -0.175788 Loss_G: -0.146818\n",
      "[986] Loss_D: -0.173780 Loss_G: -0.148004\n",
      "[987] Loss_D: -0.153873 Loss_G: -0.148372\n",
      "[988] Loss_D: -0.169820 Loss_G: -0.145124\n",
      "[989] Loss_D: -0.161120 Loss_G: -0.147852\n",
      "[990] Loss_D: -0.171160 Loss_G: -0.144978\n",
      "[991] Loss_D: -0.180357 Loss_G: -0.144284\n",
      "[992] Loss_D: -0.178223 Loss_G: -0.143769\n",
      "[993] Loss_D: -0.177689 Loss_G: -0.145234\n",
      "[994] Loss_D: -0.177524 Loss_G: -0.145547\n",
      "[995] Loss_D: -0.163567 Loss_G: -0.147262\n",
      "[996] Loss_D: -0.167220 Loss_G: -0.145859\n",
      "[997] Loss_D: -0.179987 Loss_G: -0.144136\n",
      "[998] Loss_D: -0.169604 Loss_G: -0.146114\n",
      "[999] Loss_D: -0.178666 Loss_G: -0.143774\n",
      "[1000] Loss_D: -0.161488 Loss_G: -0.148609\n",
      "[1001] Loss_D: -0.174193 Loss_G: -0.145090\n",
      "[1002] Loss_D: -0.176688 Loss_G: -0.144904\n",
      "[1003] Loss_D: -0.178430 Loss_G: -0.144973\n",
      "[1004] Loss_D: -0.169035 Loss_G: -0.145739\n",
      "[1005] Loss_D: -0.167318 Loss_G: -0.145746\n",
      "[1006] Loss_D: -0.164206 Loss_G: -0.147562\n",
      "[1007] Loss_D: -0.153480 Loss_G: -0.149117\n",
      "[1008] Loss_D: -0.169337 Loss_G: -0.146575\n",
      "[1009] Loss_D: -0.165645 Loss_G: -0.146362\n",
      "[1010] Loss_D: -0.171408 Loss_G: -0.145031\n",
      "[1011] Loss_D: -0.168675 Loss_G: -0.146583\n",
      "[1012] Loss_D: -0.179210 Loss_G: -0.145893\n",
      "[1013] Loss_D: -0.173648 Loss_G: -0.143815\n",
      "[1014] Loss_D: -0.170166 Loss_G: -0.146858\n",
      "[1015] Loss_D: -0.152022 Loss_G: -0.148635\n",
      "[1016] Loss_D: -0.171925 Loss_G: -0.145364\n",
      "[1017] Loss_D: -0.180589 Loss_G: -0.144337\n",
      "[1018] Loss_D: -0.180708 Loss_G: -0.143504\n",
      "[1019] Loss_D: -0.175903 Loss_G: -0.144984\n",
      "[1020] Loss_D: -0.174586 Loss_G: -0.144567\n",
      "[1021] Loss_D: -0.184743 Loss_G: -0.143786\n",
      "[1022] Loss_D: -0.173151 Loss_G: -0.145713\n",
      "[1023] Loss_D: -0.162884 Loss_G: -0.149051\n",
      "[1024] Loss_D: -0.166642 Loss_G: -0.148049\n",
      "[1025] Loss_D: -0.165852 Loss_G: -0.145553\n",
      "[1026] Loss_D: -0.149753 Loss_G: -0.147448\n",
      "[1027] Loss_D: -0.169459 Loss_G: -0.147716\n",
      "[1028] Loss_D: -0.169440 Loss_G: -0.146908\n",
      "[1029] Loss_D: -0.177739 Loss_G: -0.144275\n",
      "[1030] Loss_D: -0.173940 Loss_G: -0.144490\n",
      "[1031] Loss_D: -0.182295 Loss_G: -0.145060\n",
      "[1032] Loss_D: -0.173227 Loss_G: -0.146490\n",
      "[1033] Loss_D: -0.159929 Loss_G: -0.148401\n",
      "[1034] Loss_D: -0.149669 Loss_G: -0.152065\n",
      "[1035] Loss_D: -0.158287 Loss_G: -0.147650\n",
      "[1036] Loss_D: -0.168975 Loss_G: -0.145552\n",
      "[1037] Loss_D: -0.175898 Loss_G: -0.146084\n",
      "[1038] Loss_D: -0.169237 Loss_G: -0.146373\n",
      "[1039] Loss_D: -0.171847 Loss_G: -0.144471\n",
      "[1040] Loss_D: -0.166419 Loss_G: -0.148322\n",
      "[1041] Loss_D: -0.169598 Loss_G: -0.150666\n",
      "[1042] Loss_D: -0.155952 Loss_G: -0.149093\n",
      "[1043] Loss_D: -0.171507 Loss_G: -0.144905\n",
      "[1044] Loss_D: -0.169016 Loss_G: -0.149130\n",
      "[1045] Loss_D: -0.165781 Loss_G: -0.149807\n",
      "[1046] Loss_D: -0.160124 Loss_G: -0.155230\n",
      "[1047] Loss_D: -0.165567 Loss_G: -0.154386\n",
      "[1048] Loss_D: -0.161159 Loss_G: -0.144735\n",
      "[1049] Loss_D: -0.164951 Loss_G: -0.146289\n",
      "[1050] Loss_D: -0.171881 Loss_G: -0.145851\n",
      "[1051] Loss_D: -0.177643 Loss_G: -0.144315\n",
      "[1052] Loss_D: -0.175308 Loss_G: -0.147256\n",
      "[1053] Loss_D: -0.171253 Loss_G: -0.145873\n",
      "[1054] Loss_D: -0.157687 Loss_G: -0.147452\n",
      "[1055] Loss_D: -0.156340 Loss_G: -0.148112\n",
      "[1056] Loss_D: -0.173202 Loss_G: -0.149861\n",
      "[1057] Loss_D: -0.172173 Loss_G: -0.145401\n",
      "[1058] Loss_D: -0.170297 Loss_G: -0.146592\n",
      "[1059] Loss_D: -0.163820 Loss_G: -0.148732\n",
      "[1060] Loss_D: -0.158577 Loss_G: -0.157507\n",
      "[1061] Loss_D: -0.156702 Loss_G: -0.153120\n",
      "[1062] Loss_D: -0.160909 Loss_G: -0.150145\n",
      "[1063] Loss_D: -0.164222 Loss_G: -0.149403\n",
      "[1064] Loss_D: -0.168052 Loss_G: -0.144906\n",
      "[1065] Loss_D: -0.160784 Loss_G: -0.149595\n",
      "[1066] Loss_D: -0.159878 Loss_G: -0.146615\n",
      "[1067] Loss_D: -0.166067 Loss_G: -0.147226\n",
      "[1068] Loss_D: -0.166854 Loss_G: -0.147272\n",
      "[1069] Loss_D: -0.161048 Loss_G: -0.151174\n",
      "[1070] Loss_D: -0.168320 Loss_G: -0.148691\n",
      "[1071] Loss_D: -0.176372 Loss_G: -0.148527\n",
      "[1072] Loss_D: -0.164867 Loss_G: -0.145807\n",
      "[1073] Loss_D: -0.159081 Loss_G: -0.149489\n",
      "[1074] Loss_D: -0.166342 Loss_G: -0.149315\n",
      "[1075] Loss_D: -0.171076 Loss_G: -0.147861\n",
      "[1076] Loss_D: -0.169723 Loss_G: -0.149973\n",
      "[1077] Loss_D: -0.159210 Loss_G: -0.159543\n",
      "[1078] Loss_D: -0.143497 Loss_G: -0.149263\n",
      "[1079] Loss_D: -0.149244 Loss_G: -0.157585\n",
      "[1080] Loss_D: -0.154251 Loss_G: -0.147701\n",
      "[1081] Loss_D: -0.171904 Loss_G: -0.146558\n",
      "[1082] Loss_D: -0.165632 Loss_G: -0.146564\n",
      "[1083] Loss_D: -0.172569 Loss_G: -0.145771\n",
      "[1084] Loss_D: -0.173033 Loss_G: -0.146902\n",
      "[1085] Loss_D: -0.163890 Loss_G: -0.159598\n",
      "[1086] Loss_D: -0.147692 Loss_G: -0.150095\n",
      "[1087] Loss_D: -0.157824 Loss_G: -0.158759\n",
      "[1088] Loss_D: -0.153386 Loss_G: -0.157800\n",
      "[1089] Loss_D: -0.160910 Loss_G: -0.151673\n",
      "[1090] Loss_D: -0.167344 Loss_G: -0.146062\n",
      "[1091] Loss_D: -0.157444 Loss_G: -0.151135\n",
      "[1092] Loss_D: -0.174258 Loss_G: -0.144905\n",
      "[1093] Loss_D: -0.172807 Loss_G: -0.146714\n",
      "[1094] Loss_D: -0.168882 Loss_G: -0.150868\n",
      "[1095] Loss_D: -0.162177 Loss_G: -0.149441\n",
      "[1096] Loss_D: -0.167785 Loss_G: -0.159448\n",
      "[1097] Loss_D: -0.151984 Loss_G: -0.153428\n",
      "[1098] Loss_D: -0.162592 Loss_G: -0.147437\n",
      "[1099] Loss_D: -0.168544 Loss_G: -0.151547\n",
      "[1100] Loss_D: -0.171099 Loss_G: -0.145414\n",
      "[1101] Loss_D: -0.161710 Loss_G: -0.155159\n",
      "[1102] Loss_D: -0.161537 Loss_G: -0.152743\n",
      "[1103] Loss_D: -0.170364 Loss_G: -0.147451\n",
      "[1104] Loss_D: -0.167234 Loss_G: -0.146075\n",
      "[1105] Loss_D: -0.163978 Loss_G: -0.146951\n",
      "[1106] Loss_D: -0.170415 Loss_G: -0.150104\n",
      "[1107] Loss_D: -0.154955 Loss_G: -0.152374\n",
      "[1108] Loss_D: -0.162877 Loss_G: -0.151342\n",
      "[1109] Loss_D: -0.167455 Loss_G: -0.151241\n",
      "[1110] Loss_D: -0.160391 Loss_G: -0.145731\n",
      "[1111] Loss_D: -0.158075 Loss_G: -0.151672\n",
      "[1112] Loss_D: -0.165136 Loss_G: -0.147462\n",
      "[1113] Loss_D: -0.160641 Loss_G: -0.151729\n",
      "[1114] Loss_D: -0.166077 Loss_G: -0.147799\n",
      "[1115] Loss_D: -0.167757 Loss_G: -0.146855\n",
      "[1116] Loss_D: -0.169347 Loss_G: -0.147950\n",
      "[1117] Loss_D: -0.161606 Loss_G: -0.151595\n",
      "[1118] Loss_D: -0.160253 Loss_G: -0.152640\n",
      "[1119] Loss_D: -0.162109 Loss_G: -0.148786\n",
      "[1120] Loss_D: -0.165024 Loss_G: -0.159447\n",
      "[1121] Loss_D: -0.151554 Loss_G: -0.151836\n",
      "[1122] Loss_D: -0.161490 Loss_G: -0.147743\n",
      "[1123] Loss_D: -0.168907 Loss_G: -0.154097\n",
      "[1124] Loss_D: -0.159421 Loss_G: -0.156717\n",
      "[1125] Loss_D: -0.158922 Loss_G: -0.155811\n",
      "[1126] Loss_D: -0.166443 Loss_G: -0.145432\n",
      "[1127] Loss_D: -0.164071 Loss_G: -0.157332\n",
      "[1128] Loss_D: -0.153846 Loss_G: -0.152641\n",
      "[1129] Loss_D: -0.169078 Loss_G: -0.147099\n",
      "[1130] Loss_D: -0.166487 Loss_G: -0.152417\n",
      "[1131] Loss_D: -0.162278 Loss_G: -0.157971\n",
      "[1132] Loss_D: -0.159700 Loss_G: -0.152196\n",
      "[1133] Loss_D: -0.163208 Loss_G: -0.150986\n",
      "[1134] Loss_D: -0.167158 Loss_G: -0.156470\n",
      "[1135] Loss_D: -0.159101 Loss_G: -0.146337\n",
      "[1136] Loss_D: -0.156733 Loss_G: -0.146387\n",
      "[1137] Loss_D: -0.160718 Loss_G: -0.149747\n",
      "[1138] Loss_D: -0.162373 Loss_G: -0.160315\n",
      "[1139] Loss_D: -0.152101 Loss_G: -0.151301\n",
      "[1140] Loss_D: -0.163404 Loss_G: -0.146154\n",
      "[1141] Loss_D: -0.159641 Loss_G: -0.159705\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fd9bc9605546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdisc_updates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mreal_examples_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-53c6f1ccbd5b>\u001b[0m in \u001b[0;36mget_train_minibatch\u001b[0;34m(self, index, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"Return a minibatch of real and fake examples.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mreal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mfake_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_train_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(15000):\n",
    "    discriminator_losses = []\n",
    "    generator_losses = []\n",
    "    for j in range(0, iterator.num_train, 32):        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        \n",
    "        for disc_updates in range(5):\n",
    "            real_examples_full, real_examples, fake_images = iterator.get_train_minibatch(j, 32)\n",
    "            D1 = discriminator(real_examples)\n",
    "            fake = generator(fake_images)\n",
    "            D2 = discriminator(fake)\n",
    "            discriminator_loss = -.5 * ((D1 - D2).mean())\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            discriminator_losses.append(discriminator_loss.data[0])\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        \n",
    "        generated_images = generator(fake_images)\n",
    "        generator_loss = -.5 * discriminator( generated_images).mean() + loss_criterion(generated_images, real_examples)\n",
    "        optimizer_generator.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "        generator_losses.append(generator_loss.data[0])\n",
    "\n",
    "    print('[%d] Loss_D: %f Loss_G: %f' % (i, np.mean(discriminator_losses), np.mean(generator_losses)))\n",
    "    save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    # Forward Pass\n",
    "    scores = model(X)\n",
    "    scores = scores.gather(1, y.view(-1, 1)).squeeze()  \n",
    "    \n",
    "    # Backward Pass\n",
    "    gradients_init = torch.FloatTensor([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "    scores.backward(gradients_init)\n",
    "\n",
    "    saliency = X.grad.data\n",
    "    saliency = saliency.abs()\n",
    "    saliency, i = torch.max(saliency, dim = 1)\n",
    "    saliency = saliency.squeeze() \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n",
    "show_saliency_maps(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fooling_np = deprocess(fake_images.clone())\n",
    "X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X[idx])\n",
    "plt.title(class_names[y[idx]])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_fooling_np)\n",
    "plt.title(class_names[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "X_pre = preprocess(Image.fromarray(X[idx]))\n",
    "diff = np.asarray(deprocess(fake_images - real_examples, should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "diff = np.asarray(deprocess(10 * (fake_images - real_examples), should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(12, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_examples[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake_images[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

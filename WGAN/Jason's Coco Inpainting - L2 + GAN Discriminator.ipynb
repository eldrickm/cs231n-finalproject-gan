{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  6930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc8082704f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = dset.CocoCaptions(\n",
    "    root='inpainting/train2014',\n",
    "    annFile='inpainting/annotations/captions_train2014.json',\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    \"\"\"Data Iterator for COCO.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path='inpainting/train2014',\n",
    "        dev_path='inpainting/val2014',\n",
    "        train_annotation_path='inpainting/annotations/captions_train2014.json',\n",
    "        dev_annotation_path='inpainting/annotations/captions_val2014.json',\n",
    "    ):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.train_annotation_path = train_annotation_path\n",
    "        self.dev_path = dev_path\n",
    "        self.dev_annotation_path = dev_annotation_path\n",
    "        print('Processing data ...')\n",
    "        self._get_real_and_fake_images()\n",
    "\n",
    "    def _get_real_and_fake_images(self):\n",
    "        \"\"\"Get real and fake images from path.\"\"\"\n",
    "        self.train_dataset = dset.CocoCaptions(\n",
    "            root=self.train_path,\n",
    "            annFile=self.train_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.valid_dataset = dset.CocoCaptions(\n",
    "            root=self.dev_path, \n",
    "            annFile=self.dev_annotation_path,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        \n",
    "        print('Populating training images & captions ...')\n",
    "        train_images = []\n",
    "        train_captions = []\n",
    "        \n",
    "        # There appears to be one image missing for some weird reason.\n",
    "        try:\n",
    "            for img, captions in self.train_dataset:\n",
    "                train_images.append(img)\n",
    "                train_captions.append(captions)\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "        train_images = torch.stack(train_images)\n",
    "        \n",
    "        print('Populating validation images ...')\n",
    "        valid_images = torch.stack([x[0] for x in self.valid_dataset])\n",
    "        valid_captions = [x[1] for x in self.valid_dataset]\n",
    "        \n",
    "        print('Cropping 32x32 patch for training images ...')\n",
    "        noisy_train_images = copy.deepcopy(train_images.numpy())\n",
    "        noisy_train_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_train_images = torch.from_numpy(noisy_train_images)\n",
    "\n",
    "        print('Cropping 32x32 patch for validation images ...')\n",
    "        noisy_valid_images = copy.deepcopy(valid_images.numpy())\n",
    "        noisy_valid_images[:, :, 16:48, 16:48] = 0\n",
    "        noisy_valid_images = torch.from_numpy(noisy_valid_images)\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "\n",
    "        self.noisy_train_images = noisy_train_images\n",
    "        self.noisy_valid_images = noisy_valid_images\n",
    "        \n",
    "        self.num_train = len(train_images)\n",
    "        self.num_valid = len(valid_images)\n",
    "\n",
    "    def get_train_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.train_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_train_images[index: index + batch_size]).cuda()\n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples\n",
    "\n",
    "    def get_valid_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.valid_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_valid_images[index: index + batch_size]).cuda()\n",
    "        \n",
    "        return real_examples, real_examples[:, :, 16:48, 16:48], fake_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Populating training images & captions ...\n",
      "Populating validation images ...\n",
      "Cropping 32x32 patch for training images ...\n",
      "Cropping 32x32 patch for validation images ...\n"
     ]
    }
   ],
   "source": [
    "iterator = DataIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(iterator.train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(iterator.noisy_train_images[14].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGeneratorSkip(nn.Module):\n",
    "    \"\"\"Generator module.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize generator.\"\"\"\n",
    "        super(UNetGeneratorSkip, self).__init__()\n",
    "\n",
    "        #################################\n",
    "        ####### DOWNSAMPLER MODULE ######\n",
    "        #################################\n",
    "\n",
    "        # 3 x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        #################################\n",
    "        ####### UPSAMPLER MODULE ########\n",
    "        #################################\n",
    "\n",
    "        # 64 x 4 x 4\n",
    "        self.tconv1 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels = 4000,\n",
    "            kernel_size=4, bias=False\n",
    "        )\n",
    "        self.tbn1 = nn.BatchNorm2d(4000)\n",
    "\n",
    "        # 48 x 8 x 8 + 48 x 8 x 8 = [96 x 8 x 8]\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=4000, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.tbn2 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 32 x 16 x 16 + 32 x 16 x 16 = [64 x 16 x 16]\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 8, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "        \n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn4 = nn.BatchNorm2d(start_filter * 2)\n",
    "        \n",
    "        self.tconv5 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.tbn5 = nn.BatchNorm2d(start_filter)\n",
    "        \n",
    "        self.tconv6 = nn.ConvTranspose2d(\n",
    "            in_channels=start_filter, out_channels=3,\n",
    "            kernel_size=5, stride=1, padding=2, bias=False\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the generator.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        \n",
    "        # Upsampling steps.\n",
    "        tconv1 = F.leaky_relu(self.tbn1(self.tconv1(conv4)), negative_slope=0.2, inplace=True)\n",
    "        tconv2 = F.relu(self.tbn2(self.tconv2(tconv1)), True)\n",
    "        tconv3 = F.relu(self.tbn3(self.tconv3(tconv2)), True)\n",
    "        tconv4 = F.relu(self.tbn4(self.tconv4(tconv3)), True)\n",
    "        tconv5 = F.relu(self.tbn5(self.tconv5(tconv4)), True)\n",
    "        tconv6 = F.tanh(self.tconv6(tconv5))\n",
    "\n",
    "        return tconv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 3 x 32 x 32\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=start_filter, kernel_size=4,\n",
    "            stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(start_filter)\n",
    "\n",
    "        # 16 x 16 x 16\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=start_filter, out_channels=start_filter * 2,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(start_filter * 2)\n",
    "\n",
    "        # 32 x 8 x 8\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=start_filter * 2, out_channels=start_filter * 4,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(start_filter * 4)\n",
    "\n",
    "        # 48 x 4 x 4\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=start_filter * 4, out_channels=start_filter * 8,\n",
    "            kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(start_filter * 8)\n",
    "\n",
    "        # 64 x 2 x 2\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=start_filter * 8, out_channels=1,\n",
    "            kernel_size=2, stride=1, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        # Downsampling steps.\n",
    "        # print 'input', input.size()\n",
    "        conv1 = F.leaky_relu(self.bn1(self.conv1(input)), negative_slope=0.2, inplace=True)\n",
    "        conv2 = F.leaky_relu(self.bn2(self.conv2(conv1)), negative_slope=0.2, inplace=True)\n",
    "        conv3 = F.leaky_relu(self.bn3(self.conv3(conv2)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.leaky_relu(self.bn4(self.conv4(conv3)), negative_slope=0.2, inplace=True)\n",
    "        conv4 = F.sigmoid(self.conv5(conv4))\n",
    "        return conv4.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNetGeneratorSkip(start_filter=32).cuda()\n",
    "discriminator = Discriminator(start_filter=32).cuda()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "clamp_lower = -0.03\n",
    "clamp_upper = 0.03\n",
    "loss_criterion = nn.MSELoss().cuda()\n",
    "save_dir = 'inpainting/samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(epoch, fake_images, real_images, real_examples_full):\n",
    "    j = np.random.randint(low=0, high=500)\n",
    "    real_examples_full, real_examples, fake_images = iterator.get_valid_minibatch(j, 32)\n",
    "    generator.eval()\n",
    "    reconstructions = generator(fake_images)\n",
    "    # fig = plt.figure(figsize=(20, 40))\n",
    "    # idx = 1\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    real = real_examples_full.data.cpu().numpy()\n",
    "    real_copy = copy.deepcopy(real)\n",
    "    real_copy[:, :, 16:48, 16:48] = reconstructions\n",
    "    real_copy = torch.from_numpy(real_copy)\n",
    "    real = torch.from_numpy(real)\n",
    "    out_tensor = torch.zeros(1, real_copy.size(1), real_copy.size(2), real_copy.size(3))\n",
    "    for zz, zzz in zip(real_copy[:10], real[:10]):\n",
    "        out_tensor = torch.cat([out_tensor, zz.unsqueeze(0)])\n",
    "        out_tensor = torch.cat([out_tensor, zzz.unsqueeze(0)])\n",
    "    vutils.save_image(out_tensor[1:], 'inpainting/samples/epoch_%d_samples.png' % (epoch), normalize=True, scale_each=True, nrow=4)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[0] Loss_D: -0.183781 Loss_G: -0.101822\n",
      "[1] Loss_D: -0.147319 Loss_G: -0.120449\n",
      "[2] Loss_D: -0.090972 Loss_G: -0.166253\n",
      "[3] Loss_D: -0.199895 Loss_G: -0.098228\n",
      "[4] Loss_D: -0.204985 Loss_G: -0.104863\n",
      "[5] Loss_D: -0.069599 Loss_G: -0.199079\n",
      "[6] Loss_D: -0.163615 Loss_G: -0.094405\n",
      "[7] Loss_D: -0.159474 Loss_G: -0.106967\n",
      "[8] Loss_D: -0.195363 Loss_G: -0.100920\n",
      "[9] Loss_D: -0.205226 Loss_G: -0.107400\n",
      "[10] Loss_D: -0.206264 Loss_G: -0.109709\n",
      "[11] Loss_D: -0.206622 Loss_G: -0.111383\n",
      "[12] Loss_D: -0.206849 Loss_G: -0.112846\n",
      "[13] Loss_D: -0.206923 Loss_G: -0.114108\n",
      "[14] Loss_D: -0.206977 Loss_G: -0.114994\n",
      "[15] Loss_D: -0.207092 Loss_G: -0.115664\n",
      "[16] Loss_D: -0.207078 Loss_G: -0.115790\n",
      "[17] Loss_D: -0.207123 Loss_G: -0.116650\n",
      "[18] Loss_D: -0.207129 Loss_G: -0.118425\n",
      "[19] Loss_D: -0.207092 Loss_G: -0.119325\n",
      "[20] Loss_D: -0.207112 Loss_G: -0.120400\n",
      "[21] Loss_D: -0.207141 Loss_G: -0.121075\n",
      "[22] Loss_D: -0.113299 Loss_G: -0.135107\n",
      "[23] Loss_D: -0.196163 Loss_G: -0.106867\n",
      "[24] Loss_D: -0.205439 Loss_G: -0.114841\n",
      "[25] Loss_D: -0.206267 Loss_G: -0.118359\n",
      "[26] Loss_D: -0.206312 Loss_G: -0.120829\n",
      "[27] Loss_D: -0.206674 Loss_G: -0.122144\n",
      "[28] Loss_D: -0.206489 Loss_G: -0.123020\n",
      "[29] Loss_D: -0.206705 Loss_G: -0.123778\n",
      "[30] Loss_D: -0.206859 Loss_G: -0.123900\n",
      "[31] Loss_D: -0.206958 Loss_G: -0.124888\n",
      "[32] Loss_D: -0.207050 Loss_G: -0.125684\n",
      "[33] Loss_D: -0.206596 Loss_G: -0.142662\n",
      "[34] Loss_D: -0.076318 Loss_G: -0.185108\n",
      "[35] Loss_D: -0.178913 Loss_G: -0.125275\n",
      "[36] Loss_D: -0.178623 Loss_G: -0.113717\n",
      "[37] Loss_D: -0.204278 Loss_G: -0.119255\n",
      "[38] Loss_D: -0.205662 Loss_G: -0.122744\n",
      "[39] Loss_D: -0.206058 Loss_G: -0.124495\n",
      "[40] Loss_D: -0.206492 Loss_G: -0.125669\n",
      "[41] Loss_D: -0.206682 Loss_G: -0.126448\n",
      "[42] Loss_D: -0.206701 Loss_G: -0.127285\n",
      "[43] Loss_D: -0.206756 Loss_G: -0.128012\n",
      "[44] Loss_D: -0.206828 Loss_G: -0.128217\n",
      "[45] Loss_D: -0.206945 Loss_G: -0.128077\n",
      "[46] Loss_D: -0.206972 Loss_G: -0.127910\n",
      "[47] Loss_D: -0.206413 Loss_G: -0.128462\n",
      "[48] Loss_D: -0.201707 Loss_G: -0.128974\n",
      "[49] Loss_D: -0.005485 Loss_G: -0.258322\n",
      "[50] Loss_D: -0.056510 Loss_G: -0.154212\n",
      "[51] Loss_D: -0.107946 Loss_G: -0.135441\n",
      "[52] Loss_D: -0.101627 Loss_G: -0.146960\n",
      "[53] Loss_D: -0.182300 Loss_G: -0.117019\n",
      "[54] Loss_D: -0.203368 Loss_G: -0.120645\n",
      "[55] Loss_D: -0.204647 Loss_G: -0.122983\n",
      "[56] Loss_D: -0.204412 Loss_G: -0.124768\n",
      "[57] Loss_D: -0.119853 Loss_G: -0.140712\n",
      "[58] Loss_D: -0.198266 Loss_G: -0.121206\n",
      "[59] Loss_D: -0.201796 Loss_G: -0.125619\n",
      "[60] Loss_D: -0.203670 Loss_G: -0.127216\n",
      "[61] Loss_D: -0.106207 Loss_G: -0.193010\n",
      "[62] Loss_D: -0.116491 Loss_G: -0.175385\n",
      "[63] Loss_D: -0.145617 Loss_G: -0.156886\n",
      "[64] Loss_D: -0.178499 Loss_G: -0.127215\n",
      "[65] Loss_D: -0.190403 Loss_G: -0.129135\n",
      "[66] Loss_D: -0.168983 Loss_G: -0.130507\n",
      "[67] Loss_D: -0.200967 Loss_G: -0.127665\n",
      "[68] Loss_D: -0.202062 Loss_G: -0.128938\n",
      "[69] Loss_D: -0.203264 Loss_G: -0.129730\n",
      "[70] Loss_D: -0.115270 Loss_G: -0.134413\n",
      "[71] Loss_D: -0.156861 Loss_G: -0.131664\n",
      "[72] Loss_D: -0.200520 Loss_G: -0.126822\n",
      "[73] Loss_D: -0.203652 Loss_G: -0.129113\n",
      "[74] Loss_D: -0.203886 Loss_G: -0.129804\n",
      "[75] Loss_D: -0.204772 Loss_G: -0.130311\n",
      "[76] Loss_D: -0.205068 Loss_G: -0.130846\n",
      "[77] Loss_D: -0.205640 Loss_G: -0.131173\n",
      "[78] Loss_D: -0.205710 Loss_G: -0.131233\n",
      "[79] Loss_D: -0.205966 Loss_G: -0.131339\n",
      "[80] Loss_D: -0.206081 Loss_G: -0.131480\n",
      "[81] Loss_D: -0.206053 Loss_G: -0.131604\n",
      "[82] Loss_D: -0.205967 Loss_G: -0.131406\n",
      "[83] Loss_D: -0.206133 Loss_G: -0.131476\n",
      "[84] Loss_D: -0.205749 Loss_G: -0.131600\n",
      "[85] Loss_D: -0.206126 Loss_G: -0.131383\n",
      "[86] Loss_D: -0.150999 Loss_G: -0.143700\n",
      "[87] Loss_D: -0.104856 Loss_G: -0.172136\n",
      "[88] Loss_D: -0.198568 Loss_G: -0.118286\n",
      "[89] Loss_D: -0.204236 Loss_G: -0.126561\n",
      "[90] Loss_D: -0.205139 Loss_G: -0.128662\n",
      "[91] Loss_D: -0.205449 Loss_G: -0.129988\n",
      "[92] Loss_D: -0.197379 Loss_G: -0.132433\n",
      "[93] Loss_D: -0.112348 Loss_G: -0.141056\n",
      "[94] Loss_D: -0.173000 Loss_G: -0.129152\n",
      "[95] Loss_D: -0.195817 Loss_G: -0.129898\n",
      "[96] Loss_D: -0.172989 Loss_G: -0.143008\n",
      "[97] Loss_D: -0.203717 Loss_G: -0.131265\n",
      "[98] Loss_D: -0.205210 Loss_G: -0.132223\n",
      "[99] Loss_D: -0.205946 Loss_G: -0.132712\n",
      "[100] Loss_D: -0.206225 Loss_G: -0.133091\n",
      "[101] Loss_D: -0.206384 Loss_G: -0.133546\n",
      "[102] Loss_D: -0.206444 Loss_G: -0.133639\n",
      "[103] Loss_D: -0.206628 Loss_G: -0.133861\n",
      "[104] Loss_D: -0.206725 Loss_G: -0.134156\n",
      "[105] Loss_D: -0.206792 Loss_G: -0.134225\n",
      "[106] Loss_D: -0.206852 Loss_G: -0.134346\n",
      "[107] Loss_D: -0.206844 Loss_G: -0.134510\n",
      "[108] Loss_D: -0.206896 Loss_G: -0.134410\n",
      "[109] Loss_D: -0.206897 Loss_G: -0.134513\n",
      "[110] Loss_D: -0.206996 Loss_G: -0.134875\n",
      "[111] Loss_D: -0.206970 Loss_G: -0.134803\n",
      "[112] Loss_D: -0.206938 Loss_G: -0.134830\n",
      "[113] Loss_D: -0.206968 Loss_G: -0.134990\n",
      "[114] Loss_D: -0.207022 Loss_G: -0.134891\n",
      "[115] Loss_D: -0.206994 Loss_G: -0.135127\n",
      "[116] Loss_D: -0.207037 Loss_G: -0.135600\n",
      "[117] Loss_D: -0.207074 Loss_G: -0.135710\n",
      "[118] Loss_D: -0.206668 Loss_G: -0.136015\n",
      "[119] Loss_D: -0.074134 Loss_G: -0.223585\n",
      "[120] Loss_D: -0.093239 Loss_G: -0.169178\n",
      "[121] Loss_D: -0.144196 Loss_G: -0.138328\n",
      "[122] Loss_D: -0.111106 Loss_G: -0.154048\n"
=======
      "[0] Loss_D: -0.439292 Loss_G: 0.309692\n",
      "[1] Loss_D: -0.402413 Loss_G: 0.334287\n",
      "[2] Loss_D: -0.587064 Loss_G: 0.347931\n",
      "[3] Loss_D: -0.600137 Loss_G: 0.344554\n",
      "[4] Loss_D: -0.504697 Loss_G: 0.329279\n",
      "[5] Loss_D: -0.601465 Loss_G: 0.351421\n",
      "[6] Loss_D: -0.604272 Loss_G: 0.345129\n",
      "[7] Loss_D: -0.605835 Loss_G: 0.342286\n",
      "[8] Loss_D: -0.606709 Loss_G: 0.340411\n",
      "[9] Loss_D: -0.607101 Loss_G: 0.338946\n",
      "[10] Loss_D: -0.607223 Loss_G: 0.337726\n",
      "[11] Loss_D: -0.607355 Loss_G: 0.336719\n",
      "[12] Loss_D: -0.607470 Loss_G: 0.335347\n",
      "[13] Loss_D: -0.607780 Loss_G: 0.335784\n",
      "[14] Loss_D: -0.606452 Loss_G: 0.332856\n",
      "[15] Loss_D: -0.607708 Loss_G: 0.332669\n",
      "[16] Loss_D: -0.579303 Loss_G: 0.285913\n",
      "[17] Loss_D: -0.211625 Loss_G: 0.126704\n",
      "[18] Loss_D: -0.515363 Loss_G: 0.343041\n",
      "[19] Loss_D: -0.434852 Loss_G: 0.328132\n",
      "[20] Loss_D: -0.434897 Loss_G: 0.325758\n",
      "[21] Loss_D: -0.483778 Loss_G: 0.330869\n",
      "[22] Loss_D: -0.488898 Loss_G: 0.291170\n",
      "[23] Loss_D: -0.352322 Loss_G: 0.234720\n",
      "[24] Loss_D: -0.517652 Loss_G: 0.330959\n",
      "[25] Loss_D: -0.448723 Loss_G: 0.320964\n",
      "[26] Loss_D: -0.474812 Loss_G: 0.290770\n",
      "[27] Loss_D: -0.347065 Loss_G: 0.197430\n",
      "[28] Loss_D: -0.490930 Loss_G: 0.322872\n",
      "[29] Loss_D: -0.349041 Loss_G: 0.142317\n",
      "[30] Loss_D: -0.457513 Loss_G: 0.302978\n",
      "[31] Loss_D: -0.572628 Loss_G: 0.338579\n",
      "[32] Loss_D: -0.410986 Loss_G: 0.313671\n",
      "[33] Loss_D: -0.486748 Loss_G: 0.325799\n",
      "[34] Loss_D: -0.533860 Loss_G: 0.329999\n",
      "[35] Loss_D: -0.541028 Loss_G: 0.331915\n",
      "[36] Loss_D: -0.578787 Loss_G: 0.334696\n",
      "[37] Loss_D: -0.441411 Loss_G: 0.254575\n",
      "[38] Loss_D: -0.548394 Loss_G: 0.332525\n",
      "[39] Loss_D: -0.586432 Loss_G: 0.334591\n",
      "[40] Loss_D: -0.588925 Loss_G: 0.333533\n",
      "[41] Loss_D: -0.592363 Loss_G: 0.333435\n",
      "[42] Loss_D: -0.504333 Loss_G: 0.304244\n",
      "[43] Loss_D: -0.596554 Loss_G: 0.348039\n",
      "[44] Loss_D: -0.348921 Loss_G: 0.126430\n",
      "[45] Loss_D: -0.476597 Loss_G: 0.326264\n",
      "[46] Loss_D: -0.524259 Loss_G: 0.329527\n",
      "[47] Loss_D: -0.469069 Loss_G: 0.273147\n",
      "[48] Loss_D: -0.404146 Loss_G: 0.184352\n",
      "[49] Loss_D: -0.494831 Loss_G: 0.251054\n",
      "[50] Loss_D: -0.309235 Loss_G: 0.182488\n",
      "[51] Loss_D: -0.450017 Loss_G: 0.315776\n",
      "[52] Loss_D: -0.487283 Loss_G: 0.324348\n",
      "[53] Loss_D: -0.487808 Loss_G: 0.322859\n",
      "[54] Loss_D: -0.499463 Loss_G: 0.293553\n",
      "[55] Loss_D: -0.485131 Loss_G: 0.315568\n",
      "[56] Loss_D: -0.463405 Loss_G: 0.320338\n",
      "[57] Loss_D: -0.551049 Loss_G: 0.329083\n",
      "[58] Loss_D: -0.496970 Loss_G: 0.294530\n",
      "[59] Loss_D: -0.577383 Loss_G: 0.331355\n",
      "[60] Loss_D: -0.524391 Loss_G: 0.321541\n",
      "[61] Loss_D: -0.514376 Loss_G: 0.323141\n",
      "[62] Loss_D: -0.573834 Loss_G: 0.328459\n",
      "[63] Loss_D: -0.478674 Loss_G: 0.286775\n",
      "[64] Loss_D: -0.559476 Loss_G: 0.330319\n",
      "[65] Loss_D: -0.582357 Loss_G: 0.329707\n",
      "[66] Loss_D: -0.396504 Loss_G: 0.187800\n",
      "[67] Loss_D: -0.507634 Loss_G: 0.316940\n",
      "[68] Loss_D: -0.575723 Loss_G: 0.327943\n",
      "[69] Loss_D: -0.499704 Loss_G: 0.317228\n",
      "[70] Loss_D: -0.455657 Loss_G: 0.239695\n",
      "[71] Loss_D: -0.574979 Loss_G: 0.327071\n",
      "[72] Loss_D: -0.553795 Loss_G: 0.323536\n",
      "[73] Loss_D: -0.509712 Loss_G: 0.305662\n",
      "[74] Loss_D: -0.583938 Loss_G: 0.329175\n",
      "[75] Loss_D: -0.530876 Loss_G: 0.320623\n",
      "[76] Loss_D: -0.547613 Loss_G: 0.288618\n",
      "[77] Loss_D: -0.470233 Loss_G: 0.281083\n",
      "[78] Loss_D: -0.501049 Loss_G: 0.266601\n",
      "[79] Loss_D: -0.521550 Loss_G: 0.327687\n",
      "[80] Loss_D: -0.425883 Loss_G: 0.251576\n",
      "[81] Loss_D: -0.498754 Loss_G: 0.316047\n",
      "[82] Loss_D: -0.470934 Loss_G: 0.315988\n",
      "[83] Loss_D: -0.499920 Loss_G: 0.300362\n",
      "[84] Loss_D: -0.495398 Loss_G: 0.313871\n",
      "[85] Loss_D: -0.419352 Loss_G: 0.307950\n",
      "[86] Loss_D: -0.491271 Loss_G: 0.317528\n",
      "[87] Loss_D: -0.565525 Loss_G: 0.324940\n",
      "[88] Loss_D: -0.556868 Loss_G: 0.323444\n",
      "[89] Loss_D: -0.530991 Loss_G: 0.319462\n",
      "[90] Loss_D: -0.475025 Loss_G: 0.245369\n",
      "[91] Loss_D: -0.542918 Loss_G: 0.323558\n",
      "[92] Loss_D: -0.574421 Loss_G: 0.323960\n",
      "[93] Loss_D: -0.483778 Loss_G: 0.284547\n",
      "[94] Loss_D: -0.593267 Loss_G: 0.338990\n",
      "[95] Loss_D: -0.591897 Loss_G: 0.332354\n",
      "[96] Loss_D: -0.349716 Loss_G: 0.207150\n",
      "[97] Loss_D: -0.446468 Loss_G: 0.314133\n",
      "[98] Loss_D: -0.458389 Loss_G: 0.314212\n",
      "[99] Loss_D: -0.504821 Loss_G: 0.319304\n",
      "[100] Loss_D: -0.548201 Loss_G: 0.322305\n",
      "[101] Loss_D: -0.515303 Loss_G: 0.317727\n",
      "[102] Loss_D: -0.567785 Loss_G: 0.323444\n",
      "[103] Loss_D: -0.574554 Loss_G: 0.324009\n",
      "[104] Loss_D: -0.576085 Loss_G: 0.324059\n",
      "[105] Loss_D: -0.579079 Loss_G: 0.324180\n",
      "[106] Loss_D: -0.553220 Loss_G: 0.320582\n",
      "[107] Loss_D: -0.583422 Loss_G: 0.323916\n",
      "[108] Loss_D: -0.480533 Loss_G: 0.311614\n",
      "[109] Loss_D: -0.501042 Loss_G: 0.313820\n",
      "[110] Loss_D: -0.565777 Loss_G: 0.321207\n",
      "[111] Loss_D: -0.533284 Loss_G: 0.312971\n",
      "[112] Loss_D: -0.579055 Loss_G: 0.322446\n",
      "[113] Loss_D: -0.583254 Loss_G: 0.323022\n",
      "[114] Loss_D: -0.585603 Loss_G: 0.323035\n",
      "[115] Loss_D: -0.503048 Loss_G: 0.271706\n",
      "[116] Loss_D: -0.524466 Loss_G: 0.264035\n",
      "[117] Loss_D: -0.448344 Loss_G: 0.306793\n",
      "[118] Loss_D: -0.484168 Loss_G: 0.310461\n",
      "[119] Loss_D: -0.521742 Loss_G: 0.313430\n",
      "[120] Loss_D: -0.500759 Loss_G: 0.315834\n",
      "[121] Loss_D: -0.517801 Loss_G: 0.314324\n",
      "[122] Loss_D: -0.561328 Loss_G: 0.317972\n",
      "[123] Loss_D: -0.573629 Loss_G: 0.319372\n",
      "[124] Loss_D: -0.574780 Loss_G: 0.319758\n",
      "[125] Loss_D: -0.514473 Loss_G: 0.287568\n",
      "[126] Loss_D: -0.544043 Loss_G: 0.311946\n",
      "[127] Loss_D: -0.519355 Loss_G: 0.294772\n",
      "[128] Loss_D: -0.575861 Loss_G: 0.325502\n",
      "[129] Loss_D: -0.532078 Loss_G: 0.315767\n",
      "[130] Loss_D: -0.435088 Loss_G: 0.238824\n",
      "[131] Loss_D: -0.518974 Loss_G: 0.318027\n",
      "[132] Loss_D: -0.515110 Loss_G: 0.313759\n",
      "[133] Loss_D: -0.507856 Loss_G: 0.313850\n",
      "[134] Loss_D: -0.459047 Loss_G: 0.308082\n",
      "[135] Loss_D: -0.490925 Loss_G: 0.310549\n",
      "[136] Loss_D: -0.533968 Loss_G: 0.315886\n",
      "[137] Loss_D: -0.546693 Loss_G: 0.315988\n",
      "[138] Loss_D: -0.570858 Loss_G: 0.318904\n",
      "[139] Loss_D: -0.554086 Loss_G: 0.316849\n",
      "[140] Loss_D: -0.576790 Loss_G: 0.319179\n",
      "[141] Loss_D: -0.579206 Loss_G: 0.320029\n",
      "[142] Loss_D: -0.482103 Loss_G: 0.309394\n",
      "[143] Loss_D: -0.553711 Loss_G: 0.316634\n",
      "[144] Loss_D: -0.574131 Loss_G: 0.318428\n",
      "[145] Loss_D: -0.513530 Loss_G: 0.300382\n",
      "[146] Loss_D: -0.510618 Loss_G: 0.318450\n",
      "[147] Loss_D: -0.509960 Loss_G: 0.315354\n",
      "[148] Loss_D: -0.499869 Loss_G: 0.311712\n",
      "[149] Loss_D: -0.553887 Loss_G: 0.317428\n",
      "[150] Loss_D: -0.535305 Loss_G: 0.315108\n",
      "[151] Loss_D: -0.571353 Loss_G: 0.318118\n",
      "[152] Loss_D: -0.575892 Loss_G: 0.318773\n",
      "[153] Loss_D: -0.578688 Loss_G: 0.318773\n",
      "[154] Loss_D: -0.495885 Loss_G: 0.310732\n",
      "[155] Loss_D: -0.536787 Loss_G: 0.314977\n",
      "[156] Loss_D: -0.574054 Loss_G: 0.318103\n",
      "[157] Loss_D: -0.467797 Loss_G: 0.308039\n",
      "[158] Loss_D: -0.507291 Loss_G: 0.311074\n",
      "[159] Loss_D: -0.558234 Loss_G: 0.316283\n",
      "[160] Loss_D: -0.551956 Loss_G: 0.314886\n",
      "[161] Loss_D: -0.572751 Loss_G: 0.317411\n",
      "[162] Loss_D: -0.544985 Loss_G: 0.311930\n",
      "[163] Loss_D: -0.577410 Loss_G: 0.317702\n",
      "[164] Loss_D: -0.579392 Loss_G: 0.317411\n",
      "[165] Loss_D: -0.485345 Loss_G: 0.268839\n",
      "[166] Loss_D: -0.572076 Loss_G: 0.317144\n",
      "[167] Loss_D: -0.531009 Loss_G: 0.312895\n",
      "[168] Loss_D: -0.505847 Loss_G: 0.289818\n",
      "[169] Loss_D: -0.583304 Loss_G: 0.319176\n",
      "[170] Loss_D: -0.454472 Loss_G: 0.263878\n",
      "[171] Loss_D: -0.466155 Loss_G: 0.307520\n",
      "[172] Loss_D: -0.541728 Loss_G: 0.314121\n",
      "[173] Loss_D: -0.542166 Loss_G: 0.313419\n",
      "[174] Loss_D: -0.539453 Loss_G: 0.312987\n",
      "[175] Loss_D: -0.565767 Loss_G: 0.315794\n",
      "[176] Loss_D: -0.578903 Loss_G: 0.317008\n",
      "[177] Loss_D: -0.574673 Loss_G: 0.316826\n",
      "[178] Loss_D: -0.531413 Loss_G: 0.311943\n",
      "[179] Loss_D: -0.504876 Loss_G: 0.310326\n",
      "[180] Loss_D: -0.512404 Loss_G: 0.310730\n",
      "[181] Loss_D: -0.552531 Loss_G: 0.314136\n",
      "[182] Loss_D: -0.575014 Loss_G: 0.315998\n",
      "[183] Loss_D: -0.577942 Loss_G: 0.315654\n",
      "[184] Loss_D: -0.580285 Loss_G: 0.315796\n",
      "[185] Loss_D: -0.435528 Loss_G: 0.301060\n",
      "[186] Loss_D: -0.489367 Loss_G: 0.311222\n",
      "[187] Loss_D: -0.511893 Loss_G: 0.310469\n",
      "[188] Loss_D: -0.544468 Loss_G: 0.313196\n",
      "[189] Loss_D: -0.541836 Loss_G: 0.313459\n",
      "[190] Loss_D: -0.542053 Loss_G: 0.312389\n",
      "[191] Loss_D: -0.503590 Loss_G: 0.279891\n",
      "[192] Loss_D: -0.522700 Loss_G: 0.311330\n",
      "[193] Loss_D: -0.552849 Loss_G: 0.314684\n",
      "[194] Loss_D: -0.544226 Loss_G: 0.312825\n",
      "[195] Loss_D: -0.577398 Loss_G: 0.315645\n",
      "[196] Loss_D: -0.577759 Loss_G: 0.315131\n",
      "[197] Loss_D: -0.455120 Loss_G: 0.304586\n",
      "[198] Loss_D: -0.515034 Loss_G: 0.310305\n",
      "[199] Loss_D: -0.551691 Loss_G: 0.312243\n",
      "[200] Loss_D: -0.574185 Loss_G: 0.314618\n",
      "[201] Loss_D: -0.576483 Loss_G: 0.314554\n",
      "[202] Loss_D: -0.548795 Loss_G: 0.311178\n",
      "[203] Loss_D: -0.578487 Loss_G: 0.314260\n",
      "[204] Loss_D: -0.576656 Loss_G: 0.314645\n",
      "[205] Loss_D: -0.463501 Loss_G: 0.304598\n",
      "[206] Loss_D: -0.530632 Loss_G: 0.309960\n",
      "[207] Loss_D: -0.573249 Loss_G: 0.313398\n",
      "[208] Loss_D: -0.576122 Loss_G: 0.313721\n",
      "[209] Loss_D: -0.536862 Loss_G: 0.310037\n",
      "[210] Loss_D: -0.573893 Loss_G: 0.313783\n",
      "[211] Loss_D: -0.578308 Loss_G: 0.313925\n",
      "[212] Loss_D: -0.493131 Loss_G: 0.307126\n",
      "[213] Loss_D: -0.527047 Loss_G: 0.309467\n",
      "[214] Loss_D: -0.557565 Loss_G: 0.310794\n",
      "[215] Loss_D: -0.564889 Loss_G: 0.312131\n",
      "[216] Loss_D: -0.562350 Loss_G: 0.311950\n",
      "[217] Loss_D: -0.579202 Loss_G: 0.313134\n",
      "[218] Loss_D: -0.576747 Loss_G: 0.312695\n",
      "[219] Loss_D: -0.554081 Loss_G: 0.310203\n",
      "[220] Loss_D: -0.552046 Loss_G: 0.309848\n",
      "[221] Loss_D: -0.483680 Loss_G: 0.306011\n",
      "[222] Loss_D: -0.511126 Loss_G: 0.307497\n",
      "[223] Loss_D: -0.489172 Loss_G: 0.306691\n",
      "[224] Loss_D: -0.495165 Loss_G: 0.306930\n",
      "[225] Loss_D: -0.523762 Loss_G: 0.308347\n",
      "[226] Loss_D: -0.558352 Loss_G: 0.311270\n",
      "[227] Loss_D: -0.564753 Loss_G: 0.310827\n",
      "[228] Loss_D: -0.544878 Loss_G: 0.308980\n",
      "[229] Loss_D: -0.523542 Loss_G: 0.307521\n",
      "[230] Loss_D: -0.547588 Loss_G: 0.308883\n",
      "[231] Loss_D: -0.571020 Loss_G: 0.311194\n",
      "[232] Loss_D: -0.519519 Loss_G: 0.306764\n",
      "[233] Loss_D: -0.532961 Loss_G: 0.308092\n",
      "[234] Loss_D: -0.541037 Loss_G: 0.308906\n",
      "[235] Loss_D: -0.555775 Loss_G: 0.310299\n",
      "[236] Loss_D: -0.570898 Loss_G: 0.311268\n",
      "[237] Loss_D: -0.539197 Loss_G: 0.307305\n",
      "[238] Loss_D: -0.483970 Loss_G: 0.304542\n",
      "[239] Loss_D: -0.550968 Loss_G: 0.310238\n",
      "[240] Loss_D: -0.554554 Loss_G: 0.309747\n",
      "[241] Loss_D: -0.567315 Loss_G: 0.310764\n",
      "[242] Loss_D: -0.568221 Loss_G: 0.310437\n",
      "[243] Loss_D: -0.567885 Loss_G: 0.310020\n",
      "[244] Loss_D: -0.568604 Loss_G: 0.310277\n",
      "[245] Loss_D: -0.463515 Loss_G: 0.303171\n",
      "[246] Loss_D: -0.524842 Loss_G: 0.307620\n",
      "[247] Loss_D: -0.521960 Loss_G: 0.307185\n",
      "[248] Loss_D: -0.547744 Loss_G: 0.309803\n",
      "[249] Loss_D: -0.547354 Loss_G: 0.308399\n",
      "[250] Loss_D: -0.569820 Loss_G: 0.310320\n",
      "[251] Loss_D: -0.579045 Loss_G: 0.311478\n",
      "[252] Loss_D: -0.577775 Loss_G: 0.310830\n",
      "[253] Loss_D: -0.565397 Loss_G: 0.310292\n",
      "[254] Loss_D: -0.430461 Loss_G: 0.301654\n",
      "[255] Loss_D: -0.496157 Loss_G: 0.304965\n",
      "[256] Loss_D: -0.540210 Loss_G: 0.307670\n",
      "[257] Loss_D: -0.563607 Loss_G: 0.309437\n",
      "[258] Loss_D: -0.572381 Loss_G: 0.310112\n",
      "[259] Loss_D: -0.505133 Loss_G: 0.289977\n",
      "[260] Loss_D: -0.474017 Loss_G: 0.304397\n",
      "[261] Loss_D: -0.513350 Loss_G: 0.307579\n",
      "[262] Loss_D: -0.533506 Loss_G: 0.308157\n",
      "[263] Loss_D: -0.559028 Loss_G: 0.309378\n",
      "[264] Loss_D: -0.566489 Loss_G: 0.309316\n",
      "[265] Loss_D: -0.573542 Loss_G: 0.310246\n",
      "[266] Loss_D: -0.541785 Loss_G: 0.307176\n",
      "[267] Loss_D: -0.461285 Loss_G: 0.302167\n",
      "[268] Loss_D: -0.512944 Loss_G: 0.306032\n",
      "[269] Loss_D: -0.528974 Loss_G: 0.306824\n",
      "[270] Loss_D: -0.541743 Loss_G: 0.308307\n",
      "[271] Loss_D: -0.563625 Loss_G: 0.308815\n",
      "[272] Loss_D: -0.576881 Loss_G: 0.310286\n",
      "[273] Loss_D: -0.501748 Loss_G: 0.296602\n",
      "[274] Loss_D: -0.491277 Loss_G: 0.306782\n",
      "[275] Loss_D: -0.541097 Loss_G: 0.308643\n",
      "[276] Loss_D: -0.495013 Loss_G: 0.306016\n",
      "[277] Loss_D: -0.525594 Loss_G: 0.306553\n",
      "[278] Loss_D: -0.552484 Loss_G: 0.308011\n",
      "[279] Loss_D: -0.550991 Loss_G: 0.308681\n",
      "[280] Loss_D: -0.506154 Loss_G: 0.304388\n",
      "[281] Loss_D: -0.502180 Loss_G: 0.305835\n",
      "[282] Loss_D: -0.562966 Loss_G: 0.309182\n",
      "[283] Loss_D: -0.546912 Loss_G: 0.307631\n",
      "[284] Loss_D: -0.573345 Loss_G: 0.309554\n",
      "[285] Loss_D: -0.468780 Loss_G: 0.303561\n",
      "[286] Loss_D: -0.486601 Loss_G: 0.303413\n",
      "[287] Loss_D: -0.540876 Loss_G: 0.307130\n",
      "[288] Loss_D: -0.539629 Loss_G: 0.305951\n",
      "[289] Loss_D: -0.512705 Loss_G: 0.304997\n",
      "[290] Loss_D: -0.533471 Loss_G: 0.306173\n",
      "[291] Loss_D: -0.560214 Loss_G: 0.307858\n",
      "[292] Loss_D: -0.577104 Loss_G: 0.309267\n",
      "[293] Loss_D: -0.549941 Loss_G: 0.306677\n",
      "[294] Loss_D: -0.519983 Loss_G: 0.305290\n",
      "[295] Loss_D: -0.498030 Loss_G: 0.303437\n",
      "[296] Loss_D: -0.531770 Loss_G: 0.306125\n",
      "[297] Loss_D: -0.548343 Loss_G: 0.306647\n",
      "[298] Loss_D: -0.566589 Loss_G: 0.307697\n",
      "[299] Loss_D: -0.573939 Loss_G: 0.308409\n",
      "[300] Loss_D: -0.557959 Loss_G: 0.307703\n",
      "[301] Loss_D: -0.464380 Loss_G: 0.299505\n",
      "[302] Loss_D: -0.466953 Loss_G: 0.303183\n",
      "[303] Loss_D: -0.447416 Loss_G: 0.298364\n",
      "[304] Loss_D: -0.435151 Loss_G: 0.305769\n",
      "[305] Loss_D: -0.433922 Loss_G: 0.302010\n",
      "[306] Loss_D: -0.465733 Loss_G: 0.305602\n",
      "[307] Loss_D: -0.436815 Loss_G: 0.303313\n",
      "[308] Loss_D: -0.490981 Loss_G: 0.302749\n",
      "[309] Loss_D: -0.520884 Loss_G: 0.311092\n",
      "[310] Loss_D: -0.485305 Loss_G: 0.307032\n",
      "[311] Loss_D: -0.505471 Loss_G: 0.306166\n",
      "[312] Loss_D: -0.516450 Loss_G: 0.306169\n",
      "[313] Loss_D: -0.530261 Loss_G: 0.306922\n",
      "[314] Loss_D: -0.548043 Loss_G: 0.308083\n",
      "[315] Loss_D: -0.496113 Loss_G: 0.303941\n",
      "[316] Loss_D: -0.558456 Loss_G: 0.308416\n",
      "[317] Loss_D: -0.514363 Loss_G: 0.304540\n",
      "[318] Loss_D: -0.559262 Loss_G: 0.307925\n",
      "[319] Loss_D: -0.563554 Loss_G: 0.308458\n",
      "[320] Loss_D: -0.530240 Loss_G: 0.302967\n",
      "[321] Loss_D: -0.479152 Loss_G: 0.302996\n",
      "[322] Loss_D: -0.531396 Loss_G: 0.306182\n",
      "[323] Loss_D: -0.560909 Loss_G: 0.307694\n",
      "[324] Loss_D: -0.573291 Loss_G: 0.309238\n",
      "[325] Loss_D: -0.573974 Loss_G: 0.308752\n",
      "[326] Loss_D: -0.506050 Loss_G: 0.302647\n",
      "[327] Loss_D: -0.494269 Loss_G: 0.302967\n",
      "[328] Loss_D: -0.556644 Loss_G: 0.307385\n",
      "[329] Loss_D: -0.560318 Loss_G: 0.307475\n",
      "[330] Loss_D: -0.559757 Loss_G: 0.307159\n",
      "[331] Loss_D: -0.580190 Loss_G: 0.308896\n",
      "[332] Loss_D: -0.576390 Loss_G: 0.308562\n",
      "[333] Loss_D: -0.465688 Loss_G: 0.294293\n",
      "[334] Loss_D: -0.508793 Loss_G: 0.304025\n",
      "[335] Loss_D: -0.491866 Loss_G: 0.302328\n",
      "[336] Loss_D: -0.523666 Loss_G: 0.305600\n",
      "[337] Loss_D: -0.511132 Loss_G: 0.304465\n",
      "[338] Loss_D: -0.521424 Loss_G: 0.305753\n",
      "[339] Loss_D: -0.532949 Loss_G: 0.304802\n",
      "[340] Loss_D: -0.572177 Loss_G: 0.308376\n",
      "[341] Loss_D: -0.575556 Loss_G: 0.308408\n",
      "[342] Loss_D: -0.568249 Loss_G: 0.307311\n",
      "[343] Loss_D: -0.508775 Loss_G: 0.298922\n",
      "[344] Loss_D: -0.494463 Loss_G: 0.305118\n",
      "[345] Loss_D: -0.481526 Loss_G: 0.303670\n",
      "[346] Loss_D: -0.446236 Loss_G: 0.303206\n",
      "[347] Loss_D: -0.485158 Loss_G: 0.303871\n",
      "[348] Loss_D: -0.465075 Loss_G: 0.282178\n",
      "[349] Loss_D: -0.563970 Loss_G: 0.311227\n",
      "[350] Loss_D: -0.446446 Loss_G: 0.301715\n",
      "[351] Loss_D: -0.434119 Loss_G: 0.283946\n",
      "[352] Loss_D: -0.482491 Loss_G: 0.303558\n",
      "[353] Loss_D: -0.445974 Loss_G: 0.302187\n",
      "[354] Loss_D: -0.512401 Loss_G: 0.305269\n",
      "[355] Loss_D: -0.518445 Loss_G: 0.304865\n",
      "[356] Loss_D: -0.508067 Loss_G: 0.304366\n",
      "[357] Loss_D: -0.522888 Loss_G: 0.305014\n",
      "[358] Loss_D: -0.555107 Loss_G: 0.307114\n",
      "[359] Loss_D: -0.510703 Loss_G: 0.302940\n",
      "[360] Loss_D: -0.545840 Loss_G: 0.305887\n",
      "[361] Loss_D: -0.561118 Loss_G: 0.307256\n",
      "[362] Loss_D: -0.573216 Loss_G: 0.308374\n",
      "[363] Loss_D: -0.494086 Loss_G: 0.301601\n",
      "[364] Loss_D: -0.508549 Loss_G: 0.304243\n",
      "[365] Loss_D: -0.563632 Loss_G: 0.307711\n",
      "[366] Loss_D: -0.533333 Loss_G: 0.304944\n",
      "[367] Loss_D: -0.531792 Loss_G: 0.304827\n",
      "[368] Loss_D: -0.571337 Loss_G: 0.307334\n",
      "[369] Loss_D: -0.575157 Loss_G: 0.307934\n",
      "[370] Loss_D: -0.571553 Loss_G: 0.307752\n",
      "[371] Loss_D: -0.570822 Loss_G: 0.307033\n",
      "[372] Loss_D: -0.491924 Loss_G: 0.302619\n",
      "[373] Loss_D: -0.446850 Loss_G: 0.300980\n",
      "[374] Loss_D: -0.516666 Loss_G: 0.304231\n",
      "[375] Loss_D: -0.525855 Loss_G: 0.303748\n",
      "[376] Loss_D: -0.558272 Loss_G: 0.306286\n",
      "[377] Loss_D: -0.470447 Loss_G: 0.301488\n",
      "[378] Loss_D: -0.538717 Loss_G: 0.305004\n",
      "[379] Loss_D: -0.513157 Loss_G: 0.302846\n",
      "[380] Loss_D: -0.502969 Loss_G: 0.302487\n",
      "[381] Loss_D: -0.513595 Loss_G: 0.304428\n",
      "[382] Loss_D: -0.517622 Loss_G: 0.303919\n",
      "[383] Loss_D: -0.529329 Loss_G: 0.305147\n",
      "[384] Loss_D: -0.567146 Loss_G: 0.307122\n",
      "[385] Loss_D: -0.575984 Loss_G: 0.307677\n",
      "[386] Loss_D: -0.569016 Loss_G: 0.307326\n",
      "[387] Loss_D: -0.481039 Loss_G: 0.300012\n",
      "[388] Loss_D: -0.470514 Loss_G: 0.299749\n",
      "[389] Loss_D: -0.547116 Loss_G: 0.306239\n",
      "[390] Loss_D: -0.543652 Loss_G: 0.305046\n",
      "[391] Loss_D: -0.533505 Loss_G: 0.302748\n",
      "[392] Loss_D: -0.568034 Loss_G: 0.307271\n",
      "[393] Loss_D: -0.534117 Loss_G: 0.301337\n",
      "[394] Loss_D: -0.497302 Loss_G: 0.301146\n",
      "[395] Loss_D: -0.512965 Loss_G: 0.302355\n",
      "[396] Loss_D: -0.515431 Loss_G: 0.302238\n",
      "[397] Loss_D: -0.508600 Loss_G: 0.302676\n",
      "[398] Loss_D: -0.557256 Loss_G: 0.305646\n",
      "[399] Loss_D: -0.564305 Loss_G: 0.306018\n",
      "[400] Loss_D: -0.555407 Loss_G: 0.305970\n",
      "[401] Loss_D: -0.485391 Loss_G: 0.300948\n",
      "[402] Loss_D: -0.514179 Loss_G: 0.303016\n",
      "[403] Loss_D: -0.516943 Loss_G: 0.302589\n",
      "[404] Loss_D: -0.543506 Loss_G: 0.304889\n",
      "[405] Loss_D: -0.567989 Loss_G: 0.306410\n",
      "[406] Loss_D: -0.501749 Loss_G: 0.300703\n",
      "[407] Loss_D: -0.483046 Loss_G: 0.301281\n",
      "[408] Loss_D: -0.538681 Loss_G: 0.304140\n",
      "[409] Loss_D: -0.565856 Loss_G: 0.306504\n",
      "[410] Loss_D: -0.507673 Loss_G: 0.302110\n",
      "[411] Loss_D: -0.530366 Loss_G: 0.304006\n",
      "[412] Loss_D: -0.556304 Loss_G: 0.305252\n",
      "[413] Loss_D: -0.486526 Loss_G: 0.299749\n",
      "[414] Loss_D: -0.530934 Loss_G: 0.303299\n",
      "[415] Loss_D: -0.565652 Loss_G: 0.306221\n",
      "[416] Loss_D: -0.561381 Loss_G: 0.305742\n",
      "[417] Loss_D: -0.507317 Loss_G: 0.300422\n",
      "[418] Loss_D: -0.549623 Loss_G: 0.308589\n",
      "[419] Loss_D: -0.482035 Loss_G: 0.301635\n",
      "[420] Loss_D: -0.530459 Loss_G: 0.304841\n",
      "[421] Loss_D: -0.562758 Loss_G: 0.306257\n",
      "[422] Loss_D: -0.547161 Loss_G: 0.305067\n",
      "[423] Loss_D: -0.529041 Loss_G: 0.303203\n",
      "[424] Loss_D: -0.504052 Loss_G: 0.301982\n",
      "[425] Loss_D: -0.546451 Loss_G: 0.303979\n",
      "[426] Loss_D: -0.565852 Loss_G: 0.305775\n",
      "[427] Loss_D: -0.559534 Loss_G: 0.305484\n",
      "[428] Loss_D: -0.504907 Loss_G: 0.300608\n",
      "[429] Loss_D: -0.556439 Loss_G: 0.305394\n",
      "[430] Loss_D: -0.558033 Loss_G: 0.305525\n",
      "[431] Loss_D: -0.575046 Loss_G: 0.306549\n",
      "[432] Loss_D: -0.573068 Loss_G: 0.306340\n",
      "[433] Loss_D: -0.569180 Loss_G: 0.305952\n",
      "[434] Loss_D: -0.477290 Loss_G: 0.300453\n",
      "[435] Loss_D: -0.451556 Loss_G: 0.297618\n",
      "[436] Loss_D: -0.534104 Loss_G: 0.303681\n",
      "[437] Loss_D: -0.522632 Loss_G: 0.302864\n",
      "[438] Loss_D: -0.536829 Loss_G: 0.303238\n",
      "[439] Loss_D: -0.543760 Loss_G: 0.303681\n",
      "[440] Loss_D: -0.569315 Loss_G: 0.305685\n",
      "[441] Loss_D: -0.570086 Loss_G: 0.305788\n",
      "[442] Loss_D: -0.558415 Loss_G: 0.303936\n",
      "[443] Loss_D: -0.424917 Loss_G: 0.286048\n",
      "[444] Loss_D: -0.408629 Loss_G: 0.291648\n",
      "[445] Loss_D: -0.491454 Loss_G: 0.303989\n",
      "[446] Loss_D: -0.481928 Loss_G: 0.301669\n",
      "[447] Loss_D: -0.495030 Loss_G: 0.302473\n",
      "[448] Loss_D: -0.518759 Loss_G: 0.303168\n",
      "[449] Loss_D: -0.554016 Loss_G: 0.305431\n",
      "[450] Loss_D: -0.533662 Loss_G: 0.303458\n",
      "[451] Loss_D: -0.523904 Loss_G: 0.302627\n",
      "[452] Loss_D: -0.552376 Loss_G: 0.305068\n",
      "[453] Loss_D: -0.529693 Loss_G: 0.301792\n",
      "[454] Loss_D: -0.477560 Loss_G: 0.299837\n",
      "[455] Loss_D: -0.537411 Loss_G: 0.302983\n",
      "[456] Loss_D: -0.541988 Loss_G: 0.303704\n",
      "[457] Loss_D: -0.554467 Loss_G: 0.304403\n",
      "[458] Loss_D: -0.534648 Loss_G: 0.302767\n",
      "[459] Loss_D: -0.524692 Loss_G: 0.302353\n",
      "[460] Loss_D: -0.495014 Loss_G: 0.299940\n",
      "[461] Loss_D: -0.464381 Loss_G: 0.298566\n",
      "[462] Loss_D: -0.537385 Loss_G: 0.303862\n",
      "[463] Loss_D: -0.552662 Loss_G: 0.304173\n",
      "[464] Loss_D: -0.549856 Loss_G: 0.304288\n",
      "[465] Loss_D: -0.523254 Loss_G: 0.301789\n",
      "[466] Loss_D: -0.500750 Loss_G: 0.300082\n",
      "[467] Loss_D: -0.561425 Loss_G: 0.304761\n",
      "[468] Loss_D: -0.574067 Loss_G: 0.305870\n",
      "[469] Loss_D: -0.573144 Loss_G: 0.305319\n",
      "[470] Loss_D: -0.558683 Loss_G: 0.304606\n",
      "[471] Loss_D: -0.423745 Loss_G: 0.294259\n",
      "[472] Loss_D: -0.474864 Loss_G: 0.300099\n",
      "[473] Loss_D: -0.550636 Loss_G: 0.304180\n",
      "[474] Loss_D: -0.566011 Loss_G: 0.305398\n",
      "[475] Loss_D: -0.519935 Loss_G: 0.297790\n",
      "[476] Loss_D: -0.528275 Loss_G: 0.303237\n",
      "[477] Loss_D: -0.526533 Loss_G: 0.302549\n",
      "[478] Loss_D: -0.563458 Loss_G: 0.305043\n",
      "[479] Loss_D: -0.480763 Loss_G: 0.298164\n",
      "[480] Loss_D: -0.545448 Loss_G: 0.303217\n",
      "[481] Loss_D: -0.533937 Loss_G: 0.301645\n",
      "[482] Loss_D: -0.479121 Loss_G: 0.300288\n",
      "[483] Loss_D: -0.537803 Loss_G: 0.303084\n",
      "[484] Loss_D: -0.565361 Loss_G: 0.305223\n",
      "[485] Loss_D: -0.567998 Loss_G: 0.304987\n",
      "[486] Loss_D: -0.478910 Loss_G: 0.298716\n",
      "[487] Loss_D: -0.514792 Loss_G: 0.301549\n",
      "[488] Loss_D: -0.545949 Loss_G: 0.303025\n",
      "[489] Loss_D: -0.481859 Loss_G: 0.298178\n",
      "[490] Loss_D: -0.526948 Loss_G: 0.301823\n",
      "[491] Loss_D: -0.554155 Loss_G: 0.303829\n",
      "[492] Loss_D: -0.531225 Loss_G: 0.297881\n",
      "[493] Loss_D: -0.496902 Loss_G: 0.300059\n",
      "[494] Loss_D: -0.533328 Loss_G: 0.301515\n",
      "[495] Loss_D: -0.543542 Loss_G: 0.303326\n",
      "[496] Loss_D: -0.547469 Loss_G: 0.302568\n",
      "[497] Loss_D: -0.552613 Loss_G: 0.303820\n",
      "[498] Loss_D: -0.498190 Loss_G: 0.299285\n",
      "[499] Loss_D: -0.450374 Loss_G: 0.297984\n",
      "[500] Loss_D: -0.546321 Loss_G: 0.303095\n",
      "[501] Loss_D: -0.514540 Loss_G: 0.300944\n",
      "[502] Loss_D: -0.474625 Loss_G: 0.296460\n",
      "[503] Loss_D: -0.530331 Loss_G: 0.301706\n",
      "[504] Loss_D: -0.555031 Loss_G: 0.303575\n",
      "[505] Loss_D: -0.567141 Loss_G: 0.304955\n",
      "[506] Loss_D: -0.465305 Loss_G: 0.296519\n",
      "[507] Loss_D: -0.528593 Loss_G: 0.302038\n",
      "[508] Loss_D: -0.549404 Loss_G: 0.303190\n",
      "[509] Loss_D: -0.531969 Loss_G: 0.300099\n",
      "[510] Loss_D: -0.479491 Loss_G: 0.297033\n",
      "[511] Loss_D: -0.472297 Loss_G: 0.298934\n",
      "[512] Loss_D: -0.520285 Loss_G: 0.301397\n",
      "[513] Loss_D: -0.551976 Loss_G: 0.303600\n",
      "[514] Loss_D: -0.560779 Loss_G: 0.304334\n",
      "[515] Loss_D: -0.514471 Loss_G: 0.291987\n",
      "[516] Loss_D: -0.452118 Loss_G: 0.297160\n",
      "[517] Loss_D: -0.520651 Loss_G: 0.301372\n",
      "[518] Loss_D: -0.546872 Loss_G: 0.302821\n",
      "[519] Loss_D: -0.554929 Loss_G: 0.303567\n",
      "[520] Loss_D: -0.558024 Loss_G: 0.304305\n",
      "[521] Loss_D: -0.566495 Loss_G: 0.304601\n",
      "[522] Loss_D: -0.502261 Loss_G: 0.297805\n",
      "[523] Loss_D: -0.516314 Loss_G: 0.300619\n",
      "[524] Loss_D: -0.558001 Loss_G: 0.304067\n",
      "[525] Loss_D: -0.568296 Loss_G: 0.304892\n",
      "[526] Loss_D: -0.568556 Loss_G: 0.304530\n",
      "[527] Loss_D: -0.556679 Loss_G: 0.303522\n",
      "[528] Loss_D: -0.507719 Loss_G: 0.300606\n",
      "[529] Loss_D: -0.475077 Loss_G: 0.295696\n",
      "[530] Loss_D: -0.506278 Loss_G: 0.300117\n",
      "[531] Loss_D: -0.494569 Loss_G: 0.299341\n",
      "[532] Loss_D: -0.500034 Loss_G: 0.300717\n",
      "[533] Loss_D: -0.539434 Loss_G: 0.302324\n",
      "[534] Loss_D: -0.542594 Loss_G: 0.302251\n",
      "[535] Loss_D: -0.562205 Loss_G: 0.304831\n",
      "[536] Loss_D: -0.570069 Loss_G: 0.305070\n",
      "[537] Loss_D: -0.520133 Loss_G: 0.299308\n",
      "[538] Loss_D: -0.475328 Loss_G: 0.301614\n",
      "[539] Loss_D: -0.506624 Loss_G: 0.300819\n",
      "[540] Loss_D: -0.540776 Loss_G: 0.303055\n",
      "[541] Loss_D: -0.484844 Loss_G: 0.296854\n",
      "[542] Loss_D: -0.533384 Loss_G: 0.301665\n",
      "[543] Loss_D: -0.560754 Loss_G: 0.304297\n",
      "[544] Loss_D: -0.566367 Loss_G: 0.304898\n",
      "[545] Loss_D: -0.550834 Loss_G: 0.303024\n",
      "[546] Loss_D: -0.467029 Loss_G: 0.289482\n",
      "[547] Loss_D: -0.487376 Loss_G: 0.299454\n",
      "[548] Loss_D: -0.521152 Loss_G: 0.301929\n",
      "[549] Loss_D: -0.552577 Loss_G: 0.303413\n",
      "[550] Loss_D: -0.537651 Loss_G: 0.300097\n",
      "[551] Loss_D: -0.550759 Loss_G: 0.302872\n",
      "[552] Loss_D: -0.525167 Loss_G: 0.300485\n",
      "[553] Loss_D: -0.487379 Loss_G: 0.293664\n",
      "[554] Loss_D: -0.527785 Loss_G: 0.301816\n",
      "[555] Loss_D: -0.545831 Loss_G: 0.303120\n",
      "[556] Loss_D: -0.489858 Loss_G: 0.297391\n",
      "[557] Loss_D: -0.546178 Loss_G: 0.303040\n",
      "[558] Loss_D: -0.566230 Loss_G: 0.304608\n",
      "[559] Loss_D: -0.562289 Loss_G: 0.304405\n",
      "[560] Loss_D: -0.468376 Loss_G: 0.294971\n",
      "[561] Loss_D: -0.476059 Loss_G: 0.297234\n",
      "[562] Loss_D: -0.509539 Loss_G: 0.299125\n",
      "[563] Loss_D: -0.527376 Loss_G: 0.300743\n",
      "[564] Loss_D: -0.531378 Loss_G: 0.300703\n",
      "[565] Loss_D: -0.547847 Loss_G: 0.303073\n",
      "[566] Loss_D: -0.568839 Loss_G: 0.304899\n",
      "[567] Loss_D: -0.553964 Loss_G: 0.303830\n",
      "[568] Loss_D: -0.491518 Loss_G: 0.290021\n",
      "[569] Loss_D: -0.502438 Loss_G: 0.300773\n",
      "[570] Loss_D: -0.464214 Loss_G: 0.296452\n",
      "[571] Loss_D: -0.545715 Loss_G: 0.302943\n",
      "[572] Loss_D: -0.563777 Loss_G: 0.303997\n",
      "[573] Loss_D: -0.564945 Loss_G: 0.303948\n",
      "[643] Loss_D: -0.299360 Loss_G: 0.111448\n",
      "[644] Loss_D: -0.555499 Loss_G: 0.312869\n",
      "[645] Loss_D: -0.345699 Loss_G: 0.251411\n",
      "[646] Loss_D: -0.546610 Loss_G: 0.304791\n",
      "[647] Loss_D: -0.374144 Loss_G: 0.159993\n",
      "[648] Loss_D: -0.315718 Loss_G: 0.182305\n",
      "[649] Loss_D: -0.407943 Loss_G: 0.296418\n",
      "[650] Loss_D: -0.461398 Loss_G: 0.298969\n",
      "[651] Loss_D: -0.446819 Loss_G: 0.280732\n",
      "[652] Loss_D: -0.554239 Loss_G: 0.310449\n",
      "[653] Loss_D: -0.444231 Loss_G: 0.286354\n",
      "[654] Loss_D: -0.455553 Loss_G: 0.298060\n",
      "[655] Loss_D: -0.515797 Loss_G: 0.305562\n",
      "[656] Loss_D: -0.559753 Loss_G: 0.309042\n",
      "[657] Loss_D: -0.564864 Loss_G: 0.310193\n",
      "[658] Loss_D: -0.572478 Loss_G: 0.310751\n",
      "[659] Loss_D: -0.576745 Loss_G: 0.311131\n",
      "[660] Loss_D: -0.575509 Loss_G: 0.310954\n",
      "[661] Loss_D: -0.579786 Loss_G: 0.311407\n",
      "[662] Loss_D: -0.493027 Loss_G: 0.299877\n",
      "[663] Loss_D: -0.459638 Loss_G: 0.281318\n",
      "[664] Loss_D: -0.511828 Loss_G: 0.302953\n",
      "[665] Loss_D: -0.484427 Loss_G: 0.302905\n",
      "[666] Loss_D: -0.561944 Loss_G: 0.309104\n",
      "[667] Loss_D: -0.578122 Loss_G: 0.310442\n",
      "[668] Loss_D: -0.450958 Loss_G: 0.294344\n",
      "[669] Loss_D: -0.497044 Loss_G: 0.301763\n",
      "[670] Loss_D: -0.547065 Loss_G: 0.306823\n",
      "[671] Loss_D: -0.574486 Loss_G: 0.309462\n",
      "[672] Loss_D: -0.579337 Loss_G: 0.309885\n",
      "[673] Loss_D: -0.578144 Loss_G: 0.309854\n",
      "[674] Loss_D: -0.579661 Loss_G: 0.310199\n",
      "[675] Loss_D: -0.578741 Loss_G: 0.310031\n",
      "[676] Loss_D: -0.583535 Loss_G: 0.310036\n",
      "[677] Loss_D: -0.578273 Loss_G: 0.309816\n",
      "[678] Loss_D: -0.581967 Loss_G: 0.310018\n",
      "[679] Loss_D: -0.583889 Loss_G: 0.310046\n",
      "[680] Loss_D: -0.406368 Loss_G: 0.280698\n",
      "[681] Loss_D: -0.400936 Loss_G: 0.285995\n",
      "[682] Loss_D: -0.453029 Loss_G: 0.298931\n",
      "[683] Loss_D: -0.515773 Loss_G: 0.297539\n",
      "[684] Loss_D: -0.507817 Loss_G: 0.299549\n",
      "[685] Loss_D: -0.556501 Loss_G: 0.306757\n",
      "[686] Loss_D: -0.543309 Loss_G: 0.303514\n",
      "[687] Loss_D: -0.548975 Loss_G: 0.306367\n",
      "[688] Loss_D: -0.553447 Loss_G: 0.306268\n",
      "[689] Loss_D: -0.574132 Loss_G: 0.308477\n",
      "[690] Loss_D: -0.579429 Loss_G: 0.308841\n",
      "[691] Loss_D: -0.579855 Loss_G: 0.308942\n",
      "[692] Loss_D: -0.571428 Loss_G: 0.307490\n",
      "[693] Loss_D: -0.579830 Loss_G: 0.308761\n",
      "[694] Loss_D: -0.579398 Loss_G: 0.308309\n",
      "[695] Loss_D: -0.579433 Loss_G: 0.308686\n",
      "[696] Loss_D: -0.384505 Loss_G: 0.272805\n",
      "[697] Loss_D: -0.421567 Loss_G: 0.290873\n",
      "[698] Loss_D: -0.460348 Loss_G: 0.296310\n",
      "[699] Loss_D: -0.495318 Loss_G: 0.298668\n",
      "[700] Loss_D: -0.555115 Loss_G: 0.305600\n",
      "[701] Loss_D: -0.518869 Loss_G: 0.286190\n",
      "[702] Loss_D: -0.519817 Loss_G: 0.299262\n",
      "[703] Loss_D: -0.569914 Loss_G: 0.307137\n",
      "[704] Loss_D: -0.454737 Loss_G: 0.292149\n",
      "[705] Loss_D: -0.491953 Loss_G: 0.299677\n",
      "[706] Loss_D: -0.554282 Loss_G: 0.304959\n",
      "[707] Loss_D: -0.513780 Loss_G: 0.300718\n",
      "[708] Loss_D: -0.533742 Loss_G: 0.303636\n",
      "[709] Loss_D: -0.566608 Loss_G: 0.306704\n",
      "[710] Loss_D: -0.573164 Loss_G: 0.307555\n",
      "[711] Loss_D: -0.574729 Loss_G: 0.307344\n",
      "[712] Loss_D: -0.571538 Loss_G: 0.307245\n",
      "[713] Loss_D: -0.570880 Loss_G: 0.306965\n",
      "[714] Loss_D: -0.577675 Loss_G: 0.307679\n",
      "[715] Loss_D: -0.572195 Loss_G: 0.307115\n",
      "[716] Loss_D: -0.539561 Loss_G: 0.295090\n",
      "[717] Loss_D: -0.517178 Loss_G: 0.302556\n",
      "[718] Loss_D: -0.519386 Loss_G: 0.299973\n",
      "[719] Loss_D: -0.464947 Loss_G: 0.292306\n",
      "[720] Loss_D: -0.512356 Loss_G: 0.298149\n",
      "[721] Loss_D: -0.540815 Loss_G: 0.304188\n",
      "[722] Loss_D: -0.568167 Loss_G: 0.306341\n",
      "[723] Loss_D: -0.573622 Loss_G: 0.307034\n",
      "[724] Loss_D: -0.574213 Loss_G: 0.307183\n",
      "[725] Loss_D: -0.571767 Loss_G: 0.306854\n",
      "[726] Loss_D: -0.577625 Loss_G: 0.307500\n",
      "[727] Loss_D: -0.577820 Loss_G: 0.307282\n",
      "[728] Loss_D: -0.574560 Loss_G: 0.306757\n",
      "[729] Loss_D: -0.578834 Loss_G: 0.307374\n",
      "[730] Loss_D: -0.435349 Loss_G: 0.275265\n",
      "[731] Loss_D: -0.450126 Loss_G: 0.294579\n",
      "[732] Loss_D: -0.510827 Loss_G: 0.300796\n",
      "[733] Loss_D: -0.520421 Loss_G: 0.299617\n",
      "[734] Loss_D: -0.549206 Loss_G: 0.303983\n",
      "[735] Loss_D: -0.558827 Loss_G: 0.304897\n",
      "[736] Loss_D: -0.571223 Loss_G: 0.306324\n",
      "[737] Loss_D: -0.573871 Loss_G: 0.306184\n",
      "[738] Loss_D: -0.571896 Loss_G: 0.306505\n",
      "[739] Loss_D: -0.569058 Loss_G: 0.305851\n",
      "[740] Loss_D: -0.572691 Loss_G: 0.306708\n",
      "[741] Loss_D: -0.563036 Loss_G: 0.305159\n",
      "[742] Loss_D: -0.490139 Loss_G: 0.290941\n",
      "[743] Loss_D: -0.494659 Loss_G: 0.297819\n",
      "[744] Loss_D: -0.554222 Loss_G: 0.304524\n",
      "[745] Loss_D: -0.570936 Loss_G: 0.306206\n",
      "[746] Loss_D: -0.574766 Loss_G: 0.306693\n",
      "[747] Loss_D: -0.513732 Loss_G: 0.299831\n",
      "[748] Loss_D: -0.520281 Loss_G: 0.298645\n",
      "[749] Loss_D: -0.562863 Loss_G: 0.304964\n",
      "[750] Loss_D: -0.575473 Loss_G: 0.306639\n",
      "[751] Loss_D: -0.577805 Loss_G: 0.306178\n",
      "[752] Loss_D: -0.571563 Loss_G: 0.305194\n",
      "[753] Loss_D: -0.564203 Loss_G: 0.304994\n",
      "[754] Loss_D: -0.417933 Loss_G: 0.273023\n",
      "[755] Loss_D: -0.479280 Loss_G: 0.298467\n",
      "[756] Loss_D: -0.525692 Loss_G: 0.302075\n",
      "[757] Loss_D: -0.547670 Loss_G: 0.303111\n",
      "[758] Loss_D: -0.563119 Loss_G: 0.305499\n",
      "[759] Loss_D: -0.517472 Loss_G: 0.299027\n",
      "[760] Loss_D: -0.547985 Loss_G: 0.303785\n",
      "[761] Loss_D: -0.572911 Loss_G: 0.306180\n",
      "[762] Loss_D: -0.572669 Loss_G: 0.305899\n",
      "[763] Loss_D: -0.568629 Loss_G: 0.305558\n",
      "[764] Loss_D: -0.571185 Loss_G: 0.305753\n",
      "[765] Loss_D: -0.484068 Loss_G: 0.296469\n",
      "[766] Loss_D: -0.524127 Loss_G: 0.300789\n",
      "[767] Loss_D: -0.565333 Loss_G: 0.304484\n",
      "[768] Loss_D: -0.570733 Loss_G: 0.305432\n",
      "[769] Loss_D: -0.574145 Loss_G: 0.305711\n",
      "[770] Loss_D: -0.572298 Loss_G: 0.304580\n",
      "[771] Loss_D: -0.550899 Loss_G: 0.302572\n",
      "[772] Loss_D: -0.563744 Loss_G: 0.304956\n",
      "[773] Loss_D: -0.535425 Loss_G: 0.300454\n",
      "[774] Loss_D: -0.564189 Loss_G: 0.304799\n",
      "[775] Loss_D: -0.575770 Loss_G: 0.305784\n",
      "[776] Loss_D: -0.575031 Loss_G: 0.305929\n",
      "[777] Loss_D: -0.577123 Loss_G: 0.306174\n",
      "[778] Loss_D: -0.559320 Loss_G: 0.302811\n",
      "[779] Loss_D: -0.497348 Loss_G: 0.295625\n",
      "[780] Loss_D: -0.476707 Loss_G: 0.296747\n",
      "[781] Loss_D: -0.547291 Loss_G: 0.302672\n",
      "[782] Loss_D: -0.566378 Loss_G: 0.305002\n",
      "[783] Loss_D: -0.570291 Loss_G: 0.305046\n",
      "[784] Loss_D: -0.574666 Loss_G: 0.305509\n",
      "[785] Loss_D: -0.571055 Loss_G: 0.304436\n",
      "[786] Loss_D: -0.493164 Loss_G: 0.294632\n",
      "[787] Loss_D: -0.490522 Loss_G: 0.297066\n",
      "[788] Loss_D: -0.552222 Loss_G: 0.303489\n",
      "[789] Loss_D: -0.568692 Loss_G: 0.304791\n",
      "[790] Loss_D: -0.573234 Loss_G: 0.305550\n",
      "[791] Loss_D: -0.572811 Loss_G: 0.305366\n",
      "[792] Loss_D: -0.572791 Loss_G: 0.305632\n",
      "[793] Loss_D: -0.577059 Loss_G: 0.305731\n",
      "[794] Loss_D: -0.567404 Loss_G: 0.304306\n",
      "[795] Loss_D: -0.520956 Loss_G: 0.290114\n",
      "[796] Loss_D: -0.487586 Loss_G: 0.293041\n",
      "[797] Loss_D: -0.463116 Loss_G: 0.295185\n",
      "[798] Loss_D: -0.525351 Loss_G: 0.300731\n",
      "[799] Loss_D: -0.556973 Loss_G: 0.303905\n",
      "[800] Loss_D: -0.569787 Loss_G: 0.305183\n",
      "[801] Loss_D: -0.566837 Loss_G: 0.304497\n",
      "[802] Loss_D: -0.549683 Loss_G: 0.300497\n",
      "[803] Loss_D: -0.552632 Loss_G: 0.303827\n",
      "[804] Loss_D: -0.571492 Loss_G: 0.305532\n",
      "[805] Loss_D: -0.574916 Loss_G: 0.305907\n",
      "[806] Loss_D: -0.572156 Loss_G: 0.305850\n",
      "[807] Loss_D: -0.578149 Loss_G: 0.305862\n",
      "[808] Loss_D: -0.571001 Loss_G: 0.305077\n",
      "[809] Loss_D: -0.573327 Loss_G: 0.305514\n",
      "[810] Loss_D: -0.433819 Loss_G: 0.283168\n",
      "[811] Loss_D: -0.433754 Loss_G: 0.293573\n",
      "[812] Loss_D: -0.499837 Loss_G: 0.299363\n",
      "[813] Loss_D: -0.551466 Loss_G: 0.303176\n",
      "[814] Loss_D: -0.555263 Loss_G: 0.303453\n",
      "[815] Loss_D: -0.540972 Loss_G: 0.299088\n",
      "[816] Loss_D: -0.536676 Loss_G: 0.302598\n",
      "[817] Loss_D: -0.567916 Loss_G: 0.305614\n",
      "[818] Loss_D: -0.573423 Loss_G: 0.305727\n",
      "[819] Loss_D: -0.573289 Loss_G: 0.305684\n",
      "[820] Loss_D: -0.571743 Loss_G: 0.305551\n",
      "[821] Loss_D: -0.555617 Loss_G: 0.303112\n",
      "[822] Loss_D: -0.490909 Loss_G: 0.295980\n",
      "[823] Loss_D: -0.510298 Loss_G: 0.299031\n",
      "[824] Loss_D: -0.560352 Loss_G: 0.303793\n",
      "[825] Loss_D: -0.570426 Loss_G: 0.305385\n",
      "[826] Loss_D: -0.559836 Loss_G: 0.303485\n",
      "[827] Loss_D: -0.573424 Loss_G: 0.305476\n",
      "[828] Loss_D: -0.572540 Loss_G: 0.305128\n",
      "[829] Loss_D: -0.574182 Loss_G: 0.305674\n",
      "[830] Loss_D: -0.574867 Loss_G: 0.305502\n",
      "[831] Loss_D: -0.572188 Loss_G: 0.305031\n",
      "[832] Loss_D: -0.575458 Loss_G: 0.305363\n",
      "[833] Loss_D: -0.565858 Loss_G: 0.304455\n",
      "[834] Loss_D: -0.575967 Loss_G: 0.305445\n",
      "[835] Loss_D: -0.573452 Loss_G: 0.305070\n",
      "[836] Loss_D: -0.501707 Loss_G: 0.296699\n",
      "[837] Loss_D: -0.443574 Loss_G: 0.293775\n",
      "[838] Loss_D: -0.520080 Loss_G: 0.300130\n",
      "[839] Loss_D: -0.553979 Loss_G: 0.302778\n",
      "[840] Loss_D: -0.559662 Loss_G: 0.303948\n",
      "[841] Loss_D: -0.565933 Loss_G: 0.304461\n",
      "[842] Loss_D: -0.568888 Loss_G: 0.304828\n",
      "[843] Loss_D: -0.572724 Loss_G: 0.305040\n",
      "[844] Loss_D: -0.505703 Loss_G: 0.293185\n",
      "[845] Loss_D: -0.530451 Loss_G: 0.300467\n",
      "[846] Loss_D: -0.566856 Loss_G: 0.304503\n",
      "[847] Loss_D: -0.571633 Loss_G: 0.304916\n",
      "[848] Loss_D: -0.572321 Loss_G: 0.305217\n",
      "[849] Loss_D: -0.570030 Loss_G: 0.304590\n",
      "[850] Loss_D: -0.573090 Loss_G: 0.304868\n",
      "[851] Loss_D: -0.509582 Loss_G: 0.295555\n",
      "[852] Loss_D: -0.511721 Loss_G: 0.300271\n",
      "[853] Loss_D: -0.561300 Loss_G: 0.303720\n",
      "[854] Loss_D: -0.571097 Loss_G: 0.304804\n",
      "[855] Loss_D: -0.570259 Loss_G: 0.304801\n",
      "[856] Loss_D: -0.572657 Loss_G: 0.304636\n",
      "[857] Loss_D: -0.566137 Loss_G: 0.303642\n",
      "[858] Loss_D: -0.561393 Loss_G: 0.303856\n",
      "[859] Loss_D: -0.570332 Loss_G: 0.304257\n",
      "[860] Loss_D: -0.543531 Loss_G: 0.300424\n",
      "[861] Loss_D: -0.484199 Loss_G: 0.295662\n",
      "[862] Loss_D: -0.493238 Loss_G: 0.297927\n",
      "[863] Loss_D: -0.556244 Loss_G: 0.302613\n",
      "[864] Loss_D: -0.566756 Loss_G: 0.303855\n",
      "[865] Loss_D: -0.568529 Loss_G: 0.304238\n",
      "[866] Loss_D: -0.568035 Loss_G: 0.303987\n",
      "[867] Loss_D: -0.569571 Loss_G: 0.304440\n",
      "[868] Loss_D: -0.569792 Loss_G: 0.304182\n",
      "[869] Loss_D: -0.569918 Loss_G: 0.303719\n",
      "[870] Loss_D: -0.571311 Loss_G: 0.304431\n",
      "[871] Loss_D: -0.537734 Loss_G: 0.299483\n",
      "[872] Loss_D: -0.556970 Loss_G: 0.302870\n",
      "[873] Loss_D: -0.567877 Loss_G: 0.304488\n",
      "[874] Loss_D: -0.570341 Loss_G: 0.304540\n",
      "[875] Loss_D: -0.572849 Loss_G: 0.304646\n",
      "[876] Loss_D: -0.570068 Loss_G: 0.304005\n",
      "[877] Loss_D: -0.569282 Loss_G: 0.303791\n",
      "[878] Loss_D: -0.478664 Loss_G: 0.293089\n",
      "[879] Loss_D: -0.483798 Loss_G: 0.297089\n",
      "[880] Loss_D: -0.542034 Loss_G: 0.301055\n",
      "[881] Loss_D: -0.565554 Loss_G: 0.303345\n",
      "[882] Loss_D: -0.568708 Loss_G: 0.303754\n",
      "[883] Loss_D: -0.568594 Loss_G: 0.303143\n",
      "[884] Loss_D: -0.568637 Loss_G: 0.303816\n",
      "[885] Loss_D: -0.567208 Loss_G: 0.303955\n",
      "[886] Loss_D: -0.570736 Loss_G: 0.302834\n",
      "[887] Loss_D: -0.534919 Loss_G: 0.299094\n",
      "[888] Loss_D: -0.491984 Loss_G: 0.295903\n",
      "[889] Loss_D: -0.525916 Loss_G: 0.300240\n",
      "[890] Loss_D: -0.565288 Loss_G: 0.302937\n",
      "[891] Loss_D: -0.567513 Loss_G: 0.303773\n",
      "[892] Loss_D: -0.560125 Loss_G: 0.302024\n",
      "[893] Loss_D: -0.570829 Loss_G: 0.303766\n",
      "[894] Loss_D: -0.572758 Loss_G: 0.304078\n",
      "[895] Loss_D: -0.571692 Loss_G: 0.303778\n",
      "[896] Loss_D: -0.570259 Loss_G: 0.304257\n",
      "[897] Loss_D: -0.571963 Loss_G: 0.304378\n",
      "[898] Loss_D: -0.571106 Loss_G: 0.303262\n",
      "[899] Loss_D: -0.469960 Loss_G: 0.290483\n",
      "[900] Loss_D: -0.475482 Loss_G: 0.295416\n",
      "[901] Loss_D: -0.535491 Loss_G: 0.300691\n",
      "[902] Loss_D: -0.561421 Loss_G: 0.302377\n",
      "[903] Loss_D: -0.562116 Loss_G: 0.303434\n",
      "[904] Loss_D: -0.563320 Loss_G: 0.303415\n",
      "[905] Loss_D: -0.568044 Loss_G: 0.303903\n",
      "[906] Loss_D: -0.569702 Loss_G: 0.303552\n",
      "[907] Loss_D: -0.502952 Loss_G: 0.296914\n",
      "[908] Loss_D: -0.519608 Loss_G: 0.299067\n",
      "[909] Loss_D: -0.558685 Loss_G: 0.302555\n",
      "[910] Loss_D: -0.565467 Loss_G: 0.303509\n",
      "[911] Loss_D: -0.568483 Loss_G: 0.303521\n",
      "[912] Loss_D: -0.567297 Loss_G: 0.303781\n",
      "[913] Loss_D: -0.568564 Loss_G: 0.303758\n",
      "[914] Loss_D: -0.566382 Loss_G: 0.303589\n",
      "[915] Loss_D: -0.562340 Loss_G: 0.302327\n",
      "[916] Loss_D: -0.567390 Loss_G: 0.303870\n",
      "[917] Loss_D: -0.569746 Loss_G: 0.303102\n",
      "[918] Loss_D: -0.557060 Loss_G: 0.301969\n",
      "[919] Loss_D: -0.557941 Loss_G: 0.302521\n",
      "[920] Loss_D: -0.563492 Loss_G: 0.303567\n",
      "[921] Loss_D: -0.566804 Loss_G: 0.303210\n",
      "[922] Loss_D: -0.503544 Loss_G: 0.295285\n",
      "[923] Loss_D: -0.473291 Loss_G: 0.294865\n",
      "[924] Loss_D: -0.541451 Loss_G: 0.300374\n",
      "[925] Loss_D: -0.562798 Loss_G: 0.302603\n",
      "[926] Loss_D: -0.564025 Loss_G: 0.303024\n",
      "[927] Loss_D: -0.567476 Loss_G: 0.303328\n",
      "[928] Loss_D: -0.567889 Loss_G: 0.303907\n",
      "[929] Loss_D: -0.569522 Loss_G: 0.303482\n",
      "[930] Loss_D: -0.566168 Loss_G: 0.302971\n",
      "[931] Loss_D: -0.563243 Loss_G: 0.302188\n",
      "[932] Loss_D: -0.561988 Loss_G: 0.303000\n",
      "[933] Loss_D: -0.567711 Loss_G: 0.303268\n",
      "[934] Loss_D: -0.553819 Loss_G: 0.301161\n",
      "[935] Loss_D: -0.565721 Loss_G: 0.303618\n",
      "[936] Loss_D: -0.568764 Loss_G: 0.303412\n",
      "[937] Loss_D: -0.565387 Loss_G: 0.303575\n",
      "[938] Loss_D: -0.571014 Loss_G: 0.303995\n",
      "[939] Loss_D: -0.570068 Loss_G: 0.302876\n",
      "[940] Loss_D: -0.556292 Loss_G: 0.302803\n",
      "[941] Loss_D: -0.565041 Loss_G: 0.303298\n",
      "[942] Loss_D: -0.569933 Loss_G: 0.303457\n",
      "[943] Loss_D: -0.572402 Loss_G: 0.303797\n",
      "[944] Loss_D: -0.477744 Loss_G: 0.295272\n",
      "[945] Loss_D: -0.400449 Loss_G: 0.286548\n",
      "[946] Loss_D: -0.446459 Loss_G: 0.293611\n",
      "[947] Loss_D: -0.493064 Loss_G: 0.296599\n",
      "[948] Loss_D: -0.531601 Loss_G: 0.299598\n",
      "[949] Loss_D: -0.544632 Loss_G: 0.301335\n",
      "[950] Loss_D: -0.549327 Loss_G: 0.301370\n",
      "[951] Loss_D: -0.556798 Loss_G: 0.302465\n",
      "[952] Loss_D: -0.557086 Loss_G: 0.302113\n",
      "[953] Loss_D: -0.557010 Loss_G: 0.302586\n",
      "[954] Loss_D: -0.562982 Loss_G: 0.302707\n",
      "[955] Loss_D: -0.562000 Loss_G: 0.303342\n",
      "[956] Loss_D: -0.563422 Loss_G: 0.302968\n",
      "[957] Loss_D: -0.563760 Loss_G: 0.303156\n",
      "[958] Loss_D: -0.563011 Loss_G: 0.303681\n",
      "[959] Loss_D: -0.564851 Loss_G: 0.303026\n",
      "[960] Loss_D: -0.567790 Loss_G: 0.303351\n",
      "[961] Loss_D: -0.560003 Loss_G: 0.302690\n",
      "[962] Loss_D: -0.565902 Loss_G: 0.303593\n",
      "[963] Loss_D: -0.561212 Loss_G: 0.302700\n",
      "[964] Loss_D: -0.565895 Loss_G: 0.303260\n",
      "[965] Loss_D: -0.554768 Loss_G: 0.301869\n",
      "[966] Loss_D: -0.565360 Loss_G: 0.303584\n",
      "[967] Loss_D: -0.570086 Loss_G: 0.303367\n",
      "[968] Loss_D: -0.563838 Loss_G: 0.303091\n",
      "[969] Loss_D: -0.567767 Loss_G: 0.303471\n",
      "[970] Loss_D: -0.558702 Loss_G: 0.302215\n",
      "[971] Loss_D: -0.563847 Loss_G: 0.302878\n",
      "[972] Loss_D: -0.565512 Loss_G: 0.303504\n",
      "[973] Loss_D: -0.562364 Loss_G: 0.302413\n",
      "[974] Loss_D: -0.563968 Loss_G: 0.303033\n",
      "[975] Loss_D: -0.552105 Loss_G: 0.301231\n",
      "[976] Loss_D: -0.564860 Loss_G: 0.303303\n",
      "[977] Loss_D: -0.563383 Loss_G: 0.302888\n",
      "[978] Loss_D: -0.567864 Loss_G: 0.303508\n",
      "[979] Loss_D: -0.567519 Loss_G: 0.302534\n",
      "[980] Loss_D: -0.565621 Loss_G: 0.303321\n",
      "[981] Loss_D: -0.567347 Loss_G: 0.302964\n",
      "[982] Loss_D: -0.559829 Loss_G: 0.302501\n",
      "[983] Loss_D: -0.565515 Loss_G: 0.303222\n",
      "[984] Loss_D: -0.557114 Loss_G: 0.301536\n",
      "[985] Loss_D: -0.565451 Loss_G: 0.302845\n",
      "[986] Loss_D: -0.558820 Loss_G: 0.302763\n",
      "[987] Loss_D: -0.461297 Loss_G: 0.291984\n",
      "[988] Loss_D: -0.478642 Loss_G: 0.295283\n",
      "[989] Loss_D: -0.537312 Loss_G: 0.299521\n",
      "[990] Loss_D: -0.557239 Loss_G: 0.301776\n",
      "[991] Loss_D: -0.560306 Loss_G: 0.302495\n",
      "[992] Loss_D: -0.562950 Loss_G: 0.302467\n",
      "[993] Loss_D: -0.564435 Loss_G: 0.302892\n",
      "[994] Loss_D: -0.566088 Loss_G: 0.303113\n",
      "[995] Loss_D: -0.567923 Loss_G: 0.303268\n",
      "[996] Loss_D: -0.568485 Loss_G: 0.303375\n",
      "[997] Loss_D: -0.567351 Loss_G: 0.302746\n",
      "[998] Loss_D: -0.568338 Loss_G: 0.303163\n",
      "[999] Loss_D: -0.566649 Loss_G: 0.303320\n",
      "[1000] Loss_D: -0.569113 Loss_G: 0.303358\n",
      "[1001] Loss_D: -0.567277 Loss_G: 0.303310\n",
      "[1002] Loss_D: -0.569075 Loss_G: 0.303555\n",
      "[1003] Loss_D: -0.568208 Loss_G: 0.302817\n",
      "[1004] Loss_D: -0.565508 Loss_G: 0.302656\n",
      "[1005] Loss_D: -0.563157 Loss_G: 0.302749\n",
      "[1006] Loss_D: -0.563460 Loss_G: 0.303030\n",
      "[1007] Loss_D: -0.568976 Loss_G: 0.303196\n",
      "[1008] Loss_D: -0.521826 Loss_G: 0.295807\n",
      "[1009] Loss_D: -0.450578 Loss_G: 0.292928\n",
      "[1010] Loss_D: -0.497005 Loss_G: 0.296405\n",
      "[1011] Loss_D: -0.544614 Loss_G: 0.300527\n",
      "[1012] Loss_D: -0.557911 Loss_G: 0.301911\n",
      "[1013] Loss_D: -0.563294 Loss_G: 0.302507\n",
      "[1014] Loss_D: -0.564149 Loss_G: 0.302781\n",
      "[1015] Loss_D: -0.567012 Loss_G: 0.302414\n",
      "[1016] Loss_D: -0.566866 Loss_G: 0.303019\n",
      "[1017] Loss_D: -0.565875 Loss_G: 0.302186\n",
      "[1018] Loss_D: -0.565149 Loss_G: 0.302328\n",
      "[1019] Loss_D: -0.557037 Loss_G: 0.301190\n",
      "[1020] Loss_D: -0.564152 Loss_G: 0.303198\n",
      "[1021] Loss_D: -0.559415 Loss_G: 0.302709\n",
      "[1022] Loss_D: -0.566316 Loss_G: 0.303067\n",
      "[1023] Loss_D: -0.566562 Loss_G: 0.302676\n",
      "[1024] Loss_D: -0.567032 Loss_G: 0.302762\n",
      "[1025] Loss_D: -0.563756 Loss_G: 0.302766\n",
      "[1026] Loss_D: -0.565068 Loss_G: 0.302550\n",
      "[1027] Loss_D: -0.564321 Loss_G: 0.302487\n",
      "[1028] Loss_D: -0.564211 Loss_G: 0.303005\n",
      "[1029] Loss_D: -0.564260 Loss_G: 0.302514\n",
      "[1030] Loss_D: -0.565598 Loss_G: 0.302493\n",
      "[1031] Loss_D: -0.565119 Loss_G: 0.302362\n",
      "[1032] Loss_D: -0.565681 Loss_G: 0.303015\n",
      "[1033] Loss_D: -0.566333 Loss_G: 0.302797\n",
      "[1034] Loss_D: -0.562036 Loss_G: 0.301808\n",
      "[1035] Loss_D: -0.565128 Loss_G: 0.303024\n",
      "[1036] Loss_D: -0.563710 Loss_G: 0.302378\n",
      "[1037] Loss_D: -0.567414 Loss_G: 0.302746\n",
      "[1038] Loss_D: -0.566738 Loss_G: 0.302931\n",
      "[1039] Loss_D: -0.565269 Loss_G: 0.302292\n",
      "[1040] Loss_D: -0.552866 Loss_G: 0.301136\n",
      "[1041] Loss_D: -0.491787 Loss_G: 0.293891\n",
      "[1042] Loss_D: -0.445478 Loss_G: 0.292841\n",
      "[1043] Loss_D: -0.495308 Loss_G: 0.296592\n",
      "[1044] Loss_D: -0.542537 Loss_G: 0.300177\n",
      "[1045] Loss_D: -0.552904 Loss_G: 0.301418\n",
      "[1046] Loss_D: -0.559429 Loss_G: 0.302444\n",
      "[1047] Loss_D: -0.562226 Loss_G: 0.302402\n",
      "[1048] Loss_D: -0.564222 Loss_G: 0.302433\n",
      "[1049] Loss_D: -0.565846 Loss_G: 0.302905\n",
      "[1050] Loss_D: -0.564861 Loss_G: 0.302567\n",
      "[1051] Loss_D: -0.563977 Loss_G: 0.302908\n",
      "[1052] Loss_D: -0.562952 Loss_G: 0.302196\n",
      "[1053] Loss_D: -0.565270 Loss_G: 0.302331\n",
      "[1054] Loss_D: -0.563274 Loss_G: 0.302576\n",
      "[1055] Loss_D: -0.565200 Loss_G: 0.302534\n",
      "[1056] Loss_D: -0.564026 Loss_G: 0.302358\n",
      "[1057] Loss_D: -0.565484 Loss_G: 0.302940\n",
      "[1058] Loss_D: -0.566616 Loss_G: 0.303071\n",
      "[1059] Loss_D: -0.569656 Loss_G: 0.303110\n",
      "[1060] Loss_D: -0.565582 Loss_G: 0.302351\n",
      "[1061] Loss_D: -0.560255 Loss_G: 0.301981\n",
      "[1062] Loss_D: -0.566889 Loss_G: 0.303036\n",
      "[1063] Loss_D: -0.564639 Loss_G: 0.302562\n",
      "[1064] Loss_D: -0.567420 Loss_G: 0.301965\n",
      "[1065] Loss_D: -0.478968 Loss_G: 0.292270\n",
      "[1066] Loss_D: -0.482307 Loss_G: 0.295952\n",
      "[1067] Loss_D: -0.541115 Loss_G: 0.299599\n",
      "[1068] Loss_D: -0.555311 Loss_G: 0.301561\n",
      "[1069] Loss_D: -0.560730 Loss_G: 0.302439\n",
      "[1070] Loss_D: -0.564380 Loss_G: 0.302318\n",
      "[1071] Loss_D: -0.564804 Loss_G: 0.302422\n",
      "[1072] Loss_D: -0.565748 Loss_G: 0.302229\n",
      "[1073] Loss_D: -0.565558 Loss_G: 0.302557\n",
      "[1074] Loss_D: -0.565355 Loss_G: 0.302640\n",
      "[1075] Loss_D: -0.566302 Loss_G: 0.302372\n",
      "[1076] Loss_D: -0.567263 Loss_G: 0.302476\n",
      "[1077] Loss_D: -0.543825 Loss_G: 0.299105\n",
      "[1078] Loss_D: -0.551444 Loss_G: 0.299636\n",
      "[1079] Loss_D: -0.556509 Loss_G: 0.302520\n",
      "[1080] Loss_D: -0.539926 Loss_G: 0.299620\n",
      "[1081] Loss_D: -0.564571 Loss_G: 0.302727\n",
      "[1082] Loss_D: -0.569614 Loss_G: 0.302768\n",
      "[1083] Loss_D: -0.566064 Loss_G: 0.302442\n",
      "[1084] Loss_D: -0.567014 Loss_G: 0.302973\n",
      "[1085] Loss_D: -0.562664 Loss_G: 0.302195\n",
      "[1086] Loss_D: -0.563287 Loss_G: 0.302088\n",
      "[1087] Loss_D: -0.567134 Loss_G: 0.302479\n",
      "[1088] Loss_D: -0.563982 Loss_G: 0.302591\n",
      "[1089] Loss_D: -0.567999 Loss_G: 0.302553\n",
      "[1090] Loss_D: -0.563079 Loss_G: 0.302057\n",
      "[1091] Loss_D: -0.558223 Loss_G: 0.301690\n",
      "[1092] Loss_D: -0.562772 Loss_G: 0.301763\n",
      "[1093] Loss_D: -0.565823 Loss_G: 0.302775\n",
      "[1094] Loss_D: -0.567556 Loss_G: 0.302545\n",
      "[1095] Loss_D: -0.568134 Loss_G: 0.302648\n"
>>>>>>> a81dbdd3f57534a49e1f9c24cf5f696bb7f3b153
     ]
    }
   ],
   "source": [
    "for i in range(15000):\n",
    "    discriminator_losses = []\n",
    "    generator_losses = []\n",
    "    for j in range(0, iterator.num_train, 32):        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        \n",
    "        for disc_updates in range(5):\n",
    "            real_examples_full, real_examples, fake_images = iterator.get_train_minibatch(j, 32)\n",
    "            D1 = discriminator(real_examples)\n",
    "            fake = generator(fake_images)\n",
    "            D2 = discriminator(fake)\n",
    "            discriminator_loss = -.5 * ((D1 - D2).mean())\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            discriminator_losses.append(discriminator_loss.data[0])\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        \n",
    "        generated_images = generator(fake_images)\n",
    "        generator_loss = -.5 * discriminator( generated_images).mean() + loss_criterion(generated_images, real_examples)\n",
    "        optimizer_generator.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "        generator_losses.append(generator_loss.data[0])\n",
    "\n",
    "    print('[%d] Loss_D: %f Loss_G: %f' % (i, np.mean(discriminator_losses), np.mean(generator_losses)))\n",
    "    save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (32,3,64,64) into shape (32,3,32,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-b57993df24bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_examples_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-210-05dd7cca13c7>\u001b[0m in \u001b[0;36msave_plots\u001b[0;34m(epoch, fake_images, real_images, real_examples_full)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_examples_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mreal_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mreal_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mreal_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (32,3,64,64) into shape (32,3,32,32)"
     ]
    }
   ],
   "source": [
    "save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    # Forward Pass\n",
    "    scores = model(X)\n",
    "    scores = scores.gather(1, y.view(-1, 1)).squeeze()  \n",
    "    \n",
    "    # Backward Pass\n",
    "    gradients_init = torch.FloatTensor([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "    scores.backward(gradients_init)\n",
    "\n",
    "    saliency = X.grad.data\n",
    "    saliency = saliency.abs()\n",
    "    saliency, i = torch.max(saliency, dim = 1)\n",
    "    saliency = saliency.squeeze() \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n",
    "show_saliency_maps(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fooling_np = deprocess(fake_images.clone())\n",
    "X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X[idx])\n",
    "plt.title(class_names[y[idx]])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_fooling_np)\n",
    "plt.title(class_names[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "X_pre = preprocess(Image.fromarray(X[idx]))\n",
    "diff = np.asarray(deprocess(fake_images - real_examples, should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "diff = np.asarray(deprocess(10 * (fake_images - real_examples), should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(12, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_examples[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake_images[0].data.cpu().numpy().transpose(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1894\n"
     ]
    }
   ],
   "source": [
    "# Seed choice for deterministic results\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "print(\"Random Seed: \", manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Painter Supercategories\n",
    "# sports, accessory, animal, outdoor, vehicle, person, indoor, appliance, electronic, furniture\n",
    "# food, kitchen, water, ground, solid, sky, plant, structural, building, textile, window, floor\n",
    "# ceiling, wall, rawmaterial\n",
    "painter_type = \"sports\"\n",
    "image_size = 128\n",
    "lo_bound = image_size // 4\n",
    "up_bound = lo_bound + (image_size // 2)\n",
    "sample_size = 5000\n",
    "train_path = 'inpainting/train2014'\n",
    "dev_path = 'inpainting/val2014'\n",
    "train_annotation_path='inpainting/annotations/instances_train2014.json'\n",
    "dev_annotation_path='inpainting/annotations/instances_val2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_annotation_path, 'r') as R:\n",
    "    train_ann = json.loads(R.read())\n",
    "    \n",
    "with open(dev_annotation_path, 'r') as V:\n",
    "    valid_ann = json.loads(V.read())\n",
    "\n",
    "cat_labels = ['']*91\n",
    "for i in range(len(train_ann['categories'])):\n",
    "    cat_labels[train_ann['categories'][i]['id']] = train_ann['categories'][i]['supercategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    \"\"\"Data Iterator for COCO.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path=train_path,\n",
    "        dev_path=dev_path,\n",
    "        train_annotation_path=train_annotation_path,\n",
    "        dev_annotation_path=dev_annotation_path,\n",
    "    ):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.train_annotation_path = train_annotation_path\n",
    "        self.dev_path = dev_path\n",
    "        self.dev_annotation_path = dev_annotation_path\n",
    "        print('Processing data ...')\n",
    "        self._get_real_and_fake_images()\n",
    "\n",
    "    # JMAK - cropping to 128\n",
    "    def _get_real_and_fake_images(self):            \n",
    "        \"\"\"Get real and fake images from path.\"\"\"\n",
    "        self.train_dataset = dset.CocoDetection(\n",
    "            root=self.train_path,\n",
    "            annFile=self.train_annotation_path,\n",
    "            transform=transforms.Compose([\n",
    "                 transforms.CenterCrop(image_size),\n",
    "                 transforms.ToTensor()])\n",
    "        )\n",
    "       \n",
    "        self.valid_dataset = dset.CocoDetection(\n",
    "            root=self.dev_path, \n",
    "            annFile=self.dev_annotation_path,\n",
    "            transform=transforms.Compose([\n",
    "                 transforms.CenterCrop(image_size),\n",
    "                 transforms.ToTensor()])\n",
    "        )\n",
    "                \n",
    "        # ELDRICK: First, copy over desired number of training and validation. \n",
    "        print('Populating training images & captions ...')\n",
    "        train_images = []\n",
    "        # There appears to be one image missing for some weird reason.\n",
    "        try:\n",
    "            for img, label in self.train_dataset:\n",
    "                area_index = 0\n",
    "                largest_area = 0\n",
    "                try:\n",
    "                    for j in range(len(label)):\n",
    "                        if label[j]['area'] > largest_area:\n",
    "                            largest_area = label[j]['area']\n",
    "                            area_index = j\n",
    "                    img_cat_id = label[area_index]['category_id']\n",
    "                except:\n",
    "                    pass\n",
    "                if cat_labels[img_cat_id] == painter_type:\n",
    "                    train_images.append(img)\n",
    "                if len(train_images) % 500 == 0:\n",
    "                    print(\"Gathered \", len(train_images), \" training images so far\")\n",
    "                if len(train_images) == sample_size:\n",
    "                    break\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "        train_images = torch.stack(train_images)\n",
    "        \n",
    "        # ELDRICK: Second, changed this to match above to terminate\n",
    "        print('Populating validation images ...')\n",
    "        valid_images = []\n",
    "        try:\n",
    "            for img, label in self.valid_dataset:\n",
    "                area_index = 0\n",
    "                largest_area = 0\n",
    "                try:\n",
    "                    for j in range(len(label)):\n",
    "                        if label[j]['area'] > largest_area:\n",
    "                            largest_area = label[j]['area']\n",
    "                            area_index = j\n",
    "                    img_cat_id = label[area_index]['category_id']\n",
    "                except:\n",
    "                    pass\n",
    "                if cat_labels[img_cat_id] == painter_type:\n",
    "                    valid_images.append(img)\n",
    "                if len(valid_images) % 500 == 0:\n",
    "                    print(\"Gathered \", len(valid_images), \" validation images so far\")\n",
    "                if len(valid_images) == sample_size:\n",
    "                    break\n",
    "        except IOError:\n",
    "            pass\n",
    "        \n",
    "        valid_images = torch.stack(valid_images)\n",
    "\n",
    "        # ELDRICK: Crop out 128 by 128\n",
    "        # JMAK: Crop out 64x64 \n",
    "        print('Cropping 64x64 patch for training images ...')\n",
    "        noisy_train_images = copy.deepcopy(train_images.numpy())\n",
    "        noisy_train_images[:, :, lo_bound:up_bound, lo_bound:up_bound] = 0\n",
    "        noisy_train_images = torch.from_numpy(noisy_train_images)\n",
    "\n",
    "        print('Cropping 64x64 patch for validation images ...')\n",
    "        noisy_valid_images = copy.deepcopy(valid_images.numpy())\n",
    "        noisy_valid_images[:, :, lo_bound:up_bound, lo_bound:up_bound] = 0\n",
    "        noisy_valid_images = torch.from_numpy(noisy_valid_images)\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.valid_images = valid_images\n",
    "\n",
    "        self.noisy_train_images = noisy_train_images\n",
    "        self.noisy_valid_images = noisy_valid_images\n",
    "                \n",
    "        self.num_train = len(train_images)\n",
    "        self.num_valid = len(valid_images)\n",
    "\n",
    "    # Return proper sized samples from minibatch - 128x128\n",
    "    # return a 64 x 64 minibatch - JMAK\n",
    "    def get_train_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.train_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_train_images[index: index + batch_size]).cuda()\n",
    "        return real_examples, real_examples[:, :, lo_bound:up_bound, lo_bound:up_bound], fake_examples\n",
    "\n",
    "    def get_valid_minibatch(self, index, batch_size):\n",
    "        \"\"\"Return a minibatch of real and fake examples.\"\"\"\n",
    "        real_examples = Variable(self.valid_images[index: index + batch_size]).cuda()\n",
    "        fake_examples = Variable(self.noisy_valid_images[index: index + batch_size]).cuda()\n",
    "        return real_examples, real_examples[:, :, lo_bound:up_bound, lo_bound:up_bound], fake_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "loading annotations into memory...\n",
      "Done (t=11.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=6.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Populating training images & captions ...\n",
      "Gathered  500  training images so far\n",
      "Gathered  1000  training images so far\n",
      "Gathered  1500  training images so far\n",
      "Gathered  1500  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2000  training images so far\n",
      "Gathered  2500  training images so far\n",
      "Populating validation images ...\n",
      "Gathered  0  validation images so far\n",
      "Gathered  0  validation images so far\n",
      "Gathered  0  validation images so far\n",
      "Gathered  0  validation images so far\n",
      "Gathered  0  validation images so far\n",
      "Gathered  500  validation images so far\n",
      "Gathered  500  validation images so far\n",
      "Gathered  500  validation images so far\n",
      "Gathered  500  validation images so far\n",
      "Gathered  1000  validation images so far\n",
      "Gathered  1000  validation images so far\n",
      "Gathered  1000  validation images so far\n",
      "Gathered  1500  validation images so far\n",
      "Gathered  1500  validation images so far\n",
      "Gathered  2000  validation images so far\n",
      "Gathered  2000  validation images so far\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5410dd540eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Total Class Samples, Train: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Total Class Samples, Valid: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5dd161c84acf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_path, dev_path, train_annotation_path, dev_annotation_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_annotation_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_annotation_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing data ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_real_and_fake_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# JMAK - cropping to 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5dd161c84acf>\u001b[0m in \u001b[0;36m_get_real_and_fake_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mvalid_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0marea_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mlargest_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/envs/cs231n/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2555\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2557\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterator = DataIterator()\n",
    "print(\"Number of Total Class Samples, Train: \", iterator.num_train)\n",
    "print(\"Number of Total Class Samples, Valid: \", iterator.num_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9620654126b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iterator' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "j = np.random.randint(low=0, high=iterator.num_train)\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(iterator.train_images[j].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(iterator.noisy_train_images[j].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator module.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize generator.\"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "        # input is (nc) x 128 x 128\n",
    "        nn.Conv2d(3,start_filter,4,2,1, bias=False),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # state size: (nef) x 64 x 64\n",
    "        nn.Conv2d(start_filter,start_filter,4,2,1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # state size: (nef) x 32 x 32\n",
    "        nn.Conv2d(start_filter,start_filter*2,4,2,1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter*2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # state size: (nef*2) x 16 x 16\n",
    "        nn.Conv2d(start_filter*2,start_filter*4,4,2,1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter*4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # state size: (nef*4) x 8 x 8\n",
    "        nn.Conv2d(start_filter*4,start_filter*8,4,2,1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter*8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # state size: (nef*8) x 4 x 4\n",
    "        nn.Conv2d(start_filter*8,4000,4, bias=False),\n",
    "        \n",
    "        # state size: (nBottleneck) x 1 x 1\n",
    "        nn.BatchNorm2d(4000),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        # input is Bottleneck, going into a convolution\n",
    "        nn.ConvTranspose2d(4000, start_filter * 8, 4, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(start_filter * 8),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        nn.ConvTranspose2d(start_filter * 8, start_filter * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter * 4),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        nn.ConvTranspose2d(start_filter * 4, start_filter * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter * 2),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        nn.ConvTranspose2d(start_filter * 2, start_filter, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(start_filter),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        # state size. (ngf) x 32 x 32\n",
    "        nn.ConvTranspose2d(start_filter, 3, 4, 2, 1, bias=False),\n",
    "        nn.Tanh()\n",
    "        # state size. (nc) x 64 x 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, start_filter):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(3, start_filter, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(start_filter, start_filter * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(start_filter * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(start_filter * 2, start_filter * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(start_filter * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(start_filter * 4, start_filter * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(start_filter * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(start_filter * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate_gen = 1e-3\n",
    "learning_rate_dis = 1e-3\n",
    "betas_gen = (0.5, 0.999)\n",
    "betas_dis = (0.5, 0.999)\n",
    "\n",
    "generator = Generator(start_filter=64).cuda()\n",
    "discriminator = Discriminator(start_filter=64).cuda()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate_gen, betas=betas_gen)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate_dis, betas=betas_dis)\n",
    "clamp_lower = -0.03\n",
    "clamp_upper = 0.03\n",
    "loss_criterion = nn.MSELoss().cuda()\n",
    "save_dir = 'inpainting/samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(epoch, fake_images, real_images, real_examples_full):\n",
    "    # ELDRICK: Change random to fit your sample size\n",
    "    j = np.random.randint(low=0, high=iterator.num_valid)\n",
    "    # ELDRICK: Change minibatch size to fit samples\n",
    "    real_examples_full, real_examples, fake_images = iterator.get_valid_minibatch(j, 64)\n",
    "    generator.eval()\n",
    "    reconstructions = generator(fake_images)\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    real = real_examples_full.data.cpu().numpy()\n",
    "    real_copy = copy.deepcopy(real)\n",
    "    real_copy[:, :, lo_bound:up_bound, lo_bound:up_bound] = reconstructions\n",
    "    real_copy = torch.from_numpy(real_copy)\n",
    "    real = torch.from_numpy(real)\n",
    "    out_tensor = torch.zeros(1, real_copy.size(1), real_copy.size(2), real_copy.size(3))\n",
    "    for zz, zzz in zip(real_copy[:10], real[:10]):\n",
    "        out_tensor = torch.cat([out_tensor, zz.unsqueeze(0)])\n",
    "        out_tensor = torch.cat([out_tensor, zzz.unsqueeze(0)])\n",
    "    vutils.save_image(out_tensor[1:], 'inpainting/samples/epoch_s%d_samples.png' % (epoch), normalize=True, scale_each=True, nrow=4)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterionMSE = nn.MSELoss()\n",
    "for i in range(500):\n",
    "    discriminator_losses = []\n",
    "    generator_losses = []\n",
    "    for j in range(0, iterator.num_train, 64):        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        for disc_updates in range(5):\n",
    "            real_examples_full, real_examples, fake_images = iterator.get_train_minibatch(j, 64)\n",
    "            D1 = discriminator(real_examples)\n",
    "            fake = generator(fake_images)\n",
    "            D2 = discriminator(fake)\n",
    "            \n",
    "            true_labels = torch.ones(D1.size()).cuda()\n",
    "            fake_labels = torch.zeros(D2.size()).cuda()\n",
    "            \n",
    "            # Utilize BCE Loss\n",
    "            real_d_loss = criterion(D1, true_labels).cuda()\n",
    "            fake_d_loss = criterion(D2, fake_labels).cuda()\n",
    "            \n",
    "            discriminator_loss = real_d_loss + fake_d_loss\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            discriminator_loss.backward(retain_graph=True)\n",
    "            \n",
    "            optimizer_discriminator.step()\n",
    "            discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        \n",
    "        generated_images = generator(fake_images)\n",
    "        \n",
    "        # Changing Gen Loss to utilize BCE + L2 MSE Loss\n",
    "        gen_bce_loss = criterion(D2, true_labels).cuda()\n",
    "        gen_l2_loss = criterionMSE(fake, real_examples).cuda()\n",
    "        \n",
    "        generator_loss = 0.001*gen_bce_loss + 0.999*gen_l2_loss\n",
    "        \n",
    "        optimizer_generator.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "        generator_losses.append(generator_loss.item())\n",
    "\n",
    "    print('[%d] Loss_D: %f Loss_G: %f' % (i, np.mean(discriminator_losses), np.mean(generator_losses)))\n",
    "    save_plots(i, fake_images, real_examples, real_examples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'generator_state': generator.state_dict(),\n",
    "    'discriminator_state': discriminator.state_dict(),\n",
    "}\n",
    "torch.save(state, 'inpainting/painter_sports_state_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
